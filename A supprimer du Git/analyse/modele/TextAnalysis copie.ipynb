{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e6e79de",
   "metadata": {},
   "source": [
    "# Exercices Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb244de4",
   "metadata": {},
   "source": [
    "## 1. Expressions régulières"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7c1241",
   "metadata": {},
   "source": [
    "Les **expressions régulières** sont des outils particulièrement puissants. Si leur syntaxe peut paraître complexe, le pouvoir d'extraction que leur maîtrise vous conférera vaut largement l'effort. Les expressions régulières sont particulièrement utiles lorsqu'il s'agit de chercher une expression structurée précise dans un texte. Ce texte peut être un autre code (e.g. une page web Wikipedia), ou une source historique, comme un article de journal, ou un dictionnaire historique.\n",
    "\n",
    "Exemple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5e932c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import *\n",
    "findall('[0-9]+', 'Avenue d’Ouchy 15, 1006 Lausanne')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681b9d3a",
   "metadata": {},
   "source": [
    "Dans cet exemple, l'expression régulière ```[0-9]+``` permet d'extraire les nombres d'une\n",
    "chaîne de caractères (pour ensuite les transformer en ```int``` par exemple). En réalité, cette expression signifie : *Sélectionner un ou plusieurs (+) caractères se suivant et faisant partie de l'ensemble ```[...]``` des caractères compris entre 0 et 9 (```0-9```).*\n",
    "\n",
    "* Les crochets ```[...]``` permettent de définir un ensemble non-ordonné de caractères, au choix.\n",
    "* Le tiret ```-``` permet de signifier que l'on cherche un caractère dont le numéro Unicode est compris entre celui du caractère situé juste avant le tiret (e.g. ```0```) et celui du caractère situé juste après le tiret (e.g. ```9```). Il est possible d'obtenir le numéro Unicode d'un caractère à l'aide de la fonction intégrée ```ord()```. Ainsi, la commande ```ord('0')```, ```ord('9')``` nous indique que le caractère ```0``` porte le numéro ```48```, tandis que 9 porte le numéro 57. À titre d'exemple, ```a``` porte le numéro 97, ```à``` le numéro 224 et ```!``` le numéro 33. Veillez donc à toujours vérifier quels caractères se trouvent dans votre intervalle.\n",
    "* Le signe ```+``` permet d'indiquer que l'on cherche une ou plusieurs occurrences (par exemple un ou plusieurs caractères faisant partie d'un ensemble donné).\n",
    "\n",
    "Vous pouvez aussi rechercher une séquence précise. Par exemple, vous pourriez vouloir\n",
    "étudier la présence des femmes nommées dans les journaux lausannois du XIXème siècle.\n",
    "Pour ce faire, vous pourriez commencer par chercher le mot \"Madame\" (avec ou sans\n",
    "majuscule) suivi d'un espace, puis d'un nom commençant par une lettre majuscule. La\n",
    "syntaxe de votre requête ressemblera alors à ceci :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0ca99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Ce matin, Monsieur Larpin fera miser la maison qu'il a acquise de Madame Dupin \\\n",
    "née Mercier.\"\n",
    "findall('([Mm]adame )([A-Z][a-zà-ÿ\\-]+)', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931accc3",
   "metadata": {},
   "source": [
    "Les parenthèses servent à créer un groupe de caractères. Ici le caractère ```M``` / ```m```, puis la suite de caractères ```\"adame \"```, dans cet ordre précis. À l'intérieur d'une expression régulière, les caractères spéciaux doivent être échappés. Ici, par exemple, le tiret est échappé (```\\-```), pour ne pas être confondu avec son autre fonction, que nous avons découvert dans l'exemple précédent. Notez que les caractères accentués sont distincts des caractères non-accentués et qu'ils doivent donc être inclus expressément (```à–ÿ```).\n",
    "\n",
    "Mettons maintenant que vous travailliez à une échelle très différente, par exemple sur un\n",
    "corpus réunissant tous les journaux historiques francophones. Vous pourriez vouloir\n",
    "rendre cette instruction plus efficace. En réalité, le système des expressions régulières permet de simplement localiser le segment qui vous intéresse, tout en exploitant son contexte à l'aide des \"coups d'oeil\" : *lookbehind* (```?<=```) et *lookahead* (```?=```). Exemple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3609c735",
   "metadata": {},
   "outputs": [],
   "source": [
    "findall('(?<=[Mm]adame )([A-Z][\\-\\w]+)', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585e39d7",
   "metadata": {},
   "source": [
    "Dans cet exemple, l'expression se trouvant dans la première parenthèse n'est que\n",
    "contextuelle. Dans la deuxième parenthèse, nous avons utilisé un mécanisme pratique,\n",
    "avec le raccourci ```\\w+``` qui permet de capturer n'importe quel caractère alphabétique.\n",
    "Prenez le temps d'étudier cette expression régulière pour bien comprendre son\n",
    "fonctionnement.\n",
    "\n",
    "Il est aussi possible d'utiliser les expressions régulières pour nettoyer certaines données\n",
    "ou les remplacer par d'autres. La fonction ```re.sub``` permet de remplacer une expression par une nouvelle chaîne de caractères, un peu à l'image de la fonction *replace*, que vous\n",
    "connaissez déjà, mais avec la puissance des expressions régulières."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab778ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub('([^\\-\\w ]+)', '@', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554db89a",
   "metadata": {},
   "source": [
    "Dans cet exemple, tous les caractères *non-alphabétiques* du texte sont remplacés par un\n",
    "arobase ```@```. Le signe circonflexe ```^``` permet la négation de l'expression qui suit.\n",
    "Littéralement, l'expression remplace donc tous les caractères non-alphabétiques, qui ne\n",
    "sont ni des tirets, ni des espaces. Pour les supprimer, on pourrait remplacer ```@``` par une chaîne de caractères vide.\n",
    "\n",
    "### Exercice 1\n",
    "\n",
    "Chargez les données extraites d'Impresso à l'aide du script ci-dessous. Utilisez une expression régulière pour repérer, dans la liste ```articles```, les mentions de personnes au format ```Prénom Nom```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1f0b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('impresso_export_jeanne_hersch.csv', sep=';')\n",
    "articles = df['content'].dropna().values.tolist()[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c20160f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92e440ea",
   "metadata": {},
   "source": [
    "## 2. Entités nommées\n",
    "\n",
    "Les **entités nommées** sont des expressions textuelles caractéristiques et reconnaissables : par exemple des noms propres, des noms de lieux, des dates, des noms d'organisations,\n",
    "etc. La reconnaissance et l'extraction des entités nommées constitue un enjeu majeur du\n",
    "*natural language processing*. En humanités digitales, ces technologies peuvent être très utiles pour créer des liens entre des données textuelles, des lieux et des personnes.\n",
    "\n",
    "```\n",
    "conda install spacy && conda install spacy-transformers && python -m spacy download fr_core_news_lg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3301578b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('LOC', 'Casino'),\n",
       " ('PER', 'Mme Pollet'),\n",
       " ('LOC', 'Paris'),\n",
       " ('PER', 'MM. Girard'),\n",
       " ('PER', 'Lagoanère')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp_fr = spacy.load('fr_core_news_lg')\n",
    "doc = nlp_fr('Grand concert vocal et instrumental dans la salle du Casino, \\\n",
    "le mardi 5, par Mme Pollet, harpiste de Paris, et MM. Girard et Lagoanère;')\n",
    "[(ent.label_, ent.text) for ent in doc.ents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d5fee7",
   "metadata": {},
   "source": [
    "Les modèles NLP comme celui de spacy sont en général pré-entraînés sur des corpus très\n",
    "grands, comme par exemple Wikipedia, ou des corpus de journaux. La reconnaissance des entités nommées est évidemment spécifique à la langue. En conséquence, les modèles pour certaines langues, comme l'anglais ou le chinois, sont très performants, tandis que d'autres le sont nettement moins. Le français se trouve en général dans une tranche intermédiaire."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ea61db",
   "metadata": {},
   "source": [
    "### Exercice 2\n",
    "\n",
    "Reprenez les données de l'exercice précédent. Utilisez maintenant spacy pour repérer les entités nommées désignant des personnes dans les articles. Que constatez-vous?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c7c0dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afd61064",
   "metadata": {},
   "source": [
    "## 3. Latent Dirichlet Allocation\n",
    "\n",
    "La Latent Dirichlet Allocation (LDA, à ne pas confondre avec la Linear Discriminant Analysis) est une méthode de **topic modelling**, qui permet de distinguer\n",
    "les différents thèmes d'un texte ou d'une série de textes en s'appuyant sur la distribution\n",
    "statistique des **tokens** et en particulier sur la co-occurence de certains groupes de tokens.\n",
    "\n",
    "Pour utiliser LDA sur des données textuelles, il est nécessaire de rendre ces dernières\n",
    "numériquement intelligibles. La manière la plus simple de faire ceci est de subdiviser le\n",
    "texte en segments plus petits, que l'on appellera documents. \n",
    "\n",
    "Un document peut par\n",
    "exemple correspondre à une phrase, une réplique, ou à une entrée dans votre base de\n",
    "données. Chaque document contient un ou plusieurs tokens. L'ensemble des types (ici\n",
    "l'ensemble des tokens uniques existants) constitue le vocabulaire. Un numéro est attribué\n",
    "à chaque type du vocabulaire. \n",
    "\n",
    "Par exemple, *académie* correspondra au numéro 0, *Acadie*\n",
    "au numéro 1, *acétone* au numéro 2, et ainsi de suite. Grâce à ce système, chaque document\n",
    "peut être représenté comme une liste de 0 et de 1. La longueur du vecteur correspondra au nombre de mots compris dans le vocabulaire. Si le document contient un token correspondant au nième type du vocabulaire, la nième entrée de la liste prendra la valeur 1. Tous les autres types, absents du document en question, prendront la valeur 0 (*one-hot encoding*).\n",
    "\n",
    "Pour commencer, installez la librairie suivante\n",
    "\n",
    "```\n",
    "conda install scikit-learn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af7fbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "documents, tokens = [], []\n",
    "for article in articles:\n",
    "    words = findall('([a-zà-ÿ0-9]+)', article.lower())\n",
    "    tokens += words\n",
    "    documents.append(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17402860",
   "metadata": {},
   "source": [
    "Dans cet exemple, nous créons deux listes : **documents** et **tokens**, en itérant très simplement sur chaque article. Nous ne considérons pas la capitalisation des caractères et nous stockons chaque mot dans la liste continue tokens et dans la liste des documents, qui stocke séparément les mots de chaque article dans une sous-liste. Notez qu'il existe des manières plus efficientes de créer ces deux listes avec Python, ici nous avons privilégié la notation la plus lisible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5989e0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulaire = pd.Series(tokens).value_counts()[20:]\n",
    "vocabulaire = vocabulaire [vocabulaire >= 3].sort_index().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9385c0eb",
   "metadata": {},
   "source": [
    "Dans un deuxième temps, nous comptons toutes les occurrences uniques des tokens et ne conservons que ceux qui apparaissent au moins 3 fois. En dessous de ce seuil, l'utilisation d'un modèle statistique est peu pertinente. Nous enlevons également les 20 tokens les plus fréquents, qui correspondent aux mots tels que \"de\", \"la\", \"et\", \"à\", \"les\", \"des\", etc. Cette liste des mots-clés uniques apparaissant au moins 3 fois constitue le **vocabulaire**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75507b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temps de calcul indicatif pour cette cellule : 3 minutes\n",
    "one_hot = np.zeros((len(documents), len(vocabulaire)))\n",
    "for index, document in enumerate(documents):\n",
    "    for token in document:\n",
    "        one_hot[index][int(np.argmax(vocabulaire == token))] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4f3a41",
   "metadata": {},
   "source": [
    "Pour *chaque* document, une liste de zéros de longueur ```len(vocabulaire)``` est initialisée. Une boucle itère ensuite sur chaque **token *n*** du document et remplace la nième valeur de la liste par 1. n correspond au numéro attribué au token dans le vocabulaire.\n",
    "\n",
    "À présent, puisque nos données ont été traduites en langage numérique, il est possible\n",
    "d'appliquer la LDA. La LDA est un algorithme de clustering paramétrique. Il est en effet\n",
    "nécessaire de lui indiquer le nombre de clusters attendu (ici, nous utiliserons la valeur ```7```). Dans le cas du topic modelling,\n",
    "chaque cluster correspond intuitivement à un thème."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92784209",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=7, random_state=0)\n",
    "res = lda.fit_transform(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f070401",
   "metadata": {},
   "source": [
    "Les clusters sont simplement représentés par des numéros. Le travail d'interprétation du *topic* de chaque cluster nous revient. Pour faciliter cette interprétation, nous pouvons par exemple\n",
    "visualiser les mots les plus fréquents ou les mots les plus caractéristiques de chaque cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fc6747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mots les plus fréquents du cluster 5\n",
    "for i in np.flip(np.argsort(lda.components_[5]))[:20]:\n",
    "    print(vocabulaire[i], end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3551c511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mots les plus caractéristiques du cluster 5\n",
    "type_attribution = lda.transform(np.identity(len(vocabulaire)))\n",
    "for t in vocabulaire[np.flip(np.argsort(type_attribution[:,5]))[:20]]:\n",
    "    print(t, end=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eefeb43",
   "metadata": {},
   "source": [
    "Dans ce cas, on remarque un thème centré sur les conférences, et les réceptions, les évènements publics.\n",
    "\n",
    "On peut essayer d'encore mieux comprendre ce thème en affichant les 3 articles les plus caractéristiques de ce dernier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f52031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for art_id in np.flip(np.argsort(res[:, 5]))[:3]:\n",
    "    print(articles[art_id], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe980fb8",
   "metadata": {},
   "source": [
    "### Exercice 3\n",
    "Reproduisez l'exercice sur la *Latent Dirichlet Allocation* sur un corpus *Impresso* correspondant à l'un de vos personnages. Arrivez-vous à interpréter les thèmes obtenus ? Qu'est-ce que ces thèmes vous permettent de dire par rapport à la représentation médiatique de ce dernier ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ff2375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1018c144",
   "metadata": {},
   "source": [
    "### Exercice 4\n",
    "Reproduisez l'exemple sur la reconnaissance des entités nommées sur votre propre corpus. Calculez et affichez les lieux les plus cités."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a82a543",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
