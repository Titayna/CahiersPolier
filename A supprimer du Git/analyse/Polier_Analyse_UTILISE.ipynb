{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965206ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "polier_stopwords = ['mois', 'janvier', 'fevrier', 'mars', 'avril', 'mai', 'juin', 'juillet', 'aout', 'septembre', \n",
    "                    'octobre', 'novembre', 'decembre', 'xbre', 'jour', 'jours', 'lundi', 'mardi', 'mercredi', \n",
    "                    'jeudi', 'vendredi', 'samedi', 'dimanche', 'lun', 'mar', 'mer', 'jeu', 'ven', 'sam', 'dim',                    \n",
    "                    'mr', 'mrs', 'mde', 'mle', 'mlle', 'monsieur', 'mr de',\n",
    "                    'crutz', 'batz', 'livres']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a20ac67c",
   "metadata": {},
   "source": [
    "# Transcriptions diplomatiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe6f82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e13e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle de traitement du langage naturel pour le français\n",
    "nlp_fr = spacy.load('fr_core_news_lg')\n",
    "\n",
    "# Récupérer tous les fichiers commençant par \"transcr_\" dans le répertoire donné\n",
    "files = glob.glob('/Users/tita/Desktop/Memoire_local/analyse/polier_json/transcr_*.json')\n",
    "\n",
    "# Boucler sur chaque fichier et extraire le texte de chaque document pour analyse avec Spacy\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        for document in data:\n",
    "            doc = nlp_fr(document['transcription'])\n",
    "            entities = [(ent.label_, ent.text) for ent in doc.ents]\n",
    "            print(entities)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be456e23",
   "metadata": {},
   "source": [
    "### Entités qui apparaisent au moins trois fois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26890bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle de traitement du langage naturel pour le français\n",
    "nlp_fr = spacy.load('fr_core_news_lg')\n",
    "\n",
    "# Récupérer tous les fichiers commençant par \"transcr_\" dans le répertoire donné\n",
    "files = glob.glob('/Users/tita/Desktop/Memoire_local/analyse/polier_data/transcr_*.json')\n",
    "\n",
    "# Initialiser un dictionnaire pour stocker les entités et leur fréquence\n",
    "entity_freq = {}\n",
    "\n",
    "# Boucler sur chaque fichier et extraire le texte de chaque document pour analyse avec Spacy\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        for document in data:\n",
    "            doc = nlp_fr(document['transcription'])\n",
    "            entities = [(ent.label_, ent.text) for ent in doc.ents]\n",
    "            \n",
    "            # Ajouter chaque entité au dictionnaire et incrémenter sa fréquence\n",
    "            for entity in entities:\n",
    "                if entity in entity_freq:\n",
    "                    entity_freq[entity] += 1\n",
    "                else:\n",
    "                    entity_freq[entity] = 1\n",
    "\n",
    "# Filtrer les entités pour ne conserver que celles qui apparaissent au moins trois fois\n",
    "filtered_entities = [entity for entity, freq in entity_freq.items() if freq >= 3]\n",
    "\n",
    "# Trier les entités filtrées par ordre alphabétique\n",
    "filtered_entities.sort()\n",
    "\n",
    "# Afficher les entités filtrées avec un retour à la ligne après chaque entité\n",
    "for entity in filtered_entities:\n",
    "    print(entity[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63b36542",
   "metadata": {},
   "source": [
    "# Nettoyage des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c137e0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import glob\n",
    "import json\n",
    "from re import findall\n",
    "import spacy\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703ac463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupérer tous les fichiers commençant par \"transcr_\" dans le répertoire donné\n",
    "files = glob.glob('/Users/tita/Desktop/Memoire_local/polier_json/transcr_*.json')\n",
    "\n",
    "# Boucler sur chaque fichier et extraire le texte de chaque document pour analyse avec Spacy\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        for document in data:\n",
    "            # Convertir le texte en minuscules\n",
    "            text = document['transcription'].lower()\n",
    "            document['transcription'] = text.replace(\"  \", \" \")\n",
    "\n",
    "    # Enregistrer le document avec les entités dans le fichier JSON\n",
    "    with open(file, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3660407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle de traitement du langage naturel pour le français\n",
    "nlp_fr = spacy.load('fr_core_news_lg')\n",
    "\n",
    "# Récupérer tous les fichiers commençant par \"transcr_\" dans le répertoire donné\n",
    "files = glob.glob('/Users/tita/Desktop/Memoire_local/analyse/polier_data/transcr_*.json')\n",
    "\n",
    "# Initialiser un dictionnaire pour stocker les entités et leur fréquence\n",
    "entity_freq = {}\n",
    "\n",
    "# Boucler sur chaque fichier et extraire le texte de chaque document pour analyse avec Spacy\n",
    "for file in files:\n",
    "    try:\n",
    "        with open(file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            for document in data:\n",
    "                doc = nlp_fr(document['transcription'])\n",
    "                entities = [(ent.label_, ent.text) for ent in doc.ents]\n",
    "\n",
    "                # Ajouter chaque entité au dictionnaire et incrémenter sa fréquence\n",
    "                for entity in entities:\n",
    "                    if entity in entity_freq:\n",
    "                        entity_freq[entity] += 1\n",
    "                    else:\n",
    "                        entity_freq[entity] = 1\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Erreur de décodage JSON dans le fichier {file}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Filtrer les entités pour ne conserver que celles qui apparaissent au moins deux fois\n",
    "filtered_entities = [(entity, freq) for entity, freq in entity_freq.items() if freq >= 5]\n",
    "\n",
    "# Trier les entités filtrées par ordre alphabétique\n",
    "filtered_entities.sort()\n",
    "\n",
    "# Afficher les entités filtrées avec leur fréquence, avec un retour à la ligne après chaque entité\n",
    "for entity, freq in filtered_entities:\n",
    "    print(f\"{entity[1]} ({freq})\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed9f7d01",
   "metadata": {},
   "source": [
    "# spaCy et entité nommées"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2aeea6ef",
   "metadata": {},
   "source": [
    "## Analyse de fréquence des entités nommées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51af542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import glob\n",
    "import json\n",
    "from re import findall\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fe315211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mr 14919\n",
      "lausanne 4288\n",
      "ete 2471\n",
      "fl 2294\n",
      "pr 2239\n",
      "vernand 1632\n",
      "mr le bailli 1546\n",
      "crutz 1504\n",
      "meme 1364\n",
      "paris 1279\n",
      "louis 1256\n",
      "lutry 1124\n",
      "st 1044\n",
      "bise 1034\n",
      "temoin 905\n",
      "geneve 887\n",
      "supreme 733\n",
      "francois 705\n",
      "mrs 700\n",
      "tavel 670\n",
      "mde 668\n",
      "la haye 566\n",
      "france 538\n",
      "bottens 514\n",
      "batz 508\n",
      "morges 462\n",
      "vour 440\n",
      "messel 440\n",
      "hollande 415\n",
      "londres 389\n",
      "dito 386\n",
      "vevey 366\n",
      "rosset 363\n",
      "crousaz 362\n",
      "soleil 362\n",
      "lavanchy 332\n",
      "polier 314\n",
      "vullyamoz 312\n",
      "baud 304\n",
      "ler 293\n",
      "pully 287\n",
      "mr de vernand 284\n",
      "blanchard 269\n",
      "ven 267\n",
      "pre 264\n",
      "ent 260\n",
      "suisse 259\n",
      "chateau 253\n",
      "bonte 251\n",
      "tissot 249\n",
      "iseli 245\n",
      "blondel 240\n",
      "crissier 234\n",
      "mr de tavel 233\n",
      "saphorin 227\n",
      "cassat 224\n",
      "moitie 220\n",
      "mle 219\n",
      "chapuis 213\n",
      "moudon 213\n",
      "besson 213\n",
      "tete 212\n",
      "belmont 209\n",
      "jean 208\n",
      "berne 203\n",
      "ue 188\n",
      "bergier 184\n",
      "jeanne 183\n",
      "marie 182\n",
      "pl 180\n",
      "martin 179\n",
      "mlle 179\n",
      "alle 179\n",
      "girouette 178\n",
      "aupres 177\n",
      "ecrit 177\n",
      "ave 176\n",
      "francillon 176\n",
      "illegitime 173\n",
      "echus 173\n",
      "beaud 170\n",
      "ter 170\n",
      "secretan 169\n",
      "bl 164\n",
      "barbaz 163\n",
      "angleterre 163\n",
      "ecu 162\n",
      "ilne 161\n",
      "vi 158\n",
      "chavannes 157\n",
      "curtat 157\n",
      "gratif 155\n",
      "tou 154\n",
      "cully 154\n",
      "bise bar 154\n",
      "paudex 153\n",
      "aver 152\n",
      "mr de 152\n",
      "echu 152\n",
      "pache 151\n",
      "vaud 150\n",
      "l. 150\n",
      "ine 147\n",
      "villette 147\n",
      "pes 147\n",
      "ume 146\n",
      "viret 146\n",
      "neuchatel 145\n",
      "michoud 143\n",
      "ls 141\n",
      "dela 140\n",
      "st-francois 139\n",
      "ille 139\n",
      "tont 136\n",
      "ecrire 135\n",
      "aumone 135\n",
      "expedier 134\n",
      "gelee 132\n",
      "charite 132\n",
      "cen 131\n",
      "surete 130\n",
      "ily 129\n",
      "jean jaques 128\n",
      "marcel 128\n",
      "lle 128\n",
      "nyon 125\n",
      "senat 125\n",
      "repondre 124\n",
      "mercier 124\n",
      "baillive 124\n",
      "montrond 123\n",
      "rof 122\n",
      "corsier 121\n",
      "accouchee 119\n",
      "corbaz 119\n",
      "j. 119\n",
      "lour 117\n",
      "chandieu 117\n",
      "jean francois 117\n",
      "bise tres 117\n",
      "abraham 115\n",
      "chavan 115\n",
      "int 114\n",
      "la france 111\n",
      "jenner 111\n",
      "rochat 111\n",
      "fournee 110\n",
      "allaman 109\n",
      "freron 109\n",
      "condamnee 108\n"
     ]
    }
   ],
   "source": [
    "# Charger le modèle de traitement du langage naturel pour le français\n",
    "nlp_fr = spacy.load('fr_core_news_lg')\n",
    "\n",
    "# Récupérer tous les fichiers commençant par \"transcr_\" dans le répertoire donné\n",
    "files = glob.glob('/Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_*.json')\n",
    "\n",
    "# Initialiser un dictionnaire pour stocker les entités et leur fréquence\n",
    "entity_freq = {}\n",
    "\n",
    "# Boucler sur chaque fichier et extraire le texte de chaque document pour analyse avec Spacy\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        for document in data:\n",
    "            doc = nlp_fr(document['transcription'])\n",
    "            for ent in doc.ents:\n",
    "                if ent.text not in entity_freq:\n",
    "                    entity_freq[ent.text] = 1\n",
    "                else:\n",
    "                    entity_freq[ent.text] += 1\n",
    "\n",
    "# Afficher les 50 entités les plus fréquentes\n",
    "sorted_entities = sorted(entity_freq.items(), key=lambda x: x[1], reverse=True)[:150]\n",
    "for entity, freq in sorted_entities:\n",
    "    print(entity, freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9c3e57be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entités nommées les plus fréquentes:\n",
      "\n",
      "\n",
      "Personnes:\n",
      " mr le bailli: 1546\n",
      "temoin: 702\n",
      "supreme: 621\n",
      "rosset: 315\n",
      "vullyamoz: 284\n",
      "mr de vernand: 284\n",
      "polier: 270\n",
      "blanchard: 266\n",
      "baud: 255\n",
      "tissot: 246\n",
      "blondel: 240\n",
      "mr de tavel: 233\n",
      "besson: 212\n",
      "chapuis: 199\n",
      "jean: 185\n",
      "jeanne: 183\n",
      "marie: 178\n",
      "francillon: 176\n",
      "beaud: 170\n",
      "martin: 166\n",
      "\n",
      "Lieux:\n",
      " lausanne: 4288\n",
      "vernand: 1618\n",
      "paris: 1279\n",
      "geneve: 887\n",
      "la haye: 546\n",
      "france: 535\n",
      "bottens: 508\n",
      "morges: 447\n",
      "hollande: 412\n",
      "londres: 389\n",
      "vevey: 365\n",
      "pully: 287\n",
      "suisse: 259\n",
      "chateau: 252\n",
      "crissier: 224\n",
      "belmont: 169\n",
      "angleterre: 162\n",
      "berne: 150\n",
      "vaud: 149\n",
      "neuchatel: 141\n",
      "\n",
      "Organisation:\n",
      " crutz: 265\n",
      "echus: 150\n",
      "baillive: 122\n",
      "marechaussee: 66\n",
      "platel: 62\n",
      "supreme: 50\n",
      "legitime: 40\n",
      "benefice: 34\n",
      "extreme: 34\n",
      "chambre: 30\n",
      "present: 29\n",
      "dupuis: 28\n",
      "gratification: 28\n",
      "domiciliee: 28\n",
      "la poste: 27\n",
      "deja: 27\n",
      "servagnin: 26\n",
      "lausanne &: 23\n",
      "batz &: 22\n",
      "tt: 22\n",
      "\n",
      "Divers:\n",
      " michoud: 97\n",
      "reserve: 77\n",
      "ble: 73\n",
      "delit: 68\n",
      "idees: 60\n",
      "pre: 51\n",
      "supreme: 43\n",
      "bugnion: 40\n",
      "mezery: 38\n",
      "preter serment: 36\n",
      "fermete: 34\n",
      "repete: 34\n",
      "surete: 28\n",
      "vanite: 24\n",
      "jean: 23\n",
      "meme prix: 23\n",
      "lettre de tavel: 22\n",
      "crutz: 22\n",
      "requete: 22\n",
      "crutz 4: 21\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialiser des listes vides pour stocker les entités nommées de chaque catégorie\n",
    "personnes = []\n",
    "lieux = []\n",
    "miscellaneous_entities = []\n",
    "organisation = []\n",
    "\n",
    "# Créer une liste d'arrêts en français\n",
    "french_stopwords = [\"le\", \"la\" ,'l', \"les\", \"de\", \"du\", \"des\", \"un\", \"une\", \"et\", \"ou\", \"car\", \"par\", \"pour\", \"avec\", \"sur\", \"meme\"]\n",
    "\n",
    "# Ajouter des mots spécifiques à la variable 'polier'\n",
    "polier_stopwords = ['mois', 'janvier', 'fevrier', 'mars', 'avril', 'mai', 'juin', 'juillet', 'aout', 'septembre', \n",
    "                    'octobre', 'novembre', 'decembre', 'xbre', 'jour', 'jours', 'lundi', 'mardi', 'mercredi', \n",
    "                    'jeudi', 'vendredi', 'samedi', 'dimanche', 'lun', 'mar', 'mer', 'jeu', 'ven', 'sam', 'dim',                    \n",
    "                    'mr', 'mrs', 'mde', 'mle', 'mlle', 'monsieur', 'mr de', 'lle',                     \n",
    "                    'pr', 'ent' ,'ue', 'aver', 'ume', 'quil', 'j.', 'ler', 'trei', 'moitie', 'alle', 'aupres', 'dela', 'l.',\n",
    "                    'ave', 'pl', 'vi', 'pes', 'ls', 'gen', 'ilne', 'sen', 'ine', 'cen', 'ter', 'bl', 'por',                    \n",
    "                    'rep', 'iel', 'aue', 'ac', 'sl', 'sr', 'oue', 'lod', 'jer', 'vez', 'avc', \"il'\", 'lan',                    \n",
    "                    'sep', 'mos', 'pri', 'ds', 'sun', 'lan', 'pai', 'tl', 'gre', 'dre', 'pt', 'oir', 'sup',                    \n",
    "                    'apras', 'ps', 'vl', 'sgn', 'chz', 'bt', 'tou', 'lour', 'rom', 'sonr', 'ca', 'mad', 'ux', 'mout',                    \n",
    "                    'jur', 'sg', 'avi', 'lel', 'dat', 'oue', 'ace', 'gl', 'voux', 'ort', 'ther', 'quime', 'coy', 'mo',                    \n",
    "                    'cla', 'dev', 'ily', '& mr', 'pui', 'men', 'mid', 'jou', 'mame', 'doit', 'mo', 'cla', 'doi', 'coe', \n",
    "                    'cre', 'ill', 'abe', 'det', 'detre', 'naz', 'ony', 'ili', 'ant', 'hor', 'ate', 'lce', 'vou', 'gif',\n",
    "                    'pr 2', 'qul', 's1', 'nt', 'dune', 'exa', 'oin', 'cele', 'boi', 'nr', 'cop', 'oit', 'pa', 'voi', 'noe',\n",
    "                    'oin', 'mons', 'rof', 'pout', 'memes', 'ts', 'aui', 'fle', 'uit', 'porle', 'mlr', 'ell', 'pr mr', 'ede', \n",
    "                    'nle', 'mit', 'ons', 'cer', 'm4', 'damse', 'v1', 'vo', 'ms', 'ani', 'm1', 'faie', 'els', 'aru',\n",
    "                    'ecus', 'ecu', 'batz', 'livres', 'louis']\n",
    "\n",
    "per_stopwords = ['bise', 'ecrit', 'expedier', 'accouchee', 'bise tres', 'lutry', 'st', 'gratif', 'bise', 'bonte', 'saphorin', 'francois']\n",
    "\n",
    "loc_stopwords = ['fl', 'crutz', 'bise', 'tavel', 'batz', 'messel', 'vour', 'dito', 'crousaz', 'lavanchy', 'bergier', 'ecrire', 'viret', \n",
    "                 'gelee', 'fl', 'temoin', 'tete', 'alle', 'dela', 'temoin', 'tete', 'ille', 'secretan', 'st', 'soleil', \n",
    "                 'veau', 'fiaux', 'girouette', 'repondre', 'solvit', 'aumone', 'surete', 'mons', 'yon', 'fievre', 'freron', 'conge']\n",
    "\n",
    "org_stopwords = ['ete', 'ter', 'pre', 'cassat', 'cordey', '& &', 'etiez', 'int', 'arive', 'faie']\n",
    "\n",
    "misc_stopwords = ['iseli', 'hertig', 'cassat', 'moudon', 'tolozan', 'etablit', 'courlat', 'abert', 'vullyamoz', 'porchet', 'fl']\n",
    "\n",
    "\n",
    "# Concaténer les deux listes pour créer la liste complète des mots à supprimer\n",
    "stopwords = french_stopwords + polier_stopwords\n",
    "\n",
    "\n",
    "# Charger le modèle de traitement du langage naturel pour le français\n",
    "nlp_fr = spacy.load('fr_core_news_lg')\n",
    "\n",
    "# Récupérer tous les fichiers commençant par \"transcr_\" dans le répertoire donné\n",
    "files = glob.glob('/Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_*.json')\n",
    "\n",
    "# Boucler sur chaque fichier et extraire le texte de chaque document pour analyse avec Spacy\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        for document in data:\n",
    "            doc = nlp_fr(document['transcription'])\n",
    "            for ent in doc.ents:\n",
    "                # Ajouter l'entité nommée à la liste appropriée en fonction de sa catégorie\n",
    "                if ent.label_ == 'PER' and (ent.text.lower() not in stopwords) and (ent.text.lower() not in per_stopwords):\n",
    "                    personnes.append(ent.text)\n",
    "                elif ent.label_ == 'LOC' and (ent.text.lower() not in stopwords) and (ent.text.lower() not in loc_stopwords):\n",
    "                    lieux.append(ent.text)\n",
    "                elif ent.label_ == 'MISC' and (ent.text.lower() not in stopwords) and (ent.text.lower() not in misc_stopwords):\n",
    "                    miscellaneous_entities.append(ent.text)\n",
    "                elif ent.label_ == 'ORG' and (ent.text.lower() not in stopwords) and (ent.text.lower() not in org_stopwords):\n",
    "                    organisation.append(ent.text)\n",
    "\n",
    "# Afficher les 10 entités nommées les plus fréquentes dans chaque catégorie\n",
    "print('Entités nommées les plus fréquentes:\\n')\n",
    "print('\\nPersonnes:\\n', \"\\n\".join([f\"{entity}: {count}\" for entity, count in Counter(personnes).most_common(20)]))\n",
    "print('\\nLieux:\\n', \"\\n\".join([f\"{entity}: {count}\" for entity, count in Counter(lieux).most_common(20)]))\n",
    "print('\\nOrganisation:\\n', \"\\n\".join([f\"{entity}: {count}\" for entity, count in Counter(organisation).most_common(20)]))\n",
    "print('\\nDivers:\\n', \"\\n\".join([f\"{entity}: {count}\" for entity, count in Counter(miscellaneous_entities).most_common(20)]))\n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03cfa3bd",
   "metadata": {},
   "source": [
    "# TextBlob"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09c392c6",
   "metadata": {},
   "source": [
    "TextBlob peut identifier les sentiments suivants :\n",
    "\n",
    "La polarité du sentiment, qui indique si le texte est plutôt positif, négatif ou neutre. La polarité est une valeur comprise entre -1 (très négatif) et 1 (très positif).\n",
    "\n",
    "La subjectivité du texte, qui mesure dans quelle mesure le texte est objectif ou subjectif. La subjectivité est une valeur comprise entre 0 (très objectif) et 1 (très subjectif).\n",
    "\n",
    "Par exemple, si la polarité est de 0.5, cela signifie que le texte est plutôt positif, tandis qu'une polarité de -0.5 indique que le texte est plutôt négatif. De même, une subjectivité de 0.2 signifie que le texte est plutôt objectif, tandis qu'une subjectivité de 0.8 indique que le texte est plutôt subjectif."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51e4a1c7",
   "metadata": {},
   "source": [
    "\n",
    "En termes généraux, \"polarisé\" se réfère à une forte tendance ou orientation vers un point de vue, tandis que \"subjectif\" se réfère à quelque chose qui est influencé par les opinions personnelles ou les sentiments d'un individu.\n",
    "\n",
    "Dans le contexte de ce code, \"les entités les plus polarisées\" font référence aux entités qui ont les sentiments les plus positifs ou négatifs, c'est-à-dire les entités pour lesquelles le sentiment moyen est le plus éloigné de 0.\n",
    "\n",
    "\"Les entités les plus subjectives\", quant à elles, se réfèrent aux entités pour lesquelles les sentiments varient le plus, c'est-à-dire celles pour lesquelles il y a une grande variation dans les opinions personnelles ou les sentiments exprimés dans les transcriptions. En d'autres termes, ces entités ont tendance à susciter des réactions fortes ou contrastées chez les auditeurs ou les transcripteurs, ce qui se traduit par une grande variation dans les sentiments exprimés à leur égard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe9cdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43821717",
   "metadata": {},
   "source": [
    "## Polarité"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cbae548f",
   "metadata": {},
   "source": [
    "#### Personnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc456ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle de traitement du langage naturel pour le français\n",
    "nlp_fr = spacy.load('fr_core_news_lg')\n",
    "\n",
    "# Liste de stop words\n",
    "stop_words = [\"le\", \"la\" ,'l', \"les\", \"de\", \"du\", \"des\", \"un\", \"une\", \"et\", \"ou\", \"car\", \"par\", \"pour\", \"avec\", \"sur\", \"meme\"]\n",
    "\n",
    "polier_stopwords = ['mois', 'janvier', 'fevrier', 'mars', 'avril', 'mai', 'juin', 'juillet', 'aout', 'septembre', \n",
    "                    'octobre', 'novembre', 'decembre', 'xbre', 'jour', 'jours', 'lundi', 'mardi', 'mercredi', \n",
    "                    'jeudi', 'vendredi', 'samedi', 'dimanche', 'lun', 'mar', 'mer', 'jeu', 'ven', 'sam', 'dim',                    \n",
    "                    'mr', 'mrs', 'mde', 'mle', 'mlle', 'monsieur', 'mr de', 'lle',                     \n",
    "                    'pr', 'ent' ,'ue', 'aver', 'ume', 'quil', 'j.', 'ler', 'trei', 'moitie', 'alle', 'aupres', 'dela', 'l.',\n",
    "                    'ave', 'pl', 'vi', 'pes', 'ls', 'gen', 'ilne', 'sen', 'ine', 'cen', 'ter', 'bl', 'por',                    \n",
    "                    'rep', 'iel', 'aue', 'ac', 'sl', 'sr', 'oue', 'lod', 'jer', 'vez', 'avc', \"il'\", 'lan',                    \n",
    "                    'sep', 'mos', 'pri', 'ds', 'sun', 'lan', 'pai', 'tl', 'gre', 'dre', 'pt', 'oir', 'sup',                    \n",
    "                    'apras', 'ps', 'vl', 'sgn', 'chz', 'bt', 'tou', 'lour', 'rom', 'sonr', 'ca', 'mad', 'ux', 'mout',                    \n",
    "                    'jur', 'sg', 'avi', 'lel', 'dat', 'oue', 'ace', 'gl', 'voux', 'ort', 'ther', 'quime', 'coy', 'mo',                    \n",
    "                    'cla', 'dev', 'ily', '& mr', 'pui', 'men', 'mid', 'jou', 'mame', 'doit', 'mo', 'cla', 'doi', 'coe', \n",
    "                    'cre', 'ill', 'abe', 'det', 'detre', 'naz', 'ony', 'ili', 'ant', 'hor', 'ate', 'lce', 'vou', 'gif',\n",
    "                    'pr 2', 'qul', 's1', 'nt', 'dune', 'exa', 'oin', 'cele', 'boi', 'nr', 'cop', 'oit', 'pa', 'voi', 'noe',\n",
    "                    'oin', 'mons', 'rof', 'pout', 'memes', 'ts', 'aui', 'fle', 'uit', 'porle', 'mlr', 'ell', 'pr mr', 'ede', \n",
    "                    'nle', 'mit', 'ons', 'cer', 'm4', 'damse', 'v1', 'vo', 'ms', 'ani', 'm1', 'faie', 'els', 'aru']\n",
    "\n",
    "per_stopwords = ['bise', 'ecrit', 'expedier', 'accouchee', 'bise tres', 'lutry', 'st', 'gratif', 'bise', 'bonte', 'saphorin',\n",
    "                 'mton', 'blig', 'as mr', 'michel 1778', 'ausset', 'mandoit', 'noisset']\n",
    "\n",
    "stopwords = stop_words + per_stopwords + polier_stopwords\n",
    "\n",
    "# Récupérer tous les fichiers commençant par \"transcr_\" dans le répertoire donné\n",
    "files = glob.glob('/Users/tita/Desktop/Memoire_local/analyse/polier_data/transcr_*.json')\n",
    "\n",
    "# Initialiser des dictionnaires pour stocker les fréquences des entités nommées et les polarités par entité nommée\n",
    "entity_frequencies = {}\n",
    "entity_polarities = {}\n",
    "\n",
    "# Boucler sur chaque fichier et extraire les entités nommées et les polarités de chaque document pour analyse avec Spacy et TextBlob\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        for document in data:\n",
    "            doc = nlp_fr(document['transcription'])\n",
    "            blob = TextBlob(document['transcription'])\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ == 'PER': \n",
    "                    entity = ent.text.lower()\n",
    "                    if entity not in entity_frequencies and entity not in stopwords:\n",
    "                        entity_frequencies[entity] = 0\n",
    "                        entity_polarities[entity] = 0\n",
    "                    if entity in entity_frequencies:\n",
    "                        entity_frequencies[entity] += 1\n",
    "                        entity_polarities[entity] += blob.sentiment.polarity\n",
    "\n",
    "\n",
    "# Calculer la moyenne des polarités pour chaque entité nommée avec au moins 5 occurrences\n",
    "entity_polarities_avg = {}\n",
    "for entity, frequency in entity_frequencies.items():\n",
    "    if frequency >= 5:\n",
    "        entity_polarities_avg[entity] = entity_polarities[entity] / frequency\n",
    "\n",
    "\n",
    "# Récupérer les entités avec leur moyenne de polarité triées par ordre décroissant\n",
    "sorted_entities = sorted(entity_polarities_avg.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Imprimer les 10 entités les plus polarisées\n",
    "print(\"Les entités de personnes les plus polarisées :\")\n",
    "for entity, polarity in sorted_entities[:10]:\n",
    "    print(entity, polarity)\n",
    "    \n",
    "print('\\n')  \n",
    "\n",
    "# Imprimer les 10 entités les moins polarisées\n",
    "print(\"Les entités de personnes les moins polarisées :\")\n",
    "for entity, polarity in sorted_entities[-10:]:\n",
    "    print(entity, polarity)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c1b81fae",
   "metadata": {},
   "source": [
    "#### Lieux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfe134e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle de traitement du langage naturel pour le français\n",
    "nlp_fr = spacy.load('fr_core_news_lg')\n",
    "\n",
    "# Liste de stop words\n",
    "stop_words = ['le', 'la', 'les', 'de', 'du', 'des', 'un', 'une', 'et', 'à', 'en', 'pour']\n",
    "\n",
    "polier_stopwords = ['mois', 'janvier', 'fevrier', 'mars', 'avril', 'mai', 'juin', 'juillet', 'aout', 'septembre', 'octobre', 'novembre', 'decembre', 'xbre',\n",
    "                    'jour', 'jours', 'lundi', 'mardi', 'mercredi', 'jeudi', 'vendredi', 'samedi', 'dimanche',\n",
    "                    'lun', 'mar', 'mer', 'jeu', 'ven', 'sam', 'dim',\n",
    "                    'mr', 'mrs', 'mrs', 'mde', 'mle', 'mlle', 'monsieur', 'mr de', 'lle', \n",
    "                    'pr', 'ent' ,'ue', 'aver', 'ume', 'quil', 'j.', 'ler', 'trei', 'moitie', 'alle', 'aupres', 'dela', 'l.',\n",
    "                    'ave', 'pl', 'vi', 'pes', 'ls', 'gen', 'ilne', 'sen', 'ine', 'cen', 'ter', 'bl', 'por',\n",
    "                    'rep', 'iel', 'aue', 'ac', 'sl', 'sr', 'oue', 'lod', 'jer', 'vez', 'avc', \"il'\", 'lan',\n",
    "                    'sep', 'mos', 'pri', 'ds', 'sun', 'lan', 'pai', 'tl', 'gre', 'dre', 'pt', 'oir', 'sup',\n",
    "                    'apras', 'ps', 'vl', 'sgn', 'chz', 'bt', 'tou', 'lour', 'rom', 'sonr', 'ca', 'mad', 'ux', 'mout',\n",
    "                    'jur', 'sg', 'avi', 'lel', 'dat', 'oue', 'ace', 'gl', 'voux', 'ort', 'ther', 'quime', 'coy', 'mo',\n",
    "                    'cla', 'dev', 'ily', '& mr', 'pui', 'men', 'mid', 'jou', 'mame', 'doit', 'mo', 'cla']\n",
    "\n",
    "loc_stopwords = ['fl', 'crutz', 'bise', 'tavel', 'batz', 'messel', 'vour', 'dito', 'crousaz', 'lavanchy', 'bergier', 'ecrire', 'viret', \n",
    "                 'gelee', 'fl', 'temoin', 'tete', 'alle', 'dela', 'temoin', 'tete', 'ille', 'secretan', 'st', 'soleil']\n",
    "\n",
    "stopwords = stop_words + loc_stopwords + polier_stopwords\n",
    "\n",
    "# Récupérer tous les fichiers commençant par \"transcr_\" dans le répertoire donné\n",
    "files = glob.glob('/Users/tita/Desktop/Memoire_local/analyse/polier_data/transcr_*.json')\n",
    "\n",
    "# Initialiser des dictionnaires pour stocker les fréquences des entités nommées et les polarités par entité nommée\n",
    "entity_frequencies = {}\n",
    "entity_polarities = {}\n",
    "\n",
    "# Boucler sur chaque fichier et extraire les entités nommées et les polarités de chaque document pour analyse avec Spacy et TextBlob\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        for document in data:\n",
    "            doc = nlp_fr(document['transcription'])\n",
    "            blob = TextBlob(document['transcription'])\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ == 'LOC': \n",
    "                    entity = ent.text.lower()\n",
    "                    if entity not in entity_frequencies and entity not in stopwords:\n",
    "                        entity_frequencies[entity] = 0\n",
    "                        entity_polarities[entity] = 0\n",
    "                    if entity in entity_frequencies:  # vérifier si l'entité existe déjà dans le dictionnaire\n",
    "                        entity_frequencies[entity] += 1  # incrémenter la fréquence de l'entité d'une unité\n",
    "                        entity_polarities[entity] += blob.sentiment.polarity\n",
    "\n",
    "# Calculer la moyenne des polarités pour chaque entité nommée avec au moins 5 occurrences\n",
    "entity_polarities_avg = {}\n",
    "for entity, frequency in entity_frequencies.items():\n",
    "    if frequency >= 5:\n",
    "        entity_polarities_avg[entity] = entity_polarities[entity] / frequency\n",
    "\n",
    "\n",
    "# Récupérer les entités avec leur moyenne de polarité triées par ordre décroissant\n",
    "sorted_entities = sorted(entity_polarities_avg.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Imprimer les 10 entités les plus polarisées\n",
    "print(\"Les entités de lieux les plus polarisées :\")\n",
    "for entity, polarity in sorted_entities[:10]:\n",
    "    print(entity, polarity)\n",
    "    \n",
    "print('\\n')  \n",
    "\n",
    "# Imprimer les 10 entités les moins polarisées\n",
    "print(\"Les entités de lieux les moins polarisées :\")\n",
    "for entity, polarity in sorted_entities[-10:]:\n",
    "    print(entity, polarity)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0642049c",
   "metadata": {},
   "source": [
    "#### organisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bf0b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle de traitement du langage naturel pour le français\n",
    "nlp_fr = spacy.load('fr_core_news_lg')\n",
    "\n",
    "# Liste de stop words\n",
    "stop_words = ['le', 'la', 'les', 'de', 'du', 'des', 'un', 'une', 'et', 'à', 'en', 'pour']\n",
    "\n",
    "org_stopwords = ['ete', 'ter', 'pre', 'cassat', 'cordey']\n",
    "\n",
    "stopwords = stop_words + org_stopwords\n",
    "\n",
    "# Récupérer tous les fichiers commençant par \"transcr_\" dans le répertoire donné\n",
    "files = glob.glob('/Users/tita/Desktop/Memoire_local/analyse/polier_data/transcr_*.json')\n",
    "\n",
    "# Initialiser des dictionnaires pour stocker les fréquences des entités nommées et les polarités par entité nommée\n",
    "entity_frequencies = {}\n",
    "entity_polarities = {}\n",
    "\n",
    "# Boucler sur chaque fichier et extraire les entités nommées et les polarités de chaque document pour analyse avec Spacy et TextBlob\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        for document in data:\n",
    "            doc = nlp_fr(document['transcription'])\n",
    "            blob = TextBlob(document['transcription'])\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ == 'ORG': \n",
    "                    entity = ent.text.lower()\n",
    "                    if entity not in entity_frequencies and entity not in stopwords:\n",
    "                        entity_frequencies[entity] = 0\n",
    "                        entity_polarities[entity] = 0\n",
    "                    if entity in entity_frequencies:  # vérifier si l'entité existe déjà dans le dictionnaire\n",
    "                        entity_frequencies[entity] += 1  # incrémenter la fréquence de l'entité d'une unité\n",
    "                        entity_polarities[entity] += blob.sentiment.polarity\n",
    "\n",
    "\n",
    "# Calculer la moyenne des polarités pour chaque entité nommée avec au moins 5 occurrences\n",
    "entity_polarities_avg = {}\n",
    "for entity, frequency in entity_frequencies.items():\n",
    "    if frequency >= 5:\n",
    "        entity_polarities_avg[entity] = entity_polarities[entity] / frequency\n",
    "\n",
    "\n",
    "# Récupérer les entités avec leur moyenne de polarité triées par ordre décroissant\n",
    "sorted_entities = sorted(entity_polarities_avg.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Imprimer les 10 entités les plus polarisées\n",
    "print(\"Les entités d'organisations les plus polarisées :\")\n",
    "for entity, polarity in sorted_entities[:10]:\n",
    "    print(entity, polarity)\n",
    "    \n",
    "print('\\n')  \n",
    "\n",
    "# Imprimer les 10 entités les moins polarisées\n",
    "print(\"Les entités d'organisations les moins polarisées :\")\n",
    "for entity, polarity in sorted_entities[-10:]:\n",
    "    print(entity, polarity)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ae48ba0",
   "metadata": {},
   "source": [
    "#### divers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c282f48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle de traitement du langage naturel pour le français\n",
    "nlp_fr = spacy.load('fr_core_news_lg')\n",
    "\n",
    "# Liste de stop words\n",
    "stop_words = ['le', 'la', 'les', 'de', 'du', 'des', 'un', 'une', 'et', 'à', 'en', 'pour']\n",
    "\n",
    "misc_stopwords = ['iseli', 'hertig', 'cassat', 'moudon', 'tolozan', 'etablit', 'courlat', 'abert', 'vullyamoz', 'porchet']\n",
    "\n",
    "stopwords = stop_words + misc_stopwords\n",
    "\n",
    "# Récupérer tous les fichiers commençant par \"transcr_\" dans le répertoire donné\n",
    "files = glob.glob('/Users/tita/Desktop/Memoire_local/analyse/polier_data/transcr_*.json')\n",
    "\n",
    "# Initialiser des dictionnaires pour stocker les fréquences des entités nommées et les polarités par entité nommée\n",
    "entity_frequencies = {}\n",
    "entity_polarities = {}\n",
    "\n",
    "# Boucler sur chaque fichier et extraire les entités nommées et les polarités de chaque document pour analyse avec Spacy et TextBlob\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        for document in data:\n",
    "            doc = nlp_fr(document['transcription'])\n",
    "            blob = TextBlob(document['transcription'])\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ == 'MISC': \n",
    "                    entity = ent.text.lower()\n",
    "                    if entity not in entity_frequencies and entity not in stopwords:\n",
    "                        entity_frequencies[entity] = 0\n",
    "                        entity_polarities[entity] = 0\n",
    "                    if entity in entity_frequencies:  # vérifier si l'entité existe déjà dans le dictionnaire\n",
    "                        entity_frequencies[entity] += 1  # incrémenter la fréquence de l'entité d'une unité\n",
    "                        entity_polarities[entity] += blob.sentiment.polarity\n",
    "\n",
    "\n",
    "# Calculer la moyenne des polarités pour chaque entité nommée avec au moins 5 occurrences\n",
    "entity_polarities_avg = {}\n",
    "for entity, frequency in entity_frequencies.items():\n",
    "    if frequency >= 5:\n",
    "        entity_polarities_avg[entity] = entity_polarities[entity] / frequency\n",
    "\n",
    "\n",
    "# Récupérer les entités avec leur moyenne de polarité triées par ordre décroissant\n",
    "sorted_entities = sorted(entity_polarities_avg.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Imprimer les 10 entités les plus polarisées\n",
    "print(\"Les entités diverses les plus polarisées :\")\n",
    "for entity, polarity in sorted_entities[:10]:\n",
    "    print(entity, polarity)\n",
    "    \n",
    "print('\\n')  \n",
    "\n",
    "# Imprimer les 10 entités les moins polarisées\n",
    "print(\"Les entités diverses les moins polarisées :\")\n",
    "for entity, polarity in sorted_entities[-10:]:\n",
    "    print(entity, polarity)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "508642e6",
   "metadata": {},
   "source": [
    "## Subjectivité"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3359f2c9",
   "metadata": {},
   "source": [
    "#### personnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c21cf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle de traitement du langage naturel pour le français\n",
    "nlp_fr = spacy.load('fr_core_news_lg')\n",
    "\n",
    "# Liste de stop words\n",
    "stop_words = ['le', 'la', 'les', 'de', 'du', 'des', 'un', 'une', 'et', 'à', 'en', 'pour']\n",
    "\n",
    "polier_stopwords = ['mois', 'janvier', 'fevrier', 'mars', 'avril', 'mai', 'juin', 'juillet', 'aout', 'septembre', 'octobre', 'novembre', 'decembre', 'xbre',\n",
    "                    'jour', 'jours', 'lundi', 'mardi', 'mercredi', 'jeudi', 'vendredi', 'samedi', 'dimanche',\n",
    "                    'lun', 'mar', 'mer', 'jeu', 'ven', 'sam', 'dim',\n",
    "                    'mr', 'mrs', 'mrs', 'mde', 'mle', 'mlle', 'monsieur', 'mr de', 'lle', \n",
    "                    'pr', 'ent' ,'ue', 'aver', 'ume', 'quil', 'j.', 'ler', 'trei', 'moitie', 'alle', 'aupres', 'dela', 'l.',\n",
    "                    'ave', 'pl', 'vi', 'pes', 'ls', 'gen', 'ilne', 'sen', 'ine', 'cen', 'ter', 'bl', 'por',\n",
    "                    'rep', 'iel', 'aue', 'ac', 'sl', 'sr', 'oue', 'lod', 'jer', 'vez', 'avc', \"il'\", 'lan',\n",
    "                    'sep', 'mos', 'pri', 'ds', 'sun', 'lan', 'pai', 'tl', 'gre', 'dre', 'pt', 'oir', 'sup',\n",
    "                    'apras', 'ps', 'vl', 'sgn', 'chz', 'bt', 'tou', 'lour', 'rom', 'sonr', 'ca', 'mad', 'ux', 'mout',\n",
    "                    'jur', 'sg', 'avi', 'lel', 'dat', 'oue', 'ace', 'gl', 'voux', 'ort', 'ther', 'quime', 'coy', 'mo',\n",
    "                    'cla', 'dev', 'ily', '& mr', 'pui', 'men', 'mid', 'jou', 'mame', 'doit', 'mo', 'cla']\n",
    "\n",
    "per_stopwords = ['bise', 'ecrit', 'expedier', 'accouchee', 'bise tres', 'lutry', 'st', 'gratif', 'bise', 'bonte', 'saphorin']\n",
    "\n",
    "stopwords = stop_words + per_stopwords + polier_stopwords\n",
    "\n",
    "\n",
    "# Récupérer tous les fichiers commençant par \"transcr_\" dans le répertoire donné\n",
    "files = glob.glob('/Users/tita/Desktop/Memoire_local/analyse/polier_data/transcr_*.json')\n",
    "\n",
    "# Initialiser des dictionnaires pour stocker les fréquences des entités nommées et les subjectivités par entité nommée\n",
    "entity_frequencies = {}\n",
    "entity_subjectivities = {}\n",
    "\n",
    "# Boucler sur chaque fichier et extraire les entités nommées et les subjectivités de chaque document pour analyse avec Spacy et TextBlob\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        for document in data:\n",
    "            doc = nlp_fr(document['transcription'])\n",
    "            blob = TextBlob(document['transcription'])\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ == 'PER':\n",
    "                    entity = ent.text.lower()\n",
    "                    if entity not in entity_frequencies and entity not in stopwords:\n",
    "                        entity_frequencies[entity] = 0\n",
    "                        entity_subjectivities[entity] = 0\n",
    "                    entity_frequencies[entity] += 1\n",
    "                    entity_subjectivities[entity] += blob.sentiment.subjectivity\n",
    "\n",
    "# Calculer la moyenne des subjectivités pour chaque entité nommée avec au moins 5 occurrences\n",
    "entity_subjectivities_avg = {}\n",
    "for entity, frequency in entity_frequencies.items():\n",
    "    if frequency >= 5:\n",
    "        entity_subjectivities_avg[entity] = entity_subjectivities[entity] / frequency\n",
    "\n",
    "# Récupérer les entités avec leur moyenne de subjectivité triées par ordre décroissant\n",
    "sorted_entities = sorted(entity_subjectivities_avg.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Imprimer les 10 entités les plus subjectives\n",
    "print(\"Les entités de personnes les plus subjectives :\")\n",
    "for entity, subjectivity in sorted_entities[:10]:\n",
    "    print(entity, subjectivity)\n",
    "\n",
    "print('\\n')  \n",
    "\n",
    "# Imprimer les 10 entités les moins subjectives\n",
    "print(\"Les entités de personnes les moins subjectives :\")\n",
    "for entity, subjectivity in sorted_entities[-10:]:\n",
    "    print(entity, subjectivity)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9176bba2",
   "metadata": {},
   "source": [
    "#### Lieux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29543a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle de traitement du langage naturel pour le français\n",
    "nlp_fr = spacy.load('fr_core_news_lg')\n",
    "\n",
    "# Liste de stop words\n",
    "stop_words = ['le', 'la', 'les', 'de', 'du', 'des', 'un', 'une', 'et', 'à', 'en', 'pour']\n",
    "\n",
    "polier_stopwords = ['mois', 'janvier', 'fevrier', 'mars', 'avril', 'mai', 'juin', 'juillet', 'aout', 'septembre', 'octobre', 'novembre', 'decembre', 'xbre',\n",
    "                    'jour', 'jours', 'lundi', 'mardi', 'mercredi', 'jeudi', 'vendredi', 'samedi', 'dimanche',\n",
    "                    'lun', 'mar', 'mer', 'jeu', 'ven', 'sam', 'dim',\n",
    "                    'mr', 'mrs', 'mrs', 'mde', 'mle', 'mlle', 'monsieur', 'mr de', 'lle', \n",
    "                    'pr', 'ent' ,'ue', 'aver', 'ume', 'quil', 'j.', 'ler', 'trei', 'moitie', 'alle', 'aupres', 'dela', 'l.',\n",
    "                    'ave', 'pl', 'vi', 'pes', 'ls', 'gen', 'ilne', 'sen', 'ine', 'cen', 'ter', 'bl', 'por',\n",
    "                    'rep', 'iel', 'aue', 'ac', 'sl', 'sr', 'oue', 'lod', 'jer', 'vez', 'avc', \"il'\", 'lan',\n",
    "                    'sep', 'mos', 'pri', 'ds', 'sun', 'lan', 'pai', 'tl', 'gre', 'dre', 'pt', 'oir', 'sup',\n",
    "                    'apras', 'ps', 'vl', 'sgn', 'chz', 'bt', 'tou', 'lour', 'rom', 'sonr', 'ca', 'mad', 'ux', 'mout',\n",
    "                    'jur', 'sg', 'avi', 'lel', 'dat', 'oue', 'ace', 'gl', 'voux', 'ort', 'ther', 'quime', 'coy', 'mo',\n",
    "                    'cla', 'dev', 'ily', '& mr', 'pui', 'men', 'mid', 'jou', 'mame', 'doit', 'mo', 'cla']\n",
    "\n",
    "loc_stopwords = ['fl', 'crutz', 'bise', 'tavel', 'batz', 'messel', 'vour', 'dito', 'crousaz', 'lavanchy', 'bergier', 'ecrire', 'viret', \n",
    "                 'gelee', 'fl', 'temoin', 'tete', 'alle', 'dela', 'temoin', 'tete', 'ille', 'secretan', 'st', 'soleil']\n",
    "\n",
    "stopwords = stop_words + loc_stopwords + polier_stopwords\n",
    "\n",
    "\n",
    "# Récupérer tous les fichiers commençant par \"transcr_\" dans le répertoire donné\n",
    "files = glob.glob('/Users/tita/Desktop/Memoire_local/analyse/polier_data/transcr_*.json')\n",
    "\n",
    "# Initialiser des dictionnaires pour stocker les fréquences des entités nommées et les subjectivités par entité nommée\n",
    "entity_frequencies = {}\n",
    "entity_subjectivities = {}\n",
    "\n",
    "# Boucler sur chaque fichier et extraire les entités nommées et les subjectivités de chaque document pour analyse avec Spacy et TextBlob\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        for document in data:\n",
    "            doc = nlp_fr(document['transcription'])\n",
    "            blob = TextBlob(document['transcription'])\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ == 'LOC':\n",
    "                    entity = ent.text.lower()\n",
    "                    if entity not in entity_frequencies and entity not in stopwords:\n",
    "                        entity_frequencies[entity] = 0\n",
    "                        entity_subjectivities[entity] = 0\n",
    "                    entity_frequencies[entity] += 1\n",
    "                    entity_subjectivities[entity] += blob.sentiment.subjectivity\n",
    "\n",
    "# Calculer la moyenne des subjectivités pour chaque entité nommée avec au moins 5 occurrences\n",
    "entity_subjectivities_avg = {}\n",
    "for entity, frequency in entity_frequencies.items():\n",
    "    if frequency >= 5:\n",
    "        entity_subjectivities_avg[entity] = entity_subjectivities[entity] / frequency\n",
    "\n",
    "# Récupérer les entités avec leur moyenne de subjectivité triées par ordre décroissant\n",
    "sorted_entities = sorted(entity_subjectivities_avg.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Imprimer les 10 entités les plus subjectives\n",
    "print(\"Les entités de lieux les plus subjectives :\")\n",
    "for entity, subjectivity in sorted_entities[:10]:\n",
    "    print(entity, subjectivity)\n",
    "\n",
    "print('\\n')  \n",
    "\n",
    "# Imprimer les 10 entités les moins subjectives\n",
    "print(\"Les entités de lieux les moins subjectives :\")\n",
    "for entity, subjectivity in sorted_entities[-10:]:\n",
    "    print(entity, subjectivity)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "11c39d78",
   "metadata": {},
   "source": [
    "#### organisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c02d528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle de traitement du langage naturel pour le français\n",
    "nlp_fr = spacy.load('fr_core_news_lg')\n",
    "\n",
    "# Liste de stop words\n",
    "stop_words = ['le', 'la', 'les', 'de', 'du', 'des', 'un', 'une', 'et', 'à', 'en', 'pour']\n",
    "\n",
    "polier_stopwords = ['mois', 'janvier', 'fevrier', 'mars', 'avril', 'mai', 'juin', 'juillet', 'aout', 'septembre', 'octobre', 'novembre', 'decembre', 'xbre',\n",
    "                    'jour', 'jours', 'lundi', 'mardi', 'mercredi', 'jeudi', 'vendredi', 'samedi', 'dimanche',\n",
    "                    'lun', 'mar', 'mer', 'jeu', 'ven', 'sam', 'dim',\n",
    "                    'mr', 'mrs', 'mrs', 'mde', 'mle', 'mlle', 'monsieur', 'mr de', 'lle', \n",
    "                    'pr', 'ent' ,'ue', 'aver', 'ume', 'quil', 'j.', 'ler', 'trei', 'moitie', 'alle', 'aupres', 'dela', 'l.',\n",
    "                    'ave', 'pl', 'vi', 'pes', 'ls', 'gen', 'ilne', 'sen', 'ine', 'cen', 'ter', 'bl', 'por',\n",
    "                    'rep', 'iel', 'aue', 'ac', 'sl', 'sr', 'oue', 'lod', 'jer', 'vez', 'avc', \"il'\", 'lan',\n",
    "                    'sep', 'mos', 'pri', 'ds', 'sun', 'lan', 'pai', 'tl', 'gre', 'dre', 'pt', 'oir', 'sup',\n",
    "                    'apras', 'ps', 'vl', 'sgn', 'chz', 'bt', 'tou', 'lour', 'rom', 'sonr', 'ca', 'mad', 'ux', 'mout',\n",
    "                    'jur', 'sg', 'avi', 'lel', 'dat', 'oue', 'ace', 'gl', 'voux', 'ort', 'ther', 'quime', 'coy', 'mo',\n",
    "                    'cla', 'dev', 'ily', '& mr', 'pui', 'men', 'mid', 'jou', 'mame', 'doit', 'mo', 'cla']\n",
    "\n",
    "org_stopwords = ['ete', 'ter', 'pre', 'cassat', 'cordey']\n",
    "\n",
    "stopwords = stop_words + org_stopwords + polier_stopwords\n",
    "\n",
    "\n",
    "# Récupérer tous les fichiers commençant par \"transcr_\" dans le répertoire donné\n",
    "files = glob.glob('/Users/tita/Desktop/Memoire_local/analyse/polier_data/transcr_*.json')\n",
    "\n",
    "# Initialiser des dictionnaires pour stocker les fréquences des entités nommées et les subjectivités par entité nommée\n",
    "entity_frequencies = {}\n",
    "entity_subjectivities = {}\n",
    "\n",
    "# Boucler sur chaque fichier et extraire les entités nommées et les subjectivités de chaque document pour analyse avec Spacy et TextBlob\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        for document in data:\n",
    "            doc = nlp_fr(document['transcription'])\n",
    "            blob = TextBlob(document['transcription'])\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ == 'ORG':\n",
    "                    entity = ent.text.lower()\n",
    "                    if entity not in entity_frequencies and entity not in stopwords:\n",
    "                        entity_frequencies[entity] = 0\n",
    "                        entity_subjectivities[entity] = 0\n",
    "                    entity_frequencies[entity] += 1\n",
    "                    entity_subjectivities[entity] += blob.sentiment.subjectivity\n",
    "\n",
    "# Calculer la moyenne des subjectivités pour chaque entité nommée avec au moins 5 occurrences\n",
    "entity_subjectivities_avg = {}\n",
    "for entity, frequency in entity_frequencies.items():\n",
    "    if frequency >= 5:\n",
    "        entity_subjectivities_avg[entity] = entity_subjectivities[entity] / frequency\n",
    "\n",
    "# Récupérer les entités avec leur moyenne de subjectivité triées par ordre décroissant\n",
    "sorted_entities = sorted(entity_subjectivities_avg.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Imprimer les 10 entités les plus subjectives\n",
    "print(\"Les entités de lieux les plus subjectives :\")\n",
    "for entity, subjectivity in sorted_entities[:10]:\n",
    "    print(entity, subjectivity)\n",
    "\n",
    "print('\\n')  \n",
    "\n",
    "# Imprimer les 10 entités les moins subjectives\n",
    "print(\"Les entités de lieux les moins subjectives :\")\n",
    "for entity, subjectivity in sorted_entities[-10:]:\n",
    "    print(entity, subjectivity)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b758057",
   "metadata": {},
   "source": [
    "#### divers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cb0501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle de traitement du langage naturel pour le français\n",
    "nlp_fr = spacy.load('fr_core_news_lg')\n",
    "\n",
    "# Liste de stop words\n",
    "stop_words = ['le', 'la', 'les', 'de', 'du', 'des', 'un', 'une', 'et', 'à', 'en', 'pour']\n",
    "\n",
    "polier_stopwords = ['mois', 'janvier', 'fevrier', 'mars', 'avril', 'mai', 'juin', 'juillet', 'aout', 'septembre', 'octobre', 'novembre', 'decembre', 'xbre',\n",
    "                    'jour', 'jours', 'lundi', 'mardi', 'mercredi', 'jeudi', 'vendredi', 'samedi', 'dimanche',\n",
    "                    'lun', 'mar', 'mer', 'jeu', 'ven', 'sam', 'dim',\n",
    "                    'mr', 'mrs', 'mrs', 'mde', 'mle', 'mlle', 'monsieur', 'mr de', 'lle', \n",
    "                    'pr', 'ent' ,'ue', 'aver', 'ume', 'quil', 'j.', 'ler', 'trei', 'moitie', 'alle', 'aupres', 'dela', 'l.',\n",
    "                    'ave', 'pl', 'vi', 'pes', 'ls', 'gen', 'ilne', 'sen', 'ine', 'cen', 'ter', 'bl', 'por',\n",
    "                    'rep', 'iel', 'aue', 'ac', 'sl', 'sr', 'oue', 'lod', 'jer', 'vez', 'avc', \"il'\", 'lan',\n",
    "                    'sep', 'mos', 'pri', 'ds', 'sun', 'lan', 'pai', 'tl', 'gre', 'dre', 'pt', 'oir', 'sup',\n",
    "                    'apras', 'ps', 'vl', 'sgn', 'chz', 'bt', 'tou', 'lour', 'rom', 'sonr', 'ca', 'mad', 'ux', 'mout',\n",
    "                    'jur', 'sg', 'avi', 'lel', 'dat', 'oue', 'ace', 'gl', 'voux', 'ort', 'ther', 'quime', 'coy', 'mo',\n",
    "                    'cla', 'dev', 'ily', '& mr', 'pui', 'men', 'mid', 'jou', 'mame', 'doit', 'mo', 'cla']\n",
    "\n",
    "misc_stopwords = ['iseli', 'hertig', 'cassat', 'moudon', 'tolozan', 'etablit', 'courlat', 'abert', 'vullyamoz', 'porchet']\n",
    "\n",
    "stopwords = stop_words + org_stopwords + polier_stopwords\n",
    "\n",
    "\n",
    "# Récupérer tous les fichiers commençant par \"transcr_\" dans le répertoire donné\n",
    "files = glob.glob('/Users/tita/Desktop/Memoire_local/analyse/polier_data/transcr_*.json')\n",
    "\n",
    "# Initialiser des dictionnaires pour stocker les fréquences des entités nommées et les subjectivités par entité nommée\n",
    "entity_frequencies = {}\n",
    "entity_subjectivities = {}\n",
    "\n",
    "# Boucler sur chaque fichier et extraire les entités nommées et les subjectivités de chaque document pour analyse avec Spacy et TextBlob\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        for document in data:\n",
    "            doc = nlp_fr(document['transcription'])\n",
    "            blob = TextBlob(document['transcription'])\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ == 'MISC':\n",
    "                    entity = ent.text.lower()\n",
    "                    if entity not in entity_frequencies and entity not in stopwords:\n",
    "                        entity_frequencies[entity] = 0\n",
    "                        entity_subjectivities[entity] = 0\n",
    "                    entity_frequencies[entity] += 1\n",
    "                    entity_subjectivities[entity] += blob.sentiment.subjectivity\n",
    "\n",
    "# Calculer la moyenne des subjectivités pour chaque entité nommée avec au moins 5 occurrences\n",
    "entity_subjectivities_avg = {}\n",
    "for entity, frequency in entity_frequencies.items():\n",
    "    if frequency >= 5:\n",
    "        entity_subjectivities_avg[entity] = entity_subjectivities[entity] / frequency\n",
    "\n",
    "# Récupérer les entités avec leur moyenne de subjectivité triées par ordre décroissant\n",
    "sorted_entities = sorted(entity_subjectivities_avg.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Imprimer les 10 entités les plus subjectives\n",
    "print(\"Les entités diverses les plus subjectives :\")\n",
    "for entity, subjectivity in sorted_entities[:10]:\n",
    "    print(entity, subjectivity)\n",
    "\n",
    "print('\\n')  \n",
    "\n",
    "# Imprimer les 10 entités les moins subjectives\n",
    "print(\"Les entités diverses les moins subjectives :\")\n",
    "for entity, subjectivity in sorted_entities[-10:]:\n",
    "    print(entity, subjectivity)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "141acc77",
   "metadata": {},
   "source": [
    "# LDA  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e493696",
   "metadata": {},
   "source": [
    "Le code 2.2. sélectionne les 20 mots les plus fréquents dans l'ensemble des documents, puis utilise ces mots pour créer la matrice termes-document.\n",
    "\n",
    "Le deuxième code LDA est un algorithme LDA amélioré qui utilise une méthode de régularisation pour améliorer la précision de la modélisation des thèmes. Cette méthode de régularisation consiste à ajouter une pénalité pour les paramètres qui varient trop par rapport à leur valeur moyenne. Cela permet de réduire le surajustement et d'améliorer la capacité de généralisation du modèle.\n",
    "\n",
    "Dans le code 2.2., seuls les mots qui apparaissent au moins trois fois dans les documents et qui ne sont pas dans les 20 mots les plus fréquents sont sélectionnés pour la modélisation LDA. Cette étape de sélection permet de réduire la dimensionnalité de la représentation vectorielle des documents, ce qui peut améliorer les performances de la modélisation LDA. \n",
    "\n",
    "En outre, le deuxième code utilise des vecteurs one-hot pour représenter chaque document, tandis que le premier code (2.1.) utilise des vecteurs de fréquence de termes (TF). Les vecteurs one-hot sont utiles pour des modèles d'apprentissage automatique comme LDA qui n'ont pas besoin de la fréquence des termes, mais seulement de leur présence ou de leur absence dans un document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b17208a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d5fc8ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement du fichier 1/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_177.json\n",
      "Traitement du fichier 2/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_032.json\n",
      "Traitement du fichier 3/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_120.json\n",
      "Traitement du fichier 4/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_065.json\n",
      "Traitement du fichier 5/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_136.json\n",
      "Traitement du fichier 6/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_073.json\n",
      "Traitement du fichier 7/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_161.json\n",
      "Traitement du fichier 8/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_024.json\n",
      "Traitement du fichier 9/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_008.json\n",
      "Traitement du fichier 10/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_049.json\n",
      "Traitement du fichier 11/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_086.json\n",
      "Traitement du fichier 12/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_069.json\n",
      "Traitement du fichier 13/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_090.json\n",
      "Traitement du fichier 14/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_028.json\n",
      "Traitement du fichier 15/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_182.json\n",
      "Traitement du fichier 16/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_053.json\n",
      "Traitement du fichier 17/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_116.json\n",
      "Traitement du fichier 18/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_004.json\n",
      "Traitement du fichier 19/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_141.json\n",
      "Traitement du fichier 20/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_012.json\n",
      "Traitement du fichier 21/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_157.json\n",
      "Traitement du fichier 22/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_045.json\n",
      "Traitement du fichier 23/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_100.json\n",
      "Traitement du fichier 24/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_044.json\n",
      "Traitement du fichier 25/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_101.json\n",
      "Traitement du fichier 26/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_013.json\n",
      "Traitement du fichier 27/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_156.json\n",
      "Traitement du fichier 28/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_005.json\n",
      "Traitement du fichier 29/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_140.json\n",
      "Traitement du fichier 30/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_052.json\n",
      "Traitement du fichier 31/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_117.json\n",
      "Traitement du fichier 32/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_029.json\n",
      "Traitement du fichier 33/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_183.json\n",
      "Traitement du fichier 34/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_091.json\n",
      "Traitement du fichier 35/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_087.json\n",
      "Traitement du fichier 36/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_068.json\n",
      "Traitement du fichier 37/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_048.json\n",
      "Traitement du fichier 38/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_009.json\n",
      "Traitement du fichier 39/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_160.json\n",
      "Traitement du fichier 40/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_025.json\n",
      "Traitement du fichier 41/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_137.json\n",
      "Traitement du fichier 42/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_072.json\n",
      "Traitement du fichier 43/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_121.json\n",
      "Traitement du fichier 44/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_064.json\n",
      "Traitement du fichier 45/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_176.json\n",
      "Traitement du fichier 46/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_033.json\n",
      "Traitement du fichier 47/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_038.json\n",
      "Traitement du fichier 48/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_080.json\n",
      "Traitement du fichier 49/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_096.json\n",
      "Traitement du fichier 50/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_079.json\n",
      "Traitement du fichier 51/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_184.json\n",
      "Traitement du fichier 52/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_055.json\n",
      "Traitement du fichier 53/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_110.json\n",
      "Traitement du fichier 54/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_002.json\n",
      "Traitement du fichier 55/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_147.json\n",
      "Traitement du fichier 56/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_014.json\n",
      "Traitement du fichier 57/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_151.json\n",
      "Traitement du fichier 58/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_043.json\n",
      "Traitement du fichier 59/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_106.json\n",
      "Traitement du fichier 60/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_171.json\n",
      "Traitement du fichier 61/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_034.json\n",
      "Traitement du fichier 62/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_126.json\n",
      "Traitement du fichier 63/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_063.json\n",
      "Traitement du fichier 64/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_130.json\n",
      "Traitement du fichier 65/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_075.json\n",
      "Traitement du fichier 66/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_167.json\n",
      "Traitement du fichier 67/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_022.json\n",
      "Traitement du fichier 68/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_059.json\n",
      "Traitement du fichier 69/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_018.json\n",
      "Traitement du fichier 70/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_019.json\n",
      "Traitement du fichier 71/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_058.json\n",
      "Traitement du fichier 72/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_166.json\n",
      "Traitement du fichier 73/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_023.json\n",
      "Traitement du fichier 74/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_131.json\n",
      "Traitement du fichier 75/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_074.json\n",
      "Traitement du fichier 76/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_127.json\n",
      "Traitement du fichier 77/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_062.json\n",
      "Traitement du fichier 78/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_170.json\n",
      "Traitement du fichier 79/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_035.json\n",
      "Traitement du fichier 80/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_042.json\n",
      "Traitement du fichier 81/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_107.json\n",
      "Traitement du fichier 82/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_015.json\n",
      "Traitement du fichier 83/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_150.json\n",
      "Traitement du fichier 84/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_003.json\n",
      "Traitement du fichier 85/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_146.json\n",
      "Traitement du fichier 86/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_054.json\n",
      "Traitement du fichier 87/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_111.json\n",
      "Traitement du fichier 88/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_185.json\n",
      "Traitement du fichier 89/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_097.json\n",
      "Traitement du fichier 90/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_078.json\n",
      "Traitement du fichier 91/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_081.json\n",
      "Traitement du fichier 92/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_039.json\n",
      "Traitement du fichier 93/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_104.json\n",
      "Traitement du fichier 94/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_041.json\n",
      "Traitement du fichier 95/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_153.json\n",
      "Traitement du fichier 96/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_016.json\n",
      "Traitement du fichier 97/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_145.json\n",
      "Traitement du fichier 98/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_112.json\n",
      "Traitement du fichier 99/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_057.json\n",
      "Traitement du fichier 100/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_186.json\n",
      "Traitement du fichier 101/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_169.json\n",
      "Traitement du fichier 102/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_094.json\n",
      "Traitement du fichier 103/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_128.json\n",
      "Traitement du fichier 104/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_082.json\n",
      "Traitement du fichier 105/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_108.json\n",
      "Traitement du fichier 106/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_149.json\n",
      "Traitement du fichier 107/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_165.json\n",
      "Traitement du fichier 108/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_077.json\n",
      "Traitement du fichier 109/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_132.json\n",
      "Traitement du fichier 110/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_098.json\n",
      "Traitement du fichier 111/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_061.json\n",
      "Traitement du fichier 112/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_124.json\n",
      "Traitement du fichier 113/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_036.json\n",
      "Traitement du fichier 114/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_173.json\n",
      "Traitement du fichier 115/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_037.json\n",
      "Traitement du fichier 116/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_172.json\n",
      "Traitement du fichier 117/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_060.json\n",
      "Traitement du fichier 118/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_125.json\n",
      "Traitement du fichier 119/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_076.json\n",
      "Traitement du fichier 120/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_133.json\n",
      "Traitement du fichier 121/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_099.json\n",
      "Traitement du fichier 122/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_021.json\n",
      "Traitement du fichier 123/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_164.json\n",
      "Traitement du fichier 124/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_148.json\n",
      "Traitement du fichier 125/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_109.json\n",
      "Traitement du fichier 126/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_129.json\n",
      "Traitement du fichier 127/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_083.json\n",
      "Traitement du fichier 128/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_095.json\n",
      "Traitement du fichier 129/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_168.json\n",
      "Traitement du fichier 130/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_113.json\n",
      "Traitement du fichier 131/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_056.json\n",
      "Traitement du fichier 132/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_144.json\n",
      "Traitement du fichier 133/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_001.json\n",
      "Traitement du fichier 134/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_152.json\n",
      "Traitement du fichier 135/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_017.json\n",
      "Traitement du fichier 136/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_105.json\n",
      "Traitement du fichier 137/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_040.json\n",
      "Traitement du fichier 138/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_159.json\n",
      "Traitement du fichier 139/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_118.json\n",
      "Traitement du fichier 140/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_026.json\n",
      "Traitement du fichier 141/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_163.json\n",
      "Traitement du fichier 142/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_071.json\n",
      "Traitement du fichier 143/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_134.json\n",
      "Traitement du fichier 144/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_067.json\n",
      "Traitement du fichier 145/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_088.json\n",
      "Traitement du fichier 146/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_122.json\n",
      "Traitement du fichier 147/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_030.json\n",
      "Traitement du fichier 148/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_175.json\n",
      "Traitement du fichier 149/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_102.json\n",
      "Traitement du fichier 150/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_047.json\n",
      "Traitement du fichier 151/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_155.json\n",
      "Traitement du fichier 152/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_010.json\n",
      "Traitement du fichier 153/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_143.json\n",
      "Traitement du fichier 154/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_006.json\n",
      "Traitement du fichier 155/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_114.json\n",
      "Traitement du fichier 156/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_051.json\n",
      "Traitement du fichier 157/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_180.json\n",
      "Traitement du fichier 158/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_092.json\n",
      "Traitement du fichier 159/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_138.json\n",
      "Traitement du fichier 160/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_084.json\n",
      "Traitement du fichier 161/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_179.json\n",
      "Traitement du fichier 162/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_178.json\n",
      "Traitement du fichier 163/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_085.json\n",
      "Traitement du fichier 164/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_093.json\n",
      "Traitement du fichier 165/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_139.json\n",
      "Traitement du fichier 166/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_181.json\n",
      "Traitement du fichier 167/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_115.json\n",
      "Traitement du fichier 168/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_050.json\n",
      "Traitement du fichier 169/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_142.json\n",
      "Traitement du fichier 170/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_007.json\n",
      "Traitement du fichier 171/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_154.json\n",
      "Traitement du fichier 172/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_011.json\n",
      "Traitement du fichier 173/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_103.json\n",
      "Traitement du fichier 174/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_046.json\n",
      "Traitement du fichier 175/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_031.json\n",
      "Traitement du fichier 176/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_174.json\n",
      "Traitement du fichier 177/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_066.json\n",
      "Traitement du fichier 178/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_089.json\n",
      "Traitement du fichier 179/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_123.json\n",
      "Traitement du fichier 180/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_070.json\n",
      "Traitement du fichier 181/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_135.json\n",
      "Traitement du fichier 182/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_027.json\n",
      "Traitement du fichier 183/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_162.json\n",
      "Traitement du fichier 184/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_119.json\n",
      "Traitement du fichier 185/185: /Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_158.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tita/miniforge3/envs/htr/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1380: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-14 {color: black;background-color: white;}#sk-container-id-14 pre{padding: 0;}#sk-container-id-14 div.sk-toggleable {background-color: white;}#sk-container-id-14 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-14 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-14 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-14 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-14 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-14 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-14 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-14 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-14 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-14 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-14 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-14 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-14 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-14 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-14 div.sk-item {position: relative;z-index: 1;}#sk-container-id-14 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-14 div.sk-item::before, #sk-container-id-14 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-14 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-14 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-14 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-14 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-14 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-14 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-14 div.sk-label-container {text-align: center;}#sk-container-id-14 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-14 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-14\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(max_iter=20, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" checked><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(max_iter=20, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LatentDirichletAllocation(max_iter=20, random_state=42)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Charger le modèle de traitement du langage naturel pour le français\n",
    "nlp_fr = spacy.load('fr_core_news_lg')\n",
    "\n",
    "# Récupérer tous les fichiers commençant par \"transcr_\" dans le répertoire donné\n",
    "files = glob.glob('/Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_*.json')\n",
    "\n",
    "# Boucler sur chaque fichier et extraire le texte de chaque document pour analyse avec Spacy\n",
    "documents, tokens = [], []\n",
    "\n",
    "for i, file in enumerate(files):\n",
    "    print(f\"Traitement du fichier {i+1}/{len(files)}: {file}\")\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        for document in data:\n",
    "            doc = nlp_fr(document['transcription'])\n",
    "            words = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha and len(token.lemma_) > 2]\n",
    "            tokens += words\n",
    "            documents.append(words)\n",
    "\n",
    "# Créer une liste d'arrêts en français\n",
    "french_stopwords = [\"le\", \"la\" ,'l', \"les\", \"de\", \"du\", \"des\", \"un\", \"une\", \"et\", \"ou\", \"car\", \"par\", \"pour\", \"avec\", \"sur\", \"meme\"]\n",
    "\n",
    "# Ajouter des mots spécifiques à la variable 'polier'\n",
    "polier_stopwords = ['mois', 'janvier', 'fevrier', 'mars', 'avril', 'mai', 'juin', 'juillet', 'aout', 'septembre', 'octobre', 'novembre', 'decembre', 'xbre',\n",
    "                    'jour', 'jours', 'lundi', 'mardi', 'mercredi', 'jeudi', 'vendredi', 'samedi', 'dimanche',\n",
    "                    'lun', 'mar', 'mer', 'jeu', 'ven', 'sam', 'dim',\n",
    "                    'jeanne', 'pierre', 'mrs', 'mrs', 'monsieur', 'michoud', 'andre', 'sieur', 'aud'\n",
    "                    'non', 'trop', 'petit', 'voir', 'boul', 'juat', 'con', 'voir', 'vour', 'ment', 'ent', \n",
    "                    'hela', 'quide', 'allol', 'anog', 'ivine', 'oblet', 'queli', 'lco', 'dlul', 'flir', 'brod',\n",
    "                    'teal', 'etuv', 'itit', 'dito', 'tit', 'heure', 'char', 'iseli', 'din', 'seli', 'non', 'tion', 'party',\n",
    "                    'bout', 'ous', 'tavel', 'neuf', 'mlle', 'tem', 'rien', 'muid', 'bout', 'sout']\n",
    "\n",
    "# Concaténer les deux listes pour créer la liste complète des mots à supprimer\n",
    "stopwords = french_stopwords + polier_stopwords\n",
    "\n",
    "# Sélectionner le vocabulaire à utiliser pour LDA\n",
    "vocabulaire = pd.Series(tokens).value_counts()[10:]\n",
    "vocabulaire = vocabulaire[vocabulaire >= 5].sort_index().keys()\n",
    "\n",
    "# Convertir les données en vecteurs pour LDA\n",
    "count_vectorizer = CountVectorizer(stop_words=stopwords, vocabulary=vocabulaire)\n",
    "X = count_vectorizer.fit_transform([' '.join(doc) for doc in documents])\n",
    "\n",
    "\n",
    "# Exécuter LDA sur les données\n",
    "lda_model = LatentDirichletAllocation(n_components=10, max_iter=20, random_state=42)\n",
    "lda_model.fit(X)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47a2ed96",
   "metadata": {},
   "source": [
    "## Affichage des résultats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3023dfee",
   "metadata": {},
   "source": [
    "Dans ce code, la différence entre \"mots les plus fréquents\" et \"mots les plus caractéristiques\" est la suivante :\n",
    "\n",
    "\"Mots les plus fréquents\" correspond aux mots les plus souvent utilisés dans le cluster considéré, c'est-à-dire le nombre de fois que chaque mot apparaît dans le cluster. Cette mesure permet de donner une idée générale des sujets abordés dans le cluster.\n",
    "\n",
    "\"Mots les plus caractéristiques\", quant à eux, correspondent aux mots qui permettent de distinguer le plus le cluster considéré des autres clusters. Ces mots sont donc les plus spécifiques au cluster en question. Cette mesure permet de mieux comprendre le sujet principal du cluster, en se concentrant sur les mots qui le caractérisent le mieux.\n",
    "\n",
    "En résumé, les \"mots les plus fréquents\" donnent une idée générale des sujets abordés dans le cluster, tandis que les \"mots les plus caractéristiques\" permettent de mieux comprendre le sujet principal du cluster en se concentrant sur les mots qui le distinguent le plus des autres clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "61d0edc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les mots les plus représentatifs du topic #0 sont : sol soleil livre bar beau quittance girouette crutz recu payer thermometre batz demi therm bise vent pluie paye grand barometre\n",
      "Les mots les plus représentatifs du topic #1 sont : bailli bar chateau venir fort thermometre calme demande soleil bise vouloir matin soir conseil billet girouette vent porter beau pied\n",
      "Les mots les plus représentatifs du topic #2 sont : pot vin lausanne septier rouge mont signe allaman blanc vigneron vigne fuste brantee vernand romand partisseur bon prix recu vendange\n",
      "Les mots les plus représentatifs du topic #3 sont : fol interet tuteur debiteur int solde somme quittance porte gramme payer lieu echu capital rente sol jusque the billet article\n",
      "Les mots les plus représentatifs du topic #4 sont : lausanne ville seigneur donne date bailli honneur berne chambre conseil enfant donner seigneurie avoir consistoire signe bon ordre contre sujet\n",
      "Les mots les plus représentatifs du topic #5 sont : contre femme fille enfant francois mandat demande bailli pasteur consistoire marier tuteur vouloir veuve fils mariage sentence lausanne secretan rapport\n",
      "Les mots les plus représentatifs du topic #6 sont : grand homme être venir mort femme roi mal enfant bon age pari fort donne pied mourir prendre beaucoup monde vouloir\n",
      "Les mots les plus représentatifs du topic #7 sont : porter sac batz caisse beau bar ecu gros soleil francois bailli gramme matin debit fort messel bise venir froment payer\n",
      "Les mots les plus représentatifs du topic #8 sont : homme loi roi grand vertu esprit ame peuple mettre pari etat pouvoir vie jamais droit und être falloir der dieu\n",
      "Les mots les plus représentatifs du topic #9 sont : avoir grand bon beaucoup fort prendre honneur prier donner vouloir être pouvoir cher frere venir savoir passer nouveau affaire mettre\n",
      "\n",
      "\n",
      "Les mots les plus fréquents du topic #0 sont : sol soleil livre bar beau quittance girouette crutz recu payer thermometre batz demi therm bise vent pluie paye grand barometre\n",
      "Les mots les plus fréquents du topic #1 sont : bailli bar chateau venir fort thermometre calme demande soleil bise vouloir matin soir conseil billet girouette vent porter beau pied\n",
      "Les mots les plus fréquents du topic #2 sont : pot vin lausanne septier rouge mont signe allaman blanc vigneron vigne fuste brantee vernand romand partisseur bon prix recu vendange\n",
      "Les mots les plus fréquents du topic #3 sont : fol interet tuteur debiteur int solde somme quittance porte gramme payer lieu echu capital rente sol jusque the billet article\n",
      "Les mots les plus fréquents du topic #4 sont : lausanne ville seigneur donne date bailli honneur berne chambre conseil enfant donner seigneurie avoir consistoire signe bon ordre contre sujet\n",
      "Les mots les plus fréquents du topic #5 sont : contre femme fille enfant francois mandat demande bailli pasteur consistoire marier tuteur vouloir veuve fils mariage sentence lausanne secretan rapport\n",
      "Les mots les plus fréquents du topic #6 sont : grand homme être venir mort femme roi mal enfant bon age pari fort donne pied mourir prendre beaucoup monde vouloir\n",
      "Les mots les plus fréquents du topic #7 sont : porter sac batz caisse beau bar ecu gros soleil francois bailli gramme matin debit fort messel bise venir froment payer\n",
      "Les mots les plus fréquents du topic #8 sont : homme loi roi grand vertu esprit ame peuple mettre pari etat pouvoir vie jamais droit und être falloir der dieu\n",
      "Les mots les plus fréquents du topic #9 sont : avoir grand bon beaucoup fort prendre honneur prier donner vouloir être pouvoir cher frere venir savoir passer nouveau affaire mettre\n",
      "\n",
      "\n",
      "Les mots les plus caractéristiques du topic #0 sont : fiche rober ditto etrenner vhif jaye monrond hativ etouper sechi soutire delechat cretenou gillairone blancherd filature rapor refacture reversi raimondi (fréquence : [785, 726, 204, 120, 86, 77, 70, 63, 61, 59, 57, 50, 48, 46, 41, 40, 38, 36, 35, 34])\n",
      "Les mots les plus caractéristiques du topic #1 sont : monet lrof alarie dessoin dansoit gnet plmye veuvr mosset reaumur pochet tberm meute cesjat prodon montherand rancon control soen lisabetz (fréquence : [16, 16, 14, 13, 12, 12, 11, 10, 10, 10, 9, 9, 9, 9, 8, 8, 8, 8, 8, 8])\n",
      "Les mots les plus caractéristiques du topic #2 sont : brantee setier septz pressure belossette contigny entonne romaud tetier chantemerle poupedance bossete soptier rochet cocogn servaguin meillard brulerer micard gneron (fréquence : [364, 199, 42, 39, 29, 22, 20, 20, 18, 17, 15, 15, 15, 15, 14, 13, 12, 11, 11, 11])\n",
      "Les mots les plus caractéristiques du topic #3 sont : prorata rorata prorate livance livrances vecette ith duflon porata their borata pebiteur hich lirance lacteur schelling ofthe intl this intn (fréquence : [141, 46, 27, 25, 21, 20, 20, 19, 17, 16, 16, 15, 14, 14, 13, 13, 13, 13, 12, 12])\n",
      "Les mots les plus caractéristiques du topic #4 sont : mustre amiablement favrot bavoz allog vuez titz recommandon llustre requerer titl alloc toriale requis amia seigneuri alol editale sesgneur repuliqu (fréquence : [185, 127, 68, 63, 57, 39, 39, 34, 28, 25, 24, 22, 22, 20, 19, 18, 17, 17, 16, 16])\n",
      "Les mots les plus caractéristiques du topic #5 sont : interpellee roseng hertel borgognon ducrot lmol interrogat bovard poterat coinsin sauthey schmid anneler acusation juillard boucherler haldi crotaz chervet hotz (fréquence : [66, 50, 39, 38, 31, 31, 31, 28, 27, 26, 25, 24, 23, 23, 21, 20, 20, 19, 19, 19])\n",
      "Les mots les plus caractéristiques du topic #6 sont : dependance croitre vicaire alienation vitesse fertile corneille eclater prevoir perfide serpent desole raconter superstitieux dogme recompence heureusemen elegant pement boussole (fréquence : [16, 15, 14, 14, 13, 11, 11, 11, 11, 11, 10, 10, 10, 10, 10, 11, 9, 9, 9, 9])\n",
      "Les mots les plus caractéristiques du topic #7 sont : vucheren voice cramp dixm horrid pevret martiqu quartz vuchereus dulignage pendeau charette piedr depecement repeated thermt amenoit again heli girt (fréquence : [241, 41, 40, 37, 34, 33, 30, 29, 24, 24, 24, 24, 23, 21, 19, 19, 18, 18, 18, 18])\n",
      "Les mots les plus caractéristiques du topic #8 sont : und morale genie empire richesse haben sich sublime luch lassen auch bienfaisance bevor autel nach euch datum univers monarque landvogt (fréquence : [390, 117, 109, 108, 104, 73, 61, 55, 52, 51, 48, 47, 42, 42, 40, 38, 37, 37, 37, 34])\n",
      "Les mots les plus caractéristiques du topic #9 sont : moncher charbonne grainaison eumer affectueux nagell empresses affectueusement merveiller rassurer rejoui heureu regrer cousinl desperer arden parasite nielle plimen niell (fréquence : [56, 51, 49, 47, 40, 25, 25, 22, 20, 20, 19, 19, 17, 16, 14, 14, 14, 13, 13, 13])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Afficher les mots les plus représentatifs de chaque cluster\n",
    "feature_names = np.array(list(count_vectorizer.vocabulary_.keys()))\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    top_words = [feature_names[i] for i in topic.argsort()[:-21:-1]]\n",
    "    #print(f\"Topic #{topic_idx}: {' '.join(top_words)}\")\n",
    "    print(f\"Les mots les plus représentatifs du topic #{topic_idx} sont : {' '.join(top_words)}\")\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# Afficher les mots les plus fréquents et caractéristiques pour chaque cluster\n",
    "vocabulaire = np.array(list(count_vectorizer.vocabulary_.keys()))\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    # Mots les plus fréquents\n",
    "    top_freq_words = [vocabulaire[i] for i in np.flip(np.argsort(topic))[:20]]\n",
    "    print(f\"Les mots les plus fréquents du topic #{topic_idx} sont : {' '.join(top_freq_words)}\")\n",
    "\n",
    "print('\\n')  \n",
    "\n",
    "# Mots les plus caractéristiques\n",
    "type_attribution = lda_model.transform(np.identity(len(vocabulaire)))\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    top_charac_words = [(vocabulaire[i], int(np.sum(X[:, i]))) for i in np.flip(np.argsort(type_attribution[:,topic_idx]))[:20]]\n",
    "    print(f\"Les mots les plus caractéristiques du topic #{topic_idx} sont : {' '.join([word[0] for word in top_charac_words])} (fréquence : {[word[1] for word in top_charac_words]})\")\n",
    "\n",
    "\n",
    "print('\\n') "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d6da51a",
   "metadata": {},
   "source": [
    "# Nuage de mots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eb5020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2e65797",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[39mreturn\u001b[39;00m wordcloud\n\u001b[1;32m      8\u001b[0m \u001b[39m# Afficher les mots les plus représentatifs de chaque cluster\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m feature_names \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(\u001b[39mlist\u001b[39m(count_vectorizer\u001b[39m.\u001b[39mvocabulary_\u001b[39m.\u001b[39mkeys()))\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m topic_idx, topic \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(lda_model\u001b[39m.\u001b[39mcomponents_):\n\u001b[1;32m     11\u001b[0m     top_words \u001b[39m=\u001b[39m [feature_names[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m topic\u001b[39m.\u001b[39margsort()[:\u001b[39m-\u001b[39m\u001b[39m51\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Définir la fonction pour créer un nuage de mots\n",
    "def create_wordcloud(text):\n",
    "    wordcloud = WordCloud(width = 800, height = 800, \n",
    "                background_color ='white', \n",
    "                min_font_size = 10).generate(text) \n",
    "    return wordcloud\n",
    "\n",
    "# Afficher les mots les plus représentatifs de chaque cluster\n",
    "feature_names = np.array(list(count_vectorizer.vocabulary_.keys()))\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    top_words = [feature_names[i] for i in topic.argsort()[:-51:-1]]\n",
    "    \n",
    "    # Créer un nuage de mots pour les mots les plus représentatifs\n",
    "    wordcloud = create_wordcloud(' '.join(top_words))\n",
    "    plt.figure(figsize = (8, 8), facecolor = None) \n",
    "    plt.imshow(wordcloud) \n",
    "    plt.axis(\"off\") \n",
    "    plt.tight_layout(pad = 0) \n",
    "    \n",
    "    # Enregistrer le nuage de mots\n",
    "    plt.savefig(f\"nuage_de_mots_topic_{topic_idx}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8cfd6b1e",
   "metadata": {},
   "source": [
    "## Consistoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bbc41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle de traitement du langage naturel pour le français\n",
    "nlp_fr = spacy.load('fr_core_news_lg')\n",
    "\n",
    "# Ajouter une liste de stopwords personnalisée\n",
    "custom_stop_words = ['mr', 'pr', 'ete']\n",
    "for w in custom_stop_words:\n",
    "    nlp_fr.vocab[w].is_stop = True\n",
    "\n",
    "# Ajouter la liste de stopwords par défaut de Spacy pour le français\n",
    "nlp_fr.Defaults.stop_words |= {'mot4', 'mot5', 'mot6'}\n",
    "\n",
    "# Définir l'entité à rechercher\n",
    "search_entity = \"consistoire\"\n",
    "\n",
    "# Initialiser un dictionnaire pour stocker les co-occurrences et leur fréquence\n",
    "co_occurrences = {}\n",
    "\n",
    "# Récupérer tous les fichiers commençant par \"transcr_\" dans le répertoire donné\n",
    "files = glob.glob('/Users/tita/Desktop/Memoire_local/analyse/data/polier_data/transcr_*.json')\n",
    "\n",
    "# Boucler sur chaque fichier et extraire le texte de chaque document pour analyse avec Spacy\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        for document in data:\n",
    "            doc = nlp_fr(document['transcription'])\n",
    "            for token in doc:\n",
    "                if token.text == search_entity:\n",
    "                    for word in doc:\n",
    "                        if word.is_alpha and not word.is_stop and word.text != search_entity:\n",
    "                            if word.text in co_occurrences:\n",
    "                                co_occurrences[word.text] += 1\n",
    "                            else:\n",
    "                                co_occurrences[word.text] = 1\n",
    "\n",
    "# Trier les co-occurrences par fréquence décroissante\n",
    "sorted_co_occurrences = sorted(co_occurrences.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Afficher les 10 co-occurrences les plus significatives\n",
    "for word, frequency in sorted_co_occurrences[:20]:\n",
    "    print(word, frequency)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
