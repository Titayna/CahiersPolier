{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install opencv-python tqdm glob2 numpy Pillow scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree as letree\n",
    "import cv2\n",
    "import tqdm\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "from scipy.ndimage import rotate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for folder in sorted(glob.glob('export_job_*/*/')):\n",
    "    for path in sorted(glob.glob(f'{folder}CH_ACV_P_RENE_MONOD_*/*.jpg')):\n",
    "\n",
    "        # ICI METTRE TOUT LE RESTE DU CODE (AVEC INDENTATION) => y compris les partitions (cellule suivante)\n",
    "\n",
    "    cahier_nom = path.split('/')[-1]\n",
    "\n",
    "    os.environ('mv -v BenthamDatasetR0-GT handwritten_text_recognition/raw/bentham/)\n",
    "    os.environ('python handwritten_text_recognition/src/main.py --source=bentham --transform')\n",
    "    os.environ(f'mv -v handwritten_text_recognition/data/bentham.hdf5 {cahier_nom}.hdf5')\n",
    "    os.environ('rm -rf handwritten_text_recognition/raw/bentham/BenthamDatasetR0-GT')\n",
    "'''\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline2box(pts, up: int = 20, down: int = 5, margin_l: int = 5, margin_r: int = 5):\n",
    "    min_y = np.min(pts[:, 0])-margin_l\n",
    "    max_y = np.max(pts[:, 0])+margin_r\n",
    "    min_x = np.min(pts[:, 1])-up\n",
    "    max_x = np.max(pts[:, 1])+down\n",
    "\n",
    "    return min_y, max_y, min_x, max_x\n",
    "\n",
    "\n",
    "def checkCoords(min_x, max_x, min_y, max_y):\n",
    "    return int(np.max([min_x, 0])), max_x, int(np.max([min_y, 0])), max_y\n",
    "\n",
    "\n",
    "def filterImage(gray_image):\n",
    "    fimage = cv2.bilateralFilter(gray_image, 7, 50, 50)\n",
    "    value = np.ravel(fimage)\n",
    "    paper, ink = np.percentile(value, 95), np.percentile(value, 5)\n",
    "\n",
    "    white_fimage = (fimage-ink)/(paper-ink)*255\n",
    "    white_fimage[white_fimage > 255] = 255\n",
    "    white_fimage[white_fimage < 1] = 0\n",
    "    white_fimage = white_fimage.astype('uint8')\n",
    "\n",
    "    return white_fimage\n",
    "\n",
    "\n",
    "def formatList(list_):\n",
    "    string = ''\n",
    "    for item in list_:\n",
    "        string += f'{item}\\n'\n",
    "    return string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probleme 93-98 + 101 + 106 + 108 + 111 + 113 + 116 + 117 + 118 + 121 + 122 + 123 + 126 + 127 + 128 + 130 + 131 + 132 + 133 \n",
    "# + 139 + 144 + 148 + 149 + 150 + 157 + 158 + 159 + 160 + 161 + 162 + 163 + 166 + 180 + 171 + 174 + 176 + 177 + 180 + 186 \n",
    "# + 187 + 210 \n",
    "\n",
    "#fait : 93 94 - 98 - 108 111 113 116 117 118 121 122 123 126 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 123/401 [13:34<25:49,  5.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'IndexError'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 129/401 [14:11<25:24,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'IndexError'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 134/401 [14:39<22:09,  4.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'IndexError'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 151/401 [16:14<25:52,  6.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'IndexError'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 174/401 [19:09<32:18,  8.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'IndexError'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 401/401 [34:43<00:00,  5.20s/it]\n"
     ]
    }
   ],
   "source": [
    "pad = 60\n",
    "\n",
    "\n",
    "list_notebooks = []\n",
    "\n",
    "for path in tqdm.tqdm(sorted(glob.glob('Cahiers_Polier_Complet_XML/CH_ACV_P_RENE_MONOD_18*/*.jpg'))):\n",
    "        \n",
    "        # Rest the list_segments for each notebook\n",
    "    list_segments = []\n",
    "\n",
    "        # Load image and recover its name\n",
    "    image = filterImage(cv2.imread(path, 0))\n",
    "    image_name = path.split(\"/\")[-1][:-4]\n",
    "    notebook, page_n = image_name.split('_')[-1].split('-')\n",
    "\n",
    "        # Create directory for each notebook\n",
    "    os.makedirs(f'Bentham_{notebook}/BenthamDatasetR0-GT/Images/Lines/', exist_ok=True)\n",
    "    os.makedirs(f'Bentham_{notebook}/BenthamDatasetR0-GT/Partitions/', exist_ok=True)\n",
    "    os.makedirs(f'Bentham_{notebook}/BenthamDatasetR0-GT/Transcriptions/', exist_ok=True)\n",
    "\n",
    "        # A TESTER EN ENLEVANT\n",
    "        #list_notebooks.append(notebook)\n",
    "\n",
    "\n",
    "\n",
    "        # Read Transkribus XML output\n",
    "    tree = letree.parse(\n",
    "        f'{path.split(\"/CH_ACV\")[0]}/CH_ACV_P_RENE_MONOD_{notebook}/page/CH_ACV_P_RENE_MONOD_{notebook}-{page_n}.xml')\n",
    "    root = tree.getroot()\n",
    "    page = root.find(\n",
    "            '{http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15}Page')\n",
    "\n",
    "        # Iterate over text lines detected on the page\n",
    "    counter = 0\n",
    "    for text_region in page.iter('{http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15}TextRegion'):\n",
    "        for text_line in text_region.iter('{http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15}TextLine'):\n",
    "\n",
    "\n",
    "\n",
    "                # Transkribus 'box' coords\n",
    "                coords = text_line.find(\n",
    "                    '{http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15}Coords')\n",
    "                coords = [double.split(',')\n",
    "                          for double in coords.attrib['points'].split(' ')]\n",
    "                coords = np.array([(int(coord[0]), int(coord[1]))\n",
    "                                  for coord in coords]).astype('uint32')\n",
    "\n",
    "                # Transkribus baseline coords\n",
    "                baseline = text_line.find(\n",
    "                    '{http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15}Baseline')\n",
    "                if baseline is None:\n",
    "                    continue\n",
    "\n",
    "                baseline = [double.split(\n",
    "                    ',') for double in baseline.attrib['points'].split(' ')]\n",
    "                baseline = np.array([(int(coord[0]), int(coord[1]))\n",
    "                                    for coord in baseline]).astype('uint32')\n",
    "\n",
    "                # Extend the 'box'  coords\n",
    "                min_y, max_y, min_x, max_x = baseline2box(\n",
    "                    baseline, up=76, down=25, margin_l=18, margin_r=31)\n",
    "                min_yp, max_yp, min_xp, max_xp = checkCoords(\n",
    "                    min_y-pad, max_y+pad, min_x-pad, max_x+pad)\n",
    "\n",
    "                # Select the line region\n",
    "                segment = image[min_xp:max_xp, min_yp:max_yp]\n",
    "                mask = np.zeros(segment.shape[:2], np.uint8)\n",
    "\n",
    "                # Compute the line rotation angle\n",
    "                cv2.drawContours(\n",
    "                    mask, [coords - np.array([min_yp, min_xp])], -1, (255, 255, 255), -1, cv2.LINE_AA)\n",
    "                _, _, theta = cv2.minAreaRect(coords.astype('int'))\n",
    "                if theta > 30:\n",
    "                    theta = theta-90\n",
    "\n",
    "                # Compensate the rotation\n",
    "                rot_segment = rotate(segment, theta, cval=0)\n",
    "                rot_mask = rotate(mask, theta, cval=0).astype('bool')\n",
    "\n",
    "                # Compute the area of interest\n",
    "                try:\n",
    "                    min_xm, max_xm = (np.arange(rot_mask.shape[0])[(np.sum(rot_mask, 1)/rot_mask.shape[1]) > 0.1])[\n",
    "                        np.array([0, -1])]\n",
    "                    min_ym, max_ym = (np.arange(rot_mask.shape[1])[(np.sum(rot_mask, 0)/rot_mask.shape[0]) > 0.05])[\n",
    "                        np.array([0, -1])]\n",
    "                except IndexError:\n",
    "                    print (IndexError)\n",
    "                    continue\n",
    "\n",
    "                # quand inférence si ça fonctionne pas utiliser cette ligne ;\n",
    "                #text_segment = text_line.find(\n",
    "                #        '{http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15}TextEquiv')[0].text\n",
    "                #text_segment = text_segment.replace('é', 'e').replace('è', 'e').replace('ö', 'o').replace('î', 'i').replace('ï', 'i').replace('ê', 'e').replace('ù','u').replace('à','a').replace('ç','c').replace('ò','o').replace('û','u').replace('ä','a')\n",
    "                text_segment = 'sans transcription' \n",
    "\n",
    "                # Save transcription\n",
    "                with open(f'Bentham_{notebook}/BenthamDatasetR0-GT/Transcriptions/{notebook}_{page_n}_{counter}.txt', 'w+') as f:\n",
    "                    f.write(text_segment)\n",
    "\n",
    "                # Save segment image\n",
    "                cv2.imwrite(f'Bentham_{notebook}/BenthamDatasetR0-GT/Images/Lines/{notebook}_{page_n}_{counter}.png',\n",
    "                            rot_segment[max([0, min_xm-34]):max_xm+15, max([0, min_ym-18]):max_ym+10])\n",
    "                list_segments.append(f'{notebook}_{page_n}_{counter}')\n",
    "                counter += 1\n",
    "\n",
    "    val, test = len(list_segments)*0.1, len(list_segments)*0.2\n",
    "    random.shuffle(list_segments)\n",
    "\n",
    "\n",
    "    with open(f'Bentham_{notebook}/BenthamDatasetR0-GT/Partitions/TestLines.lst', 'a') as f:\n",
    "        f.write(formatList(list_segments))\n",
    "        f.close()\n",
    "    with open(f'Bentham_{notebook}/BenthamDatasetR0-GT/Partitions/TrainLines.lst', 'a') as f:\n",
    "        f.write(formatList(list_segments[1:4]))\n",
    "        f.close()\n",
    "    with open(f'Bentham_{notebook}/BenthamDatasetR0-GT/Partitions/ValidationLines.lst', 'a') as f:\n",
    "        f.write(formatList(list_segments[4:7]))\n",
    "        f.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "htr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "63e32e6b3511a617e6b7287de7541ff63597ad6f80bd10576ad165d760067551"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
