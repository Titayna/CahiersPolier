{"cells":[{"cell_type":"markdown","metadata":{"id":"gP-v0E_S-mQP"},"source":["\n","# Handwritten Text Recognition for the Polier notebook \n","\n","Based on Transkribus Reader and Handwritten Text Recognition using TensorFlow 2.x by arthurflor23 (https://github.com/arthurflor23/handwritten-text-recognition)\n"]},{"cell_type":"markdown","metadata":{"id":"oMty1YwuWHpN"},"source":["## 1 Localhost Environment\n","\n","We'll make sure you have the project in your Google Drive with the datasets in HDF5. If you already have structured files in the cloud, skip this step."]},{"cell_type":"markdown","metadata":{"id":"39blvPTPQJpt"},"source":["### 1.1 Datasets\n","\n","The datasets that you can use:\n","\n","a. [Bentham](http://www.transcriptorium.eu/~tsdata/)\n","\n","b. [IAM](http://www.fki.inf.unibe.ch/databases/iam-handwriting-database)\n","\n","c. [Rimes](http://www.a2ialab.com/doku.php?id=rimes_database:start)\n","\n","d. [Saint Gall](https://fki.tic.heia-fr.ch/databases/saint-gall-database)\n","\n","e. [Washington](https://fki.tic.heia-fr.ch/databases/washington-database)"]},{"cell_type":"markdown","metadata":{"id":"QVBGMLifWQwl"},"source":["### 1.2 Raw folder\n","\n","On localhost, download the code project from GitHub and extract the chosen dataset (or all if you prefer) in the **raw** folder. Don't change anything of the structure of the dataset, since the scripts were made from the **original structure** of them. Your project directory will be like this:\n","\n","```\n",".\n","├── raw\n","│   ├── bentham\n","│   │   ├── BenthamDatasetR0-GT\n","│   │   └── BenthamDatasetR0-Images\n","│   ├── iam\n","│   │   ├── ascii\n","│   │   ├── forms\n","│   │   ├── largeWriterIndependentTextLineRecognitionTask\n","│   │   ├── lines\n","│   │   └── xml\n","│   ├── rimes\n","│   │   ├── eval_2011\n","│   │   ├── eval_2011_annotated.xml\n","│   │   ├── training_2011\n","│   │   └── training_2011.xml\n","│   ├── saintgall\n","│   │   ├── data\n","│   │   ├── ground_truth\n","│   │   ├── README.txt\n","│   │   └── sets\n","│   └── washington\n","│       ├── data\n","│       ├── ground_truth\n","│       ├── README.txt\n","│       └── sets\n","└── src\n","    ├── data\n","    │   ├── evaluation.py\n","    │   ├── generator.py\n","    │   ├── preproc.py\n","    │   ├── reader.py\n","    │   ├── similar_error_analysis.py\n","    ├── main.py\n","    ├── network\n","    │   ├── architecture.py\n","    │   ├── layers.py\n","    │   ├── model.py\n","    └── tutorial.ipynb\n","\n","```\n","\n","After that, create virtual environment and install the dependencies with python 3 and pip:\n","\n","> ```python -m venv .venv && source .venv/bin/activate```\n","\n","> ```pip install -r requirements.txt```"]},{"cell_type":"markdown","metadata":{"id":"WyLRbAwsWSYA"},"source":["### 1.3 HDF5 files\n","\n","Now, you'll run the *transform* function from **main.py**. For this, execute on **src** folder:\n","\n","> ```python main.py --source=<DATASET_NAME> --transform```\n","\n","Your data will be preprocess and encode, creating and saving in the **data** folder. Now your project directory will be like this:\n","\n","\n","```\n",".\n","├── data\n","│   ├── bentham.hdf5\n","│   ├── iam.hdf5\n","│   ├── rimes.hdf5\n","│   ├── saintgall.hdf5\n","│   └── washington.hdf5\n","├── raw\n","│   ├── bentham\n","│   │   ├── BenthamDatasetR0-GT\n","│   │   └── BenthamDatasetR0-Images\n","│   ├── iam\n","│   │   ├── ascii\n","│   │   ├── forms\n","│   │   ├── largeWriterIndependentTextLineRecognitionTask\n","│   │   ├── lines\n","│   │   └── xml\n","│   ├── rimes\n","│   │   ├── eval_2011\n","│   │   ├── eval_2011_annotated.xml\n","│   │   ├── training_2011\n","│   │   └── training_2011.xml\n","│   ├── saintgall\n","│   │   ├── data\n","│   │   ├── ground_truth\n","│   │   ├── README.txt\n","│   │   └── sets\n","│   └── washington\n","│       ├── data\n","│       ├── ground_truth\n","│       ├── README.txt\n","│       └── sets\n","└── src\n","    ├── data\n","    │   ├── evaluation.py\n","    │   ├── generator.py\n","    │   ├── preproc.py\n","    │   ├── reader.py\n","    │   ├── similar_error_analysis.py\n","    ├── main.py\n","    ├── network\n","    │   ├── architecture.py\n","    │   ├── layers.py\n","    │   ├── model.py\n","    └── tutorial.ipynb\n","\n","```\n","\n","Then upload the **data** and **src** folders in the same directory in your Google Drive."]},{"cell_type":"markdown","metadata":{"id":"jydsAcWgWVth"},"source":["## 2 Google Drive Environment\n"]},{"cell_type":"markdown","metadata":{"id":"wk3e7YJiXzSl"},"source":["### 2.1 TensorFlow 2.x"]},{"cell_type":"markdown","metadata":{"id":"Z7twXyNGXtbJ"},"source":["Make sure the jupyter notebook is using GPU mode."]},{"cell_type":"markdown","source":[],"metadata":{"id":"prz9pIymPp0j"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"mHw4tODULT1Z"},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FMg-B5PH9h3r"},"outputs":[],"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","\n","device_name = tf.test.gpu_device_name()\n","\n","if device_name != \"/device:GPU:0\":\n","    raise SystemError(\"GPU device not found\")\n","\n","print(\"Found GPU at: {}\".format(device_name))"]},{"cell_type":"markdown","metadata":{"id":"FyMv5wyDXxqc"},"source":["### 2.2 Google Drive"]},{"cell_type":"markdown","metadata":{"id":"P5gj6qwoX9W3"},"source":["Mount your Google Drive partition.\n","\n","**Note:** *\\\"Colab Notebooks/handwritten-text-recognition/src/\\\"* was the directory where you put the project folders, specifically the **src** folder."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"ACQn1iBF9k9O","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682757460676,"user_tz":-120,"elapsed":4015,"user":{"displayName":"Titaÿna Kauffmann","userId":"16313386342874691862"}},"outputId":"c153bb0e-676d-4373-b130-49ec60e9c911"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at ./gdrive\n","/content/gdrive/My Drive/handwritten-text-recognition_polier/src\n","total 78\n","drwx------ 2 root root  4096 Nov 11 12:35 data\n","drwx------ 2 root root  4096 Nov 13 10:55 language\n","-rw------- 1 root root 11223 Apr 15  2022 main.py\n","drwx------ 2 root root  4096 Nov 11 12:33 network\n","-rw------- 1 root root 55687 Mar 29 08:38 tutorial.ipynb\n"]}],"source":["from google.colab import drive\n","\n","drive.mount(\"./gdrive\", force_remount=True)\n","\n","%cd \"./gdrive/My Drive/handwritten-text-recognition_polier/src\"\n","!ls -l"]},{"cell_type":"markdown","metadata":{"id":"YwogUA8RZAyp"},"source":["After mount, you can see the list os files in the project folder."]},{"cell_type":"markdown","metadata":{"id":"-fj7fSngY1IX"},"source":["## 3 Set Python Classes"]},{"cell_type":"markdown","metadata":{"id":"p6Q4cOlWhNl3"},"source":["### 3.1 Environment"]},{"cell_type":"markdown","metadata":{"id":"wvqL2Eq5ZUc7"},"source":["First, let's define our environment variables.\n","\n","Set the main configuration parameters, like input size, batch size, number of epochs and list of characters. This make compatible with **main.py** and jupyter notebook:\n","\n","* **dataset**: \"bentham\", \"iam\", \"rimes\", \"saintgall\", \"washington\"\n","\n","* **arch**: network to run: \"bluche\", \"puigcerver\", \"flor\"\n","\n","* **epochs**: number of epochs\n","\n","* **batch_size**: number size of the batch"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"_Qpr3drnGMWS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682757553476,"user_tz":-120,"elapsed":321,"user":{"displayName":"Titaÿna Kauffmann","userId":"16313386342874691862"}},"outputId":"ae3f134e-f498-4039-bdda-009ca1a210f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["source: ../data/CH_ACV_P_RENE_MONOD_161.hdf5\n","output ../output/CH_ACV_P_RENE_MONOD_161/flor\n","target ../output/CH_ACV_P_RENE_MONOD_161/flor/polier_7_pretrain.hdf5\n","charset: 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \n"]}],"source":["import os\n","import datetime\n","import string\n","\n","# define parameters\n","\n","#POLIER\n","#source = \"polier_7\"\n","source = \"CH_ACV_P_RENE_MONOD_161\" # commenter si on fait une boucle\n","\n","#BENTHAM\n","#source = \"bentham\" \n","\n","arch = \"flor\"\n","epochs = 500\n","batch_size = 10\n","\n","######################################################################\n","# define paths\n","# chemin des données transformées depuis Transkribus\n","\n","#source_path = os.path.join(\"..\", \"data\", f\"{source}.hdf5\")\n","source_path = '../data/CH_ACV_P_RENE_MONOD_161.hdf5'\n","\n","\n","######################################################################\n","# chemin de sauvegarde des données (e.g. prediction)\n","\n","output_path = os.path.join(\"..\", \"output\", source, arch)\n","os.makedirs(output_path, exist_ok=True)\n","\n","######################################################################\n","  # chemin vers le modèle\n","\n","#POLIER\n","target_path = os.path.join(output_path, \"polier_7_pretrain.hdf5\")\n","\n","#BENTHAM\n","#target_path = '../output/bentham/flor/bentham_1.hdf5'\n","\n","######################################################################\n","# define input size, number max of chars per line and list of valid chars\n","input_size = (1024, 128, 1)\n","max_text_length = 128\n","charset_base = string.printable[:95]\n","\n","print(\"source:\", source_path)\n","print(\"output\", output_path)\n","print(\"target\", target_path)\n","print(\"charset:\", charset_base)\n","\n","# Coller les cellules suivantes (sauf train)"]},{"cell_type":"markdown","metadata":{"id":"BFextshOhTKr"},"source":["### 3.2 DataGenerator Class"]},{"cell_type":"markdown","metadata":{"id":"KfZ1mfvsanu1"},"source":["The second class is **DataGenerator()**, responsible for:\n","\n","* Load the dataset partitions (train, valid, test);\n","\n","* Manager batchs for train/validation/test process."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8k9vpNzMIAi2"},"outputs":[],"source":["from data.generator import DataGenerator\n","\n","dtgen = DataGenerator(source=source_path,\n","                      batch_size=batch_size,\n","                      charset=charset_base,\n","                      max_text_length=max_text_length)\n","\n","print(f\"Train images: {dtgen.size['train']}\")\n","print(f\"Validation images: {dtgen.size['valid']}\")\n","print(f\"Test images: {dtgen.size['test']}\")"]},{"cell_type":"markdown","metadata":{"id":"-OdgNLK0hYAA"},"source":["### 3.3 HTRModel Class"]},{"cell_type":"markdown","metadata":{"id":"jHktk8AFcnKy"},"source":["The third class is **HTRModel()**, was developed to be easy to use and to abstract the complicated flow of a HTR system. It's responsible for:\n","\n","* Create model with Handwritten Text Recognition flow, in which calculate the loss function by CTC and decode output to calculate the HTR metrics (CER, WER and SER);\n","\n","* Save and load model;\n","\n","* Load weights in the models (train/infer);\n","\n","* Make Train/Predict process using *generator*.\n","\n","To make a dynamic HTRModel, its parameters are the *architecture*, *input_size* and *vocab_size*."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"nV0GreStISTR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682757533805,"user_tz":-120,"elapsed":11677,"user":{"displayName":"Titaÿna Kauffmann","userId":"16313386342874691862"}},"outputId":"bfd8c3a5-7973-430b-a796-4cb1ba291acb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input (InputLayer)          [(None, 1024, 128, 1)]    0         \n","                                                                 \n"," conv2d (Conv2D)             (None, 512, 64, 16)       160       \n","                                                                 \n"," p_re_lu (PReLU)             (None, 512, 64, 16)       16        \n","                                                                 \n"," batch_normalization (BatchN  (None, 512, 64, 16)      112       \n"," ormalization)                                                   \n","                                                                 \n"," full_gated_conv2d (FullGate  (None, 512, 64, 16)      4640      \n"," dConv2D)                                                        \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 512, 64, 32)       4640      \n","                                                                 \n"," p_re_lu_1 (PReLU)           (None, 512, 64, 32)       32        \n","                                                                 \n"," batch_normalization_1 (Batc  (None, 512, 64, 32)      224       \n"," hNormalization)                                                 \n","                                                                 \n"," full_gated_conv2d_1 (FullGa  (None, 512, 64, 32)      18496     \n"," tedConv2D)                                                      \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 256, 16, 40)       10280     \n","                                                                 \n"," p_re_lu_2 (PReLU)           (None, 256, 16, 40)       40        \n","                                                                 \n"," batch_normalization_2 (Batc  (None, 256, 16, 40)      280       \n"," hNormalization)                                                 \n","                                                                 \n"," full_gated_conv2d_2 (FullGa  (None, 256, 16, 40)      28880     \n"," tedConv2D)                                                      \n","                                                                 \n"," dropout (Dropout)           (None, 256, 16, 40)       0         \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 256, 16, 48)       17328     \n","                                                                 \n"," p_re_lu_3 (PReLU)           (None, 256, 16, 48)       48        \n","                                                                 \n"," batch_normalization_3 (Batc  (None, 256, 16, 48)      336       \n"," hNormalization)                                                 \n","                                                                 \n"," full_gated_conv2d_3 (FullGa  (None, 256, 16, 48)      41568     \n"," tedConv2D)                                                      \n","                                                                 \n"," dropout_1 (Dropout)         (None, 256, 16, 48)       0         \n","                                                                 \n"," conv2d_4 (Conv2D)           (None, 128, 4, 56)        21560     \n","                                                                 \n"," p_re_lu_4 (PReLU)           (None, 128, 4, 56)        56        \n","                                                                 \n"," batch_normalization_4 (Batc  (None, 128, 4, 56)       392       \n"," hNormalization)                                                 \n","                                                                 \n"," full_gated_conv2d_4 (FullGa  (None, 128, 4, 56)       56560     \n"," tedConv2D)                                                      \n","                                                                 \n"," dropout_2 (Dropout)         (None, 128, 4, 56)        0         \n","                                                                 \n"," conv2d_5 (Conv2D)           (None, 128, 4, 64)        32320     \n","                                                                 \n"," p_re_lu_5 (PReLU)           (None, 128, 4, 64)        64        \n","                                                                 \n"," batch_normalization_5 (Batc  (None, 128, 4, 64)       448       \n"," hNormalization)                                                 \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 128, 2, 64)       0         \n"," )                                                               \n","                                                                 \n"," reshape (Reshape)           (None, 128, 128)          0         \n","                                                                 \n"," bidirectional (Bidirectiona  (None, 128, 256)         198144    \n"," l)                                                              \n","                                                                 \n"," dense (Dense)               (None, 128, 256)          65792     \n","                                                                 \n"," bidirectional_1 (Bidirectio  (None, 128, 256)         296448    \n"," nal)                                                            \n","                                                                 \n"," dense_1 (Dense)             (None, 128, 98)           25186     \n","                                                                 \n","=================================================================\n","Total params: 824,050\n","Trainable params: 822,770\n","Non-trainable params: 1,280\n","_________________________________________________________________\n"]}],"source":["from network.model import HTRModel\n","\n","# create and compile HTRModel\n","model = HTRModel(architecture=arch,\n","                 input_size=input_size,\n","                 vocab_size=dtgen.tokenizer.vocab_size,\n","                 beam_width=10,\n","                 stop_tolerance=20,\n","                 reduce_tolerance=15)\n","\n","model.compile(learning_rate=0.001)\n","model.summary(output_path, \"summary.txt\")\n","\n","# get default callbacks and load checkpoint weights file (HDF5) if exists\n","model.load_checkpoint(target=target_path)\n","\n","callbacks = model.get_callbacks(logdir=output_path, checkpoint=target_path, verbose=1)"]},{"cell_type":"markdown","metadata":{"id":"T1fnz0Eugqru"},"source":["## 4 Training"]},{"cell_type":"markdown","metadata":{"id":"w1mLOcqYgsO-"},"source":["The training process is similar to the *fit()* of the Keras. After training, the information (epochs and minimum loss) is save."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2P6MSoxCISlD"},"outputs":[],"source":["# to calculate total and average time per epoch\n","start_time = datetime.datetime.now()\n","\n","h = model.fit(x=dtgen.next_train_batch(),\n","              epochs=epochs,\n","              steps_per_epoch=dtgen.steps['train'],\n","              validation_data=dtgen.next_valid_batch(),\n","              validation_steps=dtgen.steps['valid'],\n","              callbacks=callbacks,\n","              shuffle=True,\n","              verbose=1)\n","\n","total_time = datetime.datetime.now() - start_time\n","\n","loss = h.history['loss']\n","val_loss = h.history['val_loss']\n","\n","min_val_loss = min(val_loss)\n","min_val_loss_i = val_loss.index(min_val_loss)\n","\n","time_epoch = (total_time / len(loss))\n","total_item = (dtgen.size['train'] + dtgen.size['valid'])\n","\n","t_corpus = \"\\n\".join([\n","    f\"Total train images:      {dtgen.size['train']}\",\n","    f\"Total validation images: {dtgen.size['valid']}\",\n","    f\"Batch:                   {dtgen.batch_size}\\n\",\n","    f\"Total time:              {total_time}\",\n","    f\"Time per epoch:          {time_epoch}\",\n","    f\"Time per item:           {time_epoch / total_item}\\n\",\n","    f\"Total epochs:            {len(loss)}\",\n","    f\"Best epoch               {min_val_loss_i + 1}\\n\",\n","    f\"Training loss:           {loss[min_val_loss_i]:.8f}\",\n","    f\"Validation loss:         {min_val_loss:.8f}\"\n","])\n","\n","with open(os.path.join(output_path, \"train.txt\"), \"w\") as lg:\n","    lg.write(t_corpus)\n","    print(t_corpus)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"13g7tDjWgtXV"},"source":["## 5 Predict"]},{"cell_type":"markdown","metadata":{"id":"ddO26OT-g_QK"},"source":["The predict process is similar to the *predict* of the Keras:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a9iHL6tmaL_j"},"outputs":[],"source":["from data import preproc as pp\n","from google.colab.patches import cv2_imshow\n","\n","start_time = datetime.datetime.now()\n","\n","# predict() function will return the predicts with the probabilities\n","predicts, _ = model.predict(x=dtgen.next_test_batch(),\n","                            steps=dtgen.steps['test'],\n","                            ctc_decode=True,\n","                            verbose=1)\n","\n","# decode to string\n","predicts = [dtgen.tokenizer.decode(x[0]) for x in predicts]\n","ground_truth = [x.decode() for x in dtgen.dataset['test']['gt']]\n","\n","total_time = datetime.datetime.now() - start_time\n","\n","# mount predict corpus file\n","with open(os.path.join(output_path, f\"predict_{source}.txt\"), \"w\") as lg:\n","    for pd, gt in zip(predicts, ground_truth):\n","        lg.write(f\"TE_L {gt}\\nTE_P {pd}\\n\")\n","   \n","for i, item in enumerate(dtgen.dataset['test']['dt'][:10]):\n","    print(\"=\" * 1024, \"\\n\")\n","    cv2_imshow(pp.adjust_to_see(item))\n","    print(ground_truth[i])\n","    print(predicts[i], \"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"9JcAs3Q3WNJ-"},"source":["## 6 Evaluate"]},{"cell_type":"markdown","metadata":{"id":"8LuZBRepWbom"},"source":["Evaluation process is more manual process. Here we have the `ocr_metrics`, but feel free to implement other metrics instead. In the function, we have three parameters: \n","\n","* predicts\n","* ground_truth\n","* norm_accentuation (calculation with/without accentuation)\n","* norm_punctuation (calculation with/without punctuation marks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0gCwEYdKWOPK"},"outputs":[],"source":["from data import evaluation\n","\n","evaluate = evaluation.ocr_metrics(predicts, ground_truth)\n","\n","e_corpus = \"\\n\".join([\n","    f\"Total test images:    {dtgen.size['test']}\",\n","    f\"Total time:           {total_time}\",\n","    f\"Time per item:        {total_time / dtgen.size['test']}\\n\",\n","    f\"Metrics:\",\n","    f\"Character Error Rate: {evaluate[0]:.8f}\",\n","    f\"Word Error Rate:      {evaluate[1]:.8f}\",\n","    f\"Sequence Error Rate:  {evaluate[2]:.8f}\"\n","])\n","\n","with open(os.path.join(output_path, \"evaluate.txt\"), \"w\") as lg:\n","    lg.write(e_corpus)\n","    print(e_corpus)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1IQbXYUe4BkM8nqyN9JoEQRv1DbcY4JsL","timestamp":1666095139673},{"file_id":"https://github.com/arthurflor23/handwritten-text-recognition/blob/master/src/tutorial.ipynb","timestamp":1666090932766}],"collapsed_sections":["oMty1YwuWHpN"]},"file_extension":".py","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.10"},"mimetype":"text/x-python","name":"python","npconvert_exporter":"python","orig_nbformat":2,"pygments_lexer":"ipython3","version":3,"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}