{"nbformat":4,"nbformat_minor":0,"metadata":{"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3,"colab":{"provenance":[],"collapsed_sections":["oMty1YwuWHpN"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/arthurflor23/handwritten-text-recognition/blob/master/src/tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"gP-v0E_S-mQP"},"source":["<img src=\"https://github.com/arthurflor23/handwritten-text-recognition/blob/master/doc/image/header.png?raw=true\" />\n","\n","# Handwritten Text Recognition using TensorFlow 2.x\n","\n","This tutorial shows how you can use the project [Handwritten Text Recognition](https://github.com/arthurflor23/handwritten-text-recognition) in your Google Colab.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"oMty1YwuWHpN"},"source":["## 1 Localhost Environment\n","\n","We'll make sure you have the project in your Google Drive with the datasets in HDF5. If you already have structured files in the cloud, skip this step."]},{"cell_type":"markdown","metadata":{"id":"39blvPTPQJpt"},"source":["### 1.1 Datasets\n","\n","The datasets that you can use:\n","\n","a. [Bentham](http://www.transcriptorium.eu/~tsdata/)\n","\n","b. [IAM](http://www.fki.inf.unibe.ch/databases/iam-handwriting-database)\n","\n","c. [Rimes](http://www.a2ialab.com/doku.php?id=rimes_database:start)\n","\n","d. [Saint Gall](https://fki.tic.heia-fr.ch/databases/saint-gall-database)\n","\n","e. [Washington](https://fki.tic.heia-fr.ch/databases/washington-database)"]},{"cell_type":"markdown","metadata":{"id":"QVBGMLifWQwl"},"source":["### 1.2 Raw folder\n","\n","On localhost, download the code project from GitHub and extract the chosen dataset (or all if you prefer) in the **raw** folder. Don't change anything of the structure of the dataset, since the scripts were made from the **original structure** of them. Your project directory will be like this:\n","\n","```\n",".\n","├── raw\n","│   ├── bentham\n","│   │   ├── BenthamDatasetR0-GT\n","│   │   └── BenthamDatasetR0-Images\n","│   ├── iam\n","│   │   ├── ascii\n","│   │   ├── forms\n","│   │   ├── largeWriterIndependentTextLineRecognitionTask\n","│   │   ├── lines\n","│   │   └── xml\n","│   ├── rimes\n","│   │   ├── eval_2011\n","│   │   ├── eval_2011_annotated.xml\n","│   │   ├── training_2011\n","│   │   └── training_2011.xml\n","│   ├── saintgall\n","│   │   ├── data\n","│   │   ├── ground_truth\n","│   │   ├── README.txt\n","│   │   └── sets\n","│   └── washington\n","│       ├── data\n","│       ├── ground_truth\n","│       ├── README.txt\n","│       └── sets\n","└── src\n","    ├── data\n","    │   ├── evaluation.py\n","    │   ├── generator.py\n","    │   ├── preproc.py\n","    │   ├── reader.py\n","    │   ├── similar_error_analysis.py\n","    ├── main.py\n","    ├── network\n","    │   ├── architecture.py\n","    │   ├── layers.py\n","    │   ├── model.py\n","    └── tutorial.ipynb\n","\n","```\n","\n","After that, create virtual environment and install the dependencies with python 3 and pip:\n","\n","> ```python -m venv .venv && source .venv/bin/activate```\n","\n","> ```pip install -r requirements.txt```"]},{"cell_type":"markdown","metadata":{"id":"WyLRbAwsWSYA"},"source":["### 1.3 HDF5 files\n","\n","Now, you'll run the *transform* function from **main.py**. For this, execute on **src** folder:\n","\n","> ```python main.py --source=<DATASET_NAME> --transform```\n","\n","Your data will be preprocess and encode, creating and saving in the **data** folder. Now your project directory will be like this:\n","\n","\n","```\n",".\n","├── data\n","│   ├── bentham.hdf5\n","│   ├── iam.hdf5\n","│   ├── rimes.hdf5\n","│   ├── saintgall.hdf5\n","│   └── washington.hdf5\n","├── raw\n","│   ├── bentham\n","│   │   ├── BenthamDatasetR0-GT\n","│   │   └── BenthamDatasetR0-Images\n","│   ├── iam\n","│   │   ├── ascii\n","│   │   ├── forms\n","│   │   ├── largeWriterIndependentTextLineRecognitionTask\n","│   │   ├── lines\n","│   │   └── xml\n","│   ├── rimes\n","│   │   ├── eval_2011\n","│   │   ├── eval_2011_annotated.xml\n","│   │   ├── training_2011\n","│   │   └── training_2011.xml\n","│   ├── saintgall\n","│   │   ├── data\n","│   │   ├── ground_truth\n","│   │   ├── README.txt\n","│   │   └── sets\n","│   └── washington\n","│       ├── data\n","│       ├── ground_truth\n","│       ├── README.txt\n","│       └── sets\n","└── src\n","    ├── data\n","    │   ├── evaluation.py\n","    │   ├── generator.py\n","    │   ├── preproc.py\n","    │   ├── reader.py\n","    │   ├── similar_error_analysis.py\n","    ├── main.py\n","    ├── network\n","    │   ├── architecture.py\n","    │   ├── layers.py\n","    │   ├── model.py\n","    └── tutorial.ipynb\n","\n","```\n","\n","Then upload the **data** and **src** folders in the same directory in your Google Drive."]},{"cell_type":"markdown","metadata":{"id":"jydsAcWgWVth"},"source":["## 2 Google Drive Environment\n"]},{"cell_type":"markdown","metadata":{"id":"wk3e7YJiXzSl"},"source":["### 2.1 TensorFlow 2.x"]},{"cell_type":"markdown","metadata":{"id":"Z7twXyNGXtbJ"},"source":["Make sure the jupyter notebook is using GPU mode."]},{"cell_type":"code","metadata":{"id":"mHw4tODULT1Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680079064393,"user_tz":-120,"elapsed":1499,"user":{"displayName":"Rémi Petitpierre","userId":"01448730486989080326"}},"outputId":"3c696f78-34a9-4e02-85c8-2713977e7593"},"source":["!nvidia-smi"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Mar 29 08:37:42 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   77C    P0    34W /  70W |    339MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","metadata":{"id":"FMg-B5PH9h3r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680079065270,"user_tz":-120,"elapsed":4,"user":{"displayName":"Rémi Petitpierre","userId":"01448730486989080326"}},"outputId":"e0867ba5-5b98-48c4-f834-cab27d3d566d"},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","\n","device_name = tf.test.gpu_device_name()\n","\n","if device_name != \"/device:GPU:0\":\n","    raise SystemError(\"GPU device not found\")\n","\n","print(\"Found GPU at: {}\".format(device_name))"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n","Found GPU at: /device:GPU:0\n"]}]},{"cell_type":"markdown","metadata":{"id":"FyMv5wyDXxqc"},"source":["### 2.2 Google Drive"]},{"cell_type":"markdown","metadata":{"id":"P5gj6qwoX9W3"},"source":["Mount your Google Drive partition.\n","\n","**Note:** *\\\"Colab Notebooks/handwritten-text-recognition/src/\\\"* was the directory where you put the project folders, specifically the **src** folder."]},{"cell_type":"code","source":["%cd \"./gdrive/My Drive/handwritten-text-recognition_polier/\"\n","!ls -l"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tJGAW1mzW57w","executionInfo":{"status":"ok","timestamp":1680077022784,"user_tz":-120,"elapsed":376,"user":{"displayName":"Rémi Petitpierre","userId":"01448730486989080326"}},"outputId":"199507fe-a7d0-42ff-d327-61bf98373200"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 107] Transport endpoint is not connected: './gdrive/My Drive/handwritten-text-recognition_polier/'\n","/content/gdrive\n","ls: cannot open directory '.': Transport endpoint is not connected\n"]}]},{"cell_type":"code","metadata":{"id":"ACQn1iBF9k9O","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680079078711,"user_tz":-120,"elapsed":7024,"user":{"displayName":"Rémi Petitpierre","userId":"01448730486989080326"}},"outputId":"706a4239-ab32-4cb4-cc65-4ba3091b8ce4"},"source":["from google.colab import drive\n","\n","drive.mount(\"./gdrive\", force_remount=True)\n","\n","%cd \"./gdrive/My Drive/handwritten-text-recognition_polier/src/\"\n","!ls -l"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at ./gdrive\n","[Errno 2] No such file or directory: './gdrive/My Drive/handwritten-text-recognition_polier/src/'\n","/content\n","total 8\n","drwx------ 5 root root 4096 Mar 29 08:37 gdrive\n","drwxr-xr-x 1 root root 4096 Mar 27 13:41 sample_data\n"]}]},{"cell_type":"markdown","metadata":{"id":"YwogUA8RZAyp"},"source":["After mount, you can see the list os files in the project folder."]},{"cell_type":"markdown","metadata":{"id":"-fj7fSngY1IX"},"source":["## 3 Set Python Classes"]},{"cell_type":"markdown","metadata":{"id":"p6Q4cOlWhNl3"},"source":["### 3.1 Environment"]},{"cell_type":"markdown","metadata":{"id":"wvqL2Eq5ZUc7"},"source":["First, let's define our environment variables.\n","\n","Set the main configuration parameters, like input size, batch size, number of epochs and list of characters. This make compatible with **main.py** and jupyter notebook:\n","\n","* **dataset**: \"bentham\", \"iam\", \"rimes\", \"saintgall\", \"washington\"\n","\n","* **arch**: network to run: \"bluche\", \"puigcerver\", \"flor\"\n","\n","* **epochs**: number of epochs\n","\n","* **batch_size**: number size of the batch"]},{"cell_type":"code","metadata":{"id":"_Qpr3drnGMWS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658923876643,"user_tz":-120,"elapsed":215,"user":{"displayName":"Rémi Petitpierre","userId":"01448730486989080326"}},"outputId":"8132a99a-4c55-4d45-db4f-c1ef150e79d6"},"source":["import os\n","import datetime\n","import string\n","\n","# define parameters\n","source = \"polier\"\n","arch = \"flor\"\n","epochs = 100\n","batch_size = 16\n","\n","# define paths\n","source_path = os.path.join(\"..\", \"data\", f\"{source}.hdf5\")\n","output_path = os.path.join(\"..\", \"output\", source, arch)\n","target_path = os.path.join(output_path, \"checkpoint_weights.hdf5\")\n","os.makedirs(output_path, exist_ok=True)\n","\n","# define input size, number max of chars per line and list of valid chars\n","input_size = (1024, 128, 1)\n","max_text_length = 128\n","charset_base = string.printable[:95]\n","\n","print(\"source:\", source_path)\n","print(\"output\", output_path)\n","print(\"target\", target_path)\n","print(\"charset:\", charset_base)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["source: ../data/polier.hdf5\n","output ../output/polier/flor\n","target ../output/polier/flor/checkpoint_weights.hdf5\n","charset: 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \n"]}]},{"cell_type":"markdown","metadata":{"id":"BFextshOhTKr"},"source":["### 3.2 DataGenerator Class"]},{"cell_type":"markdown","metadata":{"id":"KfZ1mfvsanu1"},"source":["The second class is **DataGenerator()**, responsible for:\n","\n","* Load the dataset partitions (train, valid, test);\n","\n","* Manager batchs for train/validation/test process."]},{"cell_type":"code","metadata":{"id":"8k9vpNzMIAi2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658923881459,"user_tz":-120,"elapsed":2402,"user":{"displayName":"Rémi Petitpierre","userId":"01448730486989080326"}},"outputId":"fb75b1f4-a905-4aa4-c4b3-c7daf6c5420c"},"source":["from data.generator import DataGenerator\n","\n","dtgen = DataGenerator(source=source_path,\n","                      batch_size=batch_size,\n","                      charset=charset_base,\n","                      max_text_length=max_text_length)\n","\n","print(f\"Train images: {dtgen.size['train']}\")\n","print(f\"Validation images: {dtgen.size['valid']}\")\n","print(f\"Test images: {dtgen.size['test']}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train images: 1199\n","Validation images: 163\n","Test images: 340\n"]}]},{"cell_type":"markdown","metadata":{"id":"-OdgNLK0hYAA"},"source":["### 3.3 HTRModel Class"]},{"cell_type":"markdown","metadata":{"id":"jHktk8AFcnKy"},"source":["The third class is **HTRModel()**, was developed to be easy to use and to abstract the complicated flow of a HTR system. It's responsible for:\n","\n","* Create model with Handwritten Text Recognition flow, in which calculate the loss function by CTC and decode output to calculate the HTR metrics (CER, WER and SER);\n","\n","* Save and load model;\n","\n","* Load weights in the models (train/infer);\n","\n","* Make Train/Predict process using *generator*.\n","\n","To make a dynamic HTRModel, its parameters are the *architecture*, *input_size* and *vocab_size*."]},{"cell_type":"code","metadata":{"id":"nV0GreStISTR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658925490202,"user_tz":-120,"elapsed":2085,"user":{"displayName":"Rémi Petitpierre","userId":"01448730486989080326"}},"outputId":"06b62cae-740b-432c-e0ef-a3dfb8d64048"},"source":["from network.model import HTRModel\n","\n","# create and compile HTRModel\n","model = HTRModel(architecture=arch,\n","                 input_size=input_size,\n","                 vocab_size=dtgen.tokenizer.vocab_size,\n","                 beam_width=10,\n","                 stop_tolerance=20,\n","                 reduce_tolerance=15)\n","\n","model.compile(learning_rate=0.001)\n","model.summary(output_path, \"summary.txt\")\n","\n","# get default callbacks and load checkpoint weights file (HDF5) if exists\n","model.load_checkpoint(target=target_path)\n","\n","callbacks = model.get_callbacks(logdir=output_path, checkpoint=target_path, verbose=1)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input (InputLayer)          [(None, 1024, 128, 1)]    0         \n","                                                                 \n"," conv2d_12 (Conv2D)          (None, 512, 64, 16)       160       \n","                                                                 \n"," p_re_lu_12 (PReLU)          (None, 512, 64, 16)       16        \n","                                                                 \n"," batch_normalization_12 (Bat  (None, 512, 64, 16)      112       \n"," chNormalization)                                                \n","                                                                 \n"," full_gated_conv2d_10 (FullG  (None, 512, 64, 16)      4640      \n"," atedConv2D)                                                     \n","                                                                 \n"," conv2d_13 (Conv2D)          (None, 512, 64, 32)       4640      \n","                                                                 \n"," p_re_lu_13 (PReLU)          (None, 512, 64, 32)       32        \n","                                                                 \n"," batch_normalization_13 (Bat  (None, 512, 64, 32)      224       \n"," chNormalization)                                                \n","                                                                 \n"," full_gated_conv2d_11 (FullG  (None, 512, 64, 32)      18496     \n"," atedConv2D)                                                     \n","                                                                 \n"," conv2d_14 (Conv2D)          (None, 256, 16, 40)       10280     \n","                                                                 \n"," p_re_lu_14 (PReLU)          (None, 256, 16, 40)       40        \n","                                                                 \n"," batch_normalization_14 (Bat  (None, 256, 16, 40)      280       \n"," chNormalization)                                                \n","                                                                 \n"," full_gated_conv2d_12 (FullG  (None, 256, 16, 40)      28880     \n"," atedConv2D)                                                     \n","                                                                 \n"," dropout_6 (Dropout)         (None, 256, 16, 40)       0         \n","                                                                 \n"," conv2d_15 (Conv2D)          (None, 256, 16, 48)       17328     \n","                                                                 \n"," p_re_lu_15 (PReLU)          (None, 256, 16, 48)       48        \n","                                                                 \n"," batch_normalization_15 (Bat  (None, 256, 16, 48)      336       \n"," chNormalization)                                                \n","                                                                 \n"," full_gated_conv2d_13 (FullG  (None, 256, 16, 48)      41568     \n"," atedConv2D)                                                     \n","                                                                 \n"," dropout_7 (Dropout)         (None, 256, 16, 48)       0         \n","                                                                 \n"," conv2d_16 (Conv2D)          (None, 128, 4, 56)        21560     \n","                                                                 \n"," p_re_lu_16 (PReLU)          (None, 128, 4, 56)        56        \n","                                                                 \n"," batch_normalization_16 (Bat  (None, 128, 4, 56)       392       \n"," chNormalization)                                                \n","                                                                 \n"," full_gated_conv2d_14 (FullG  (None, 128, 4, 56)       56560     \n"," atedConv2D)                                                     \n","                                                                 \n"," dropout_8 (Dropout)         (None, 128, 4, 56)        0         \n","                                                                 \n"," conv2d_17 (Conv2D)          (None, 128, 4, 64)        32320     \n","                                                                 \n"," p_re_lu_17 (PReLU)          (None, 128, 4, 64)        64        \n","                                                                 \n"," batch_normalization_17 (Bat  (None, 128, 4, 64)       448       \n"," chNormalization)                                                \n","                                                                 \n"," max_pooling2d_2 (MaxPooling  (None, 128, 2, 64)       0         \n"," 2D)                                                             \n","                                                                 \n"," reshape_2 (Reshape)         (None, 128, 128)          0         \n","                                                                 \n"," bidirectional_4 (Bidirectio  (None, 128, 256)         198144    \n"," nal)                                                            \n","                                                                 \n"," dense_4 (Dense)             (None, 128, 256)          65792     \n","                                                                 \n"," bidirectional_5 (Bidirectio  (None, 128, 256)         296448    \n"," nal)                                                            \n","                                                                 \n"," dense_5 (Dense)             (None, 128, 98)           25186     \n","                                                                 \n","=================================================================\n","Total params: 824,050\n","Trainable params: 822,770\n","Non-trainable params: 1,280\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","metadata":{"id":"T1fnz0Eugqru"},"source":["## 4 Training"]},{"cell_type":"markdown","metadata":{"id":"w1mLOcqYgsO-"},"source":["The training process is similar to the *fit()* of the Keras. After training, the information (epochs and minimum loss) is save."]},{"cell_type":"code","metadata":{"id":"2P6MSoxCISlD","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ec09c71b-9b98-4592-ad03-f7d0f2b8128c","executionInfo":{"status":"ok","timestamp":1658925472980,"user_tz":-120,"elapsed":603646,"user":{"displayName":"Rémi Petitpierre","userId":"01448730486989080326"}}},"source":["# to calculate total and average time per epoch\n","start_time = datetime.datetime.now()\n","\n","h = model.fit(x=dtgen.next_train_batch(),\n","              epochs=epochs,\n","              steps_per_epoch=dtgen.steps['train'],\n","              validation_data=dtgen.next_valid_batch(),\n","              validation_steps=dtgen.steps['valid'],\n","              callbacks=callbacks,\n","              shuffle=True,\n","              verbose=1)\n","\n","total_time = datetime.datetime.now() - start_time\n","\n","loss = h.history['loss']\n","val_loss = h.history['val_loss']\n","\n","min_val_loss = min(val_loss)\n","min_val_loss_i = val_loss.index(min_val_loss)\n","\n","time_epoch = (total_time / len(loss))\n","total_item = (dtgen.size['train'] + dtgen.size['valid'])\n","\n","t_corpus = \"\\n\".join([\n","    f\"Total train images:      {dtgen.size['train']}\",\n","    f\"Total validation images: {dtgen.size['valid']}\",\n","    f\"Batch:                   {dtgen.batch_size}\\n\",\n","    f\"Total time:              {total_time}\",\n","    f\"Time per epoch:          {time_epoch}\",\n","    f\"Time per item:           {time_epoch / total_item}\\n\",\n","    f\"Total epochs:            {len(loss)}\",\n","    f\"Best epoch               {min_val_loss_i + 1}\\n\",\n","    f\"Training loss:           {loss[min_val_loss_i]:.8f}\",\n","    f\"Validation loss:         {min_val_loss:.8f}\"\n","])\n","\n","with open(os.path.join(output_path, \"train.txt\"), \"w\") as lg:\n","    lg.write(t_corpus)\n","    print(t_corpus)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","75/75 [==============================] - ETA: 0s - loss: 55.2021\n","Epoch 1: val_loss improved from inf to 35.11787, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n","75/75 [==============================] - 76s 248ms/step - loss: 55.2021 - val_loss: 35.1179 - lr: 0.0010\n","Epoch 2/100\n","75/75 [==============================] - ETA: 0s - loss: 36.6615\n","Epoch 2: val_loss improved from 35.11787 to 27.38564, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n","75/75 [==============================] - 15s 206ms/step - loss: 36.6615 - val_loss: 27.3856 - lr: 0.0010\n","Epoch 3/100\n","75/75 [==============================] - ETA: 0s - loss: 30.4486\n","Epoch 3: val_loss improved from 27.38564 to 23.51434, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n","75/75 [==============================] - 15s 198ms/step - loss: 30.4486 - val_loss: 23.5143 - lr: 0.0010\n","Epoch 4/100\n","75/75 [==============================] - ETA: 0s - loss: 26.7834\n","Epoch 4: val_loss improved from 23.51434 to 21.54732, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n","75/75 [==============================] - 15s 199ms/step - loss: 26.7834 - val_loss: 21.5473 - lr: 0.0010\n","Epoch 5/100\n","75/75 [==============================] - ETA: 0s - loss: 23.8900\n","Epoch 5: val_loss improved from 21.54732 to 19.34343, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n","75/75 [==============================] - 15s 198ms/step - loss: 23.8900 - val_loss: 19.3434 - lr: 0.0010\n","Epoch 6/100\n","75/75 [==============================] - ETA: 0s - loss: 21.8941\n","Epoch 6: val_loss improved from 19.34343 to 18.04168, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n","75/75 [==============================] - 15s 200ms/step - loss: 21.8941 - val_loss: 18.0417 - lr: 0.0010\n","Epoch 7/100\n","75/75 [==============================] - ETA: 0s - loss: 20.2458\n","Epoch 7: val_loss improved from 18.04168 to 16.83247, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n","75/75 [==============================] - 15s 201ms/step - loss: 20.2458 - val_loss: 16.8325 - lr: 0.0010\n","Epoch 8/100\n","75/75 [==============================] - ETA: 0s - loss: 18.8709\n","Epoch 8: val_loss improved from 16.83247 to 16.13271, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n","75/75 [==============================] - 15s 200ms/step - loss: 18.8709 - val_loss: 16.1327 - lr: 0.0010\n","Epoch 9/100\n","75/75 [==============================] - ETA: 0s - loss: 17.5471\n","Epoch 9: val_loss improved from 16.13271 to 15.18278, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n","75/75 [==============================] - 15s 203ms/step - loss: 17.5471 - val_loss: 15.1828 - lr: 0.0010\n","Epoch 10/100\n","75/75 [==============================] - ETA: 0s - loss: 16.8908\n","Epoch 10: val_loss improved from 15.18278 to 14.47832, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n","75/75 [==============================] - 15s 202ms/step - loss: 16.8908 - val_loss: 14.4783 - lr: 0.0010\n","Epoch 11/100\n","75/75 [==============================] - ETA: 0s - loss: 15.8225\n","Epoch 11: val_loss improved from 14.47832 to 13.89777, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n","75/75 [==============================] - 15s 207ms/step - loss: 15.8225 - val_loss: 13.8978 - lr: 0.0010\n","Epoch 12/100\n","75/75 [==============================] - ETA: 0s - loss: 15.1544\n","Epoch 12: val_loss improved from 13.89777 to 13.71277, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n","75/75 [==============================] - 16s 209ms/step - loss: 15.1544 - val_loss: 13.7128 - lr: 0.0010\n","Epoch 13/100\n","75/75 [==============================] - ETA: 0s - loss: 14.4059\n","Epoch 13: val_loss improved from 13.71277 to 13.18890, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n","75/75 [==============================] - 15s 200ms/step - loss: 14.4059 - val_loss: 13.1889 - lr: 0.0010\n","Epoch 14/100\n","75/75 [==============================] - ETA: 0s - loss: 13.5217\n","Epoch 14: val_loss improved from 13.18890 to 12.99373, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n","75/75 [==============================] - 15s 201ms/step - loss: 13.5217 - val_loss: 12.9937 - lr: 0.0010\n","Epoch 15/100\n","75/75 [==============================] - ETA: 0s - loss: 13.0609\n","Epoch 15: val_loss improved from 12.99373 to 12.63241, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n","75/75 [==============================] - 16s 209ms/step - loss: 13.0609 - val_loss: 12.6324 - lr: 0.0010\n","Epoch 16/100\n","75/75 [==============================] - ETA: 0s - loss: 12.4886\n","Epoch 16: val_loss improved from 12.63241 to 12.33030, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n","75/75 [==============================] - 16s 209ms/step - loss: 12.4886 - val_loss: 12.3303 - lr: 0.0010\n","Epoch 17/100\n","75/75 [==============================] - ETA: 0s - loss: 12.2140\n","Epoch 17: val_loss improved from 12.33030 to 11.78673, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n","75/75 [==============================] - 15s 200ms/step - loss: 12.2140 - val_loss: 11.7867 - lr: 0.0010\n","Epoch 18/100\n","75/75 [==============================] - ETA: 0s - loss: 11.7749\n","Epoch 18: val_loss improved from 11.78673 to 11.74039, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n","75/75 [==============================] - 16s 210ms/step - loss: 11.7749 - val_loss: 11.7404 - lr: 0.0010\n","Epoch 19/100\n","75/75 [==============================] - ETA: 0s - loss: 11.0835\n","Epoch 19: val_loss improved from 11.74039 to 11.47816, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n","75/75 [==============================] - 16s 208ms/step - loss: 11.0835 - val_loss: 11.4782 - lr: 0.0010\n","Epoch 20/100\n","75/75 [==============================] - ETA: 0s - loss: 10.8932\n","Epoch 20: val_loss did not improve from 11.47816\n","75/75 [==============================] - 15s 198ms/step - loss: 10.8932 - val_loss: 11.5478 - lr: 0.0010\n","Epoch 21/100\n","75/75 [==============================] - ETA: 0s - loss: 10.4748\n","Epoch 21: val_loss improved from 11.47816 to 11.12544, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n","75/75 [==============================] - 16s 213ms/step - loss: 10.4748 - val_loss: 11.1254 - lr: 0.0010\n","Epoch 22/100\n","75/75 [==============================] - ETA: 0s - loss: 10.1482\n","Epoch 22: val_loss did not improve from 11.12544\n","75/75 [==============================] - 15s 197ms/step - loss: 10.1482 - val_loss: 11.4313 - lr: 0.0010\n","Epoch 23/100\n","75/75 [==============================] - ETA: 0s - loss: 9.7485\n","Epoch 23: val_loss did not improve from 11.12544\n","75/75 [==============================] - 15s 197ms/step - loss: 9.7485 - val_loss: 11.1562 - lr: 0.0010\n","Epoch 24/100\n","75/75 [==============================] - ETA: 0s - loss: 9.7555\n","Epoch 24: val_loss improved from 11.12544 to 10.87680, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n","75/75 [==============================] - 15s 202ms/step - loss: 9.7555 - val_loss: 10.8768 - lr: 0.0010\n","Epoch 25/100\n","75/75 [==============================] - ETA: 0s - loss: 9.1538\n","Epoch 25: val_loss did not improve from 10.87680\n","75/75 [==============================] - 15s 197ms/step - loss: 9.1538 - val_loss: 11.0056 - lr: 0.0010\n","Epoch 26/100\n","75/75 [==============================] - ETA: 0s - loss: 8.9136\n","Epoch 26: val_loss improved from 10.87680 to 10.50908, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n","75/75 [==============================] - 16s 208ms/step - loss: 8.9136 - val_loss: 10.5091 - lr: 0.0010\n","Epoch 27/100\n","75/75 [==============================] - ETA: 0s - loss: 8.7568\n","Epoch 27: val_loss improved from 10.50908 to 10.32722, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n","75/75 [==============================] - 15s 200ms/step - loss: 8.7568 - val_loss: 10.3272 - lr: 0.0010\n","Epoch 28/100\n","75/75 [==============================] - ETA: 0s - loss: 8.5102\n","Epoch 28: val_loss improved from 10.32722 to 10.30164, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n","75/75 [==============================] - 16s 208ms/step - loss: 8.5102 - val_loss: 10.3016 - lr: 0.0010\n","Epoch 29/100\n","75/75 [==============================] - ETA: 0s - loss: 8.0885\n","Epoch 29: val_loss did not improve from 10.30164\n","75/75 [==============================] - 15s 196ms/step - loss: 8.0885 - val_loss: 10.3039 - lr: 0.0010\n","Epoch 30/100\n","75/75 [==============================] - ETA: 0s - loss: 7.9229\n","Epoch 30: val_loss improved from 10.30164 to 9.73245, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n","75/75 [==============================] - 15s 201ms/step - loss: 7.9229 - val_loss: 9.7325 - lr: 0.0010\n","Epoch 31/100\n","75/75 [==============================] - ETA: 0s - loss: 7.9860\n","Epoch 31: val_loss did not improve from 9.73245\n","75/75 [==============================] - 16s 210ms/step - loss: 7.9860 - val_loss: 10.0181 - lr: 0.0010\n","Epoch 32/100\n","75/75 [==============================] - ETA: 0s - loss: 7.5837\n","Epoch 32: val_loss did not improve from 9.73245\n","75/75 [==============================] - 15s 205ms/step - loss: 7.5837 - val_loss: 9.9106 - lr: 0.0010\n","Epoch 33/100\n","75/75 [==============================] - ETA: 0s - loss: 7.4414\n","Epoch 33: val_loss did not improve from 9.73245\n","75/75 [==============================] - 15s 195ms/step - loss: 7.4414 - val_loss: 9.7848 - lr: 0.0010\n","Epoch 34/100\n","75/75 [==============================] - ETA: 0s - loss: 7.1667\n","Epoch 34: val_loss improved from 9.73245 to 9.57448, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n","75/75 [==============================] - 16s 207ms/step - loss: 7.1667 - val_loss: 9.5745 - lr: 0.0010\n","Epoch 35/100\n","75/75 [==============================] - ETA: 0s - loss: 6.9006\n","Epoch 35: val_loss improved from 9.57448 to 9.56188, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n","75/75 [==============================] - 16s 211ms/step - loss: 6.9006 - val_loss: 9.5619 - lr: 0.0010\n","Epoch 36/100\n","75/75 [==============================] - ETA: 0s - loss: 6.8184\n","Epoch 36: val_loss improved from 9.56188 to 9.32045, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n","75/75 [==============================] - 15s 200ms/step - loss: 6.8184 - val_loss: 9.3205 - lr: 0.0010\n","Epoch 37/100\n","75/75 [==============================] - ETA: 0s - loss: 7.1065\n","Epoch 37: val_loss did not improve from 9.32045\n","75/75 [==============================] - 15s 197ms/step - loss: 7.1065 - val_loss: 9.4712 - lr: 0.0010\n","Epoch 38/100\n","75/75 [==============================] - ETA: 0s - loss: 6.5625\n","Epoch 38: val_loss did not improve from 9.32045\n","75/75 [==============================] - 15s 205ms/step - loss: 6.5625 - val_loss: 9.4113 - lr: 0.0010\n","Epoch 39/100\n","75/75 [==============================] - ETA: 0s - loss: 6.3777\n","Epoch 39: val_loss improved from 9.32045 to 9.03265, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n","75/75 [==============================] - 16s 210ms/step - loss: 6.3777 - val_loss: 9.0326 - lr: 0.0010\n","Epoch 40/100\n","75/75 [==============================] - ETA: 0s - loss: 6.4623\n","Epoch 40: val_loss did not improve from 9.03265\n","75/75 [==============================] - 15s 196ms/step - loss: 6.4623 - val_loss: 9.4800 - lr: 0.0010\n","Epoch 41/100\n","75/75 [==============================] - ETA: 0s - loss: 6.2142\n","Epoch 41: val_loss did not improve from 9.03265\n","75/75 [==============================] - 16s 210ms/step - loss: 6.2142 - val_loss: 9.3484 - lr: 0.0010\n","Epoch 42/100\n","75/75 [==============================] - ETA: 0s - loss: 5.9495\n","Epoch 42: val_loss did not improve from 9.03265\n","75/75 [==============================] - 15s 197ms/step - loss: 5.9495 - val_loss: 9.3295 - lr: 0.0010\n","Epoch 43/100\n","75/75 [==============================] - ETA: 0s - loss: 5.7837\n","Epoch 43: val_loss did not improve from 9.03265\n","75/75 [==============================] - 15s 196ms/step - loss: 5.7837 - val_loss: 9.5641 - lr: 0.0010\n","Epoch 44/100\n","75/75 [==============================] - ETA: 0s - loss: 5.7556\n","Epoch 44: val_loss did not improve from 9.03265\n","75/75 [==============================] - 15s 203ms/step - loss: 5.7556 - val_loss: 9.3985 - lr: 0.0010\n","Epoch 45/100\n","75/75 [==============================] - ETA: 0s - loss: 5.7727\n","Epoch 45: val_loss did not improve from 9.03265\n","75/75 [==============================] - 15s 196ms/step - loss: 5.7727 - val_loss: 9.1894 - lr: 0.0010\n","Epoch 46/100\n","75/75 [==============================] - ETA: 0s - loss: 5.6432\n","Epoch 46: val_loss did not improve from 9.03265\n","75/75 [==============================] - 15s 204ms/step - loss: 5.6432 - val_loss: 9.4447 - lr: 0.0010\n","Epoch 47/100\n","75/75 [==============================] - ETA: 0s - loss: 5.4996\n","Epoch 47: val_loss did not improve from 9.03265\n","75/75 [==============================] - 15s 204ms/step - loss: 5.4996 - val_loss: 9.4518 - lr: 0.0010\n","Epoch 48/100\n","75/75 [==============================] - ETA: 0s - loss: 5.4498\n","Epoch 48: val_loss improved from 9.03265 to 8.69061, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n","75/75 [==============================] - 15s 200ms/step - loss: 5.4498 - val_loss: 8.6906 - lr: 0.0010\n","Epoch 49/100\n","75/75 [==============================] - ETA: 0s - loss: 5.2638\n","Epoch 49: val_loss did not improve from 8.69061\n","75/75 [==============================] - 15s 205ms/step - loss: 5.2638 - val_loss: 8.8051 - lr: 0.0010\n","Epoch 50/100\n","75/75 [==============================] - ETA: 0s - loss: 5.3144\n","Epoch 50: val_loss did not improve from 8.69061\n","75/75 [==============================] - 15s 196ms/step - loss: 5.3144 - val_loss: 8.8294 - lr: 0.0010\n","Epoch 51/100\n","75/75 [==============================] - ETA: 0s - loss: 5.2664\n","Epoch 51: val_loss improved from 8.69061 to 8.61088, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n","75/75 [==============================] - 15s 205ms/step - loss: 5.2664 - val_loss: 8.6109 - lr: 0.0010\n","Epoch 52/100\n","75/75 [==============================] - ETA: 0s - loss: 5.0084\n","Epoch 52: val_loss improved from 8.61088 to 8.37378, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n","75/75 [==============================] - 15s 202ms/step - loss: 5.0084 - val_loss: 8.3738 - lr: 0.0010\n","Epoch 53/100\n","75/75 [==============================] - ETA: 0s - loss: 4.9542\n","Epoch 53: val_loss did not improve from 8.37378\n","75/75 [==============================] - 15s 206ms/step - loss: 4.9542 - val_loss: 8.9650 - lr: 0.0010\n","Epoch 54/100\n","75/75 [==============================] - ETA: 0s - loss: 4.9864\n","Epoch 54: val_loss improved from 8.37378 to 8.29594, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n","75/75 [==============================] - 16s 208ms/step - loss: 4.9864 - val_loss: 8.2959 - lr: 0.0010\n","Epoch 55/100\n","75/75 [==============================] - ETA: 0s - loss: 4.8859\n","Epoch 55: val_loss did not improve from 8.29594\n","75/75 [==============================] - 15s 197ms/step - loss: 4.8859 - val_loss: 8.7296 - lr: 0.0010\n","Epoch 56/100\n","75/75 [==============================] - ETA: 0s - loss: 4.7878\n","Epoch 56: val_loss did not improve from 8.29594\n","75/75 [==============================] - 15s 205ms/step - loss: 4.7878 - val_loss: 8.5045 - lr: 0.0010\n","Epoch 57/100\n","75/75 [==============================] - ETA: 0s - loss: 4.6407\n","Epoch 57: val_loss did not improve from 8.29594\n","75/75 [==============================] - 15s 196ms/step - loss: 4.6407 - val_loss: 8.5792 - lr: 0.0010\n","Epoch 58/100\n","75/75 [==============================] - ETA: 0s - loss: 4.6409\n","Epoch 58: val_loss did not improve from 8.29594\n","75/75 [==============================] - 15s 204ms/step - loss: 4.6409 - val_loss: 8.6249 - lr: 0.0010\n","Epoch 59/100\n","75/75 [==============================] - ETA: 0s - loss: 4.4434\n","Epoch 59: val_loss did not improve from 8.29594\n","75/75 [==============================] - 15s 196ms/step - loss: 4.4434 - val_loss: 8.4592 - lr: 0.0010\n","Epoch 60/100\n","75/75 [==============================] - ETA: 0s - loss: 4.3114\n","Epoch 60: val_loss improved from 8.29594 to 8.13494, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n","75/75 [==============================] - 16s 209ms/step - loss: 4.3114 - val_loss: 8.1349 - lr: 0.0010\n","Epoch 61/100\n","75/75 [==============================] - ETA: 0s - loss: 4.2612\n","Epoch 61: val_loss did not improve from 8.13494\n","75/75 [==============================] - 15s 202ms/step - loss: 4.2612 - val_loss: 8.3343 - lr: 0.0010\n","Epoch 62/100\n","75/75 [==============================] - ETA: 0s - loss: 4.3023\n","Epoch 62: val_loss did not improve from 8.13494\n","75/75 [==============================] - 15s 206ms/step - loss: 4.3023 - val_loss: 8.5858 - lr: 0.0010\n","Epoch 63/100\n","75/75 [==============================] - ETA: 0s - loss: 4.4164\n","Epoch 63: val_loss did not improve from 8.13494\n","75/75 [==============================] - 15s 206ms/step - loss: 4.4164 - val_loss: 8.3197 - lr: 0.0010\n","Epoch 64/100\n","75/75 [==============================] - ETA: 0s - loss: 4.0075\n","Epoch 64: val_loss did not improve from 8.13494\n","75/75 [==============================] - 15s 198ms/step - loss: 4.0075 - val_loss: 8.3396 - lr: 0.0010\n","Epoch 65/100\n","75/75 [==============================] - ETA: 0s - loss: 4.1511\n","Epoch 65: val_loss did not improve from 8.13494\n","75/75 [==============================] - 15s 196ms/step - loss: 4.1511 - val_loss: 8.3017 - lr: 0.0010\n","Epoch 66/100\n","75/75 [==============================] - ETA: 0s - loss: 4.0464\n","Epoch 66: val_loss improved from 8.13494 to 7.89690, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n","75/75 [==============================] - 16s 211ms/step - loss: 4.0464 - val_loss: 7.8969 - lr: 0.0010\n","Epoch 67/100\n","75/75 [==============================] - ETA: 0s - loss: 3.9159\n","Epoch 67: val_loss did not improve from 7.89690\n","75/75 [==============================] - 15s 197ms/step - loss: 3.9159 - val_loss: 8.4663 - lr: 0.0010\n","Epoch 68/100\n","75/75 [==============================] - ETA: 0s - loss: 3.8511\n","Epoch 68: val_loss did not improve from 7.89690\n","75/75 [==============================] - 15s 198ms/step - loss: 3.8511 - val_loss: 8.3959 - lr: 0.0010\n","Epoch 69/100\n","75/75 [==============================] - ETA: 0s - loss: 3.8914\n","Epoch 69: val_loss did not improve from 7.89690\n","75/75 [==============================] - 15s 196ms/step - loss: 3.8914 - val_loss: 8.3187 - lr: 0.0010\n","Epoch 70/100\n","75/75 [==============================] - ETA: 0s - loss: 3.8265\n","Epoch 70: val_loss did not improve from 7.89690\n","75/75 [==============================] - 15s 205ms/step - loss: 3.8265 - val_loss: 8.6241 - lr: 0.0010\n","Epoch 71/100\n","75/75 [==============================] - ETA: 0s - loss: 3.6438\n","Epoch 71: val_loss did not improve from 7.89690\n","75/75 [==============================] - 15s 203ms/step - loss: 3.6438 - val_loss: 8.8721 - lr: 0.0010\n","Epoch 72/100\n","75/75 [==============================] - ETA: 0s - loss: 3.4363\n","Epoch 72: val_loss did not improve from 7.89690\n","75/75 [==============================] - 15s 197ms/step - loss: 3.4363 - val_loss: 8.3236 - lr: 0.0010\n","Epoch 73/100\n","75/75 [==============================] - ETA: 0s - loss: 3.7614\n","Epoch 73: val_loss did not improve from 7.89690\n","75/75 [==============================] - 15s 197ms/step - loss: 3.7614 - val_loss: 8.6401 - lr: 0.0010\n","Epoch 74/100\n","75/75 [==============================] - ETA: 0s - loss: 3.7912\n","Epoch 74: val_loss did not improve from 7.89690\n","75/75 [==============================] - 15s 205ms/step - loss: 3.7912 - val_loss: 8.5332 - lr: 0.0010\n","Epoch 75/100\n","75/75 [==============================] - ETA: 0s - loss: 3.4680\n","Epoch 75: val_loss did not improve from 7.89690\n","75/75 [==============================] - 15s 197ms/step - loss: 3.4680 - val_loss: 8.5067 - lr: 0.0010\n","Epoch 76/100\n","75/75 [==============================] - ETA: 0s - loss: 3.4974\n","Epoch 76: val_loss did not improve from 7.89690\n","75/75 [==============================] - 15s 198ms/step - loss: 3.4974 - val_loss: 8.4702 - lr: 0.0010\n","Epoch 77/100\n","75/75 [==============================] - ETA: 0s - loss: 3.5256\n","Epoch 77: val_loss did not improve from 7.89690\n","75/75 [==============================] - 15s 196ms/step - loss: 3.5256 - val_loss: 8.6848 - lr: 0.0010\n","Epoch 78/100\n","75/75 [==============================] - ETA: 0s - loss: 3.5318\n","Epoch 78: val_loss did not improve from 7.89690\n","75/75 [==============================] - 15s 197ms/step - loss: 3.5318 - val_loss: 7.9742 - lr: 0.0010\n","Epoch 79/100\n","75/75 [==============================] - ETA: 0s - loss: 3.3306\n","Epoch 79: val_loss did not improve from 7.89690\n","75/75 [==============================] - 15s 204ms/step - loss: 3.3306 - val_loss: 8.5728 - lr: 0.0010\n","Epoch 80/100\n","75/75 [==============================] - ETA: 0s - loss: 3.4905\n","Epoch 80: val_loss did not improve from 7.89690\n","75/75 [==============================] - 15s 206ms/step - loss: 3.4905 - val_loss: 8.2140 - lr: 0.0010\n","Epoch 81/100\n","75/75 [==============================] - ETA: 0s - loss: 3.2580\n","Epoch 81: val_loss did not improve from 7.89690\n","\n","Epoch 81: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n","75/75 [==============================] - 15s 202ms/step - loss: 3.2580 - val_loss: 8.5526 - lr: 0.0010\n","Epoch 82/100\n","75/75 [==============================] - ETA: 0s - loss: 2.9492\n","Epoch 82: val_loss did not improve from 7.89690\n","75/75 [==============================] - 15s 197ms/step - loss: 2.9492 - val_loss: 8.0530 - lr: 2.0000e-04\n","Epoch 83/100\n","75/75 [==============================] - ETA: 0s - loss: 2.5283\n","Epoch 83: val_loss did not improve from 7.89690\n","75/75 [==============================] - 15s 205ms/step - loss: 2.5283 - val_loss: 8.0358 - lr: 2.0000e-04\n","Epoch 84/100\n","75/75 [==============================] - ETA: 0s - loss: 2.4488\n","Epoch 84: val_loss did not improve from 7.89690\n","75/75 [==============================] - 15s 197ms/step - loss: 2.4488 - val_loss: 7.9398 - lr: 2.0000e-04\n","Epoch 85/100\n","75/75 [==============================] - ETA: 0s - loss: 2.3922\n","Epoch 85: val_loss improved from 7.89690 to 7.84544, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n","75/75 [==============================] - 15s 201ms/step - loss: 2.3922 - val_loss: 7.8454 - lr: 2.0000e-04\n","Epoch 86/100\n","75/75 [==============================] - ETA: 0s - loss: 2.3112\n","Epoch 86: val_loss improved from 7.84544 to 7.73745, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n","75/75 [==============================] - 15s 202ms/step - loss: 2.3112 - val_loss: 7.7374 - lr: 2.0000e-04\n","Epoch 87/100\n","75/75 [==============================] - ETA: 0s - loss: 2.3974\n","Epoch 87: val_loss improved from 7.73745 to 7.57437, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n","75/75 [==============================] - 15s 201ms/step - loss: 2.3974 - val_loss: 7.5744 - lr: 2.0000e-04\n","Epoch 88/100\n","75/75 [==============================] - ETA: 0s - loss: 2.3122\n","Epoch 88: val_loss did not improve from 7.57437\n","75/75 [==============================] - 15s 198ms/step - loss: 2.3122 - val_loss: 7.8297 - lr: 2.0000e-04\n","Epoch 89/100\n","75/75 [==============================] - ETA: 0s - loss: 2.2332\n","Epoch 89: val_loss did not improve from 7.57437\n","75/75 [==============================] - 15s 197ms/step - loss: 2.2332 - val_loss: 7.8393 - lr: 2.0000e-04\n","Epoch 90/100\n","75/75 [==============================] - ETA: 0s - loss: 2.1564\n","Epoch 90: val_loss did not improve from 7.57437\n","75/75 [==============================] - 15s 197ms/step - loss: 2.1564 - val_loss: 7.9538 - lr: 2.0000e-04\n","Epoch 91/100\n","75/75 [==============================] - ETA: 0s - loss: 2.1452\n","Epoch 91: val_loss did not improve from 7.57437\n","75/75 [==============================] - 15s 201ms/step - loss: 2.1452 - val_loss: 7.7798 - lr: 2.0000e-04\n","Epoch 92/100\n","75/75 [==============================] - ETA: 0s - loss: 2.1041\n","Epoch 92: val_loss did not improve from 7.57437\n","75/75 [==============================] - 15s 198ms/step - loss: 2.1041 - val_loss: 7.9041 - lr: 2.0000e-04\n","Epoch 93/100\n","75/75 [==============================] - ETA: 0s - loss: 2.0108\n","Epoch 93: val_loss did not improve from 7.57437\n","75/75 [==============================] - 15s 204ms/step - loss: 2.0108 - val_loss: 7.8626 - lr: 2.0000e-04\n","Epoch 94/100\n","75/75 [==============================] - ETA: 0s - loss: 2.0280\n","Epoch 94: val_loss did not improve from 7.57437\n","75/75 [==============================] - 15s 197ms/step - loss: 2.0280 - val_loss: 7.8393 - lr: 2.0000e-04\n","Epoch 95/100\n","75/75 [==============================] - ETA: 0s - loss: 2.1469\n","Epoch 95: val_loss did not improve from 7.57437\n","75/75 [==============================] - 15s 196ms/step - loss: 2.1469 - val_loss: 7.8722 - lr: 2.0000e-04\n","Epoch 96/100\n","75/75 [==============================] - ETA: 0s - loss: 1.9226\n","Epoch 96: val_loss did not improve from 7.57437\n","75/75 [==============================] - 15s 204ms/step - loss: 1.9226 - val_loss: 7.8623 - lr: 2.0000e-04\n","Epoch 97/100\n","75/75 [==============================] - ETA: 0s - loss: 1.8221\n","Epoch 97: val_loss did not improve from 7.57437\n","75/75 [==============================] - 15s 198ms/step - loss: 1.8221 - val_loss: 7.8553 - lr: 2.0000e-04\n","Epoch 98/100\n","75/75 [==============================] - ETA: 0s - loss: 1.8859\n","Epoch 98: val_loss did not improve from 7.57437\n","75/75 [==============================] - 15s 198ms/step - loss: 1.8859 - val_loss: 8.0282 - lr: 2.0000e-04\n","Epoch 99/100\n","75/75 [==============================] - ETA: 0s - loss: 2.0739\n","Epoch 99: val_loss did not improve from 7.57437\n","75/75 [==============================] - 15s 198ms/step - loss: 2.0739 - val_loss: 7.9295 - lr: 2.0000e-04\n","Epoch 100/100\n","75/75 [==============================] - ETA: 0s - loss: 1.8947\n","Epoch 100: val_loss did not improve from 7.57437\n","75/75 [==============================] - 15s 205ms/step - loss: 1.8947 - val_loss: 7.8960 - lr: 2.0000e-04\n","Total train images:      1199\n","Total validation images: 163\n","Batch:                   16\n","\n","Total time:              0:26:11.788879\n","Time per epoch:          0:00:15.717889\n","Time per item:           0:00:00.011540\n","\n","Total epochs:            100\n","Best epoch               87\n","\n","Training loss:           2.39739442\n","Validation loss:         7.57436800\n"]}]},{"cell_type":"markdown","metadata":{"id":"13g7tDjWgtXV"},"source":["## 5 Predict"]},{"cell_type":"markdown","metadata":{"id":"ddO26OT-g_QK"},"source":["The predict process is similar to the *predict* of the Keras:"]},{"cell_type":"code","metadata":{"id":"a9iHL6tmaL_j","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1658925527849,"user_tz":-120,"elapsed":33650,"user":{"displayName":"Rémi Petitpierre","userId":"01448730486989080326"}},"outputId":"a28a3600-8e20-43a3-edfd-9b8ba8bb9965"},"source":["from data import preproc as pp\n","from google.colab.patches import cv2_imshow\n","\n","start_time = datetime.datetime.now()\n","\n","# predict() function will return the predicts with the probabilities\n","predicts, _ = model.predict(x=dtgen.next_test_batch(),\n","                            steps=dtgen.steps['test'],\n","                            ctc_decode=True,\n","                            verbose=1)\n","\n","# decode to string\n","predicts = [dtgen.tokenizer.decode(x[0]) for x in predicts]\n","ground_truth = [x.decode() for x in dtgen.dataset['test']['gt']]\n","\n","total_time = datetime.datetime.now() - start_time\n","\n","# mount predict corpus file\n","with open(os.path.join(output_path, \"predict.txt\"), \"w\") as lg:\n","    for pd, gt in zip(predicts, ground_truth):\n","        lg.write(f\"TE_L {gt}\\nTE_P {pd}\\n\")\n","   \n","for i, item in enumerate(dtgen.dataset['test']['dt'][:100]):\n","    print(\"=\" * 1024, \"\\n\")\n","    cv2_imshow(pp.adjust_to_see(item))\n","    print(ground_truth[i])\n","    print(predicts[i], \"\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9JcAs3Q3WNJ-"},"source":["## 6 Evaluate"]},{"cell_type":"markdown","metadata":{"id":"8LuZBRepWbom"},"source":["Evaluation process is more manual process. Here we have the `ocr_metrics`, but feel free to implement other metrics instead. In the function, we have three parameters: \n","\n","* predicts\n","* ground_truth\n","* norm_accentuation (calculation with/without accentuation)\n","* norm_punctuation (calculation with/without punctuation marks)"]},{"cell_type":"code","metadata":{"id":"0gCwEYdKWOPK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658925527850,"user_tz":-120,"elapsed":6,"user":{"displayName":"Rémi Petitpierre","userId":"01448730486989080326"}},"outputId":"71f50fce-3730-439c-bec1-d2ce7610a1c0"},"source":["from data import evaluation\n","\n","evaluate = evaluation.ocr_metrics(predicts, ground_truth)\n","\n","e_corpus = \"\\n\".join([\n","    f\"Total test images:    {dtgen.size['test']}\",\n","    f\"Total time:           {total_time}\",\n","    f\"Time per item:        {total_time / dtgen.size['test']}\\n\",\n","    f\"Metrics:\",\n","    f\"Character Error Rate: {evaluate[0]:.8f}\",\n","    f\"Word Error Rate:      {evaluate[1]:.8f}\",\n","    f\"Sequence Error Rate:  {evaluate[2]:.8f}\"\n","])\n","\n","with open(os.path.join(output_path, \"evaluate.txt\"), \"w\") as lg:\n","    lg.write(e_corpus)\n","    print(e_corpus)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total test images:    340\n","Total time:           0:00:26.006562\n","Time per item:        0:00:00.076490\n","\n","Metrics:\n","Character Error Rate: 0.08591812\n","Word Error Rate:      0.29509539\n","Sequence Error Rate:  0.75882353\n"]}]}]}