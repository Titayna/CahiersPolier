{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/arthurflor23/handwritten-text-recognition/blob/master/src/tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"gP-v0E_S-mQP"},"source":["<img src=\"https://github.com/arthurflor23/handwritten-text-recognition/blob/master/doc/image/header.png?raw=true\" />\n","\n","# Handwritten Text Recognition using TensorFlow 2.x\n","\n","This tutorial shows how you can use the project [Handwritten Text Recognition](https://github.com/arthurflor23/handwritten-text-recognition) in your Google Colab.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"oMty1YwuWHpN"},"source":["## 1 Localhost Environment\n","\n","We'll make sure you have the project in your Google Drive with the datasets in HDF5. If you already have structured files in the cloud, skip this step."]},{"cell_type":"markdown","metadata":{"id":"39blvPTPQJpt"},"source":["### 1.1 Datasets\n","\n","The datasets that you can use:\n","\n","a. [Bentham](http://www.transcriptorium.eu/~tsdata/)\n","\n","b. [IAM](http://www.fki.inf.unibe.ch/databases/iam-handwriting-database)\n","\n","c. [Rimes](http://www.a2ialab.com/doku.php?id=rimes_database:start)\n","\n","d. [Saint Gall](https://fki.tic.heia-fr.ch/databases/saint-gall-database)\n","\n","e. [Washington](https://fki.tic.heia-fr.ch/databases/washington-database)"]},{"cell_type":"markdown","metadata":{"id":"QVBGMLifWQwl"},"source":["### 1.2 Raw folder\n","\n","On localhost, download the code project from GitHub and extract the chosen dataset (or all if you prefer) in the **raw** folder. Don't change anything of the structure of the dataset, since the scripts were made from the **original structure** of them. Your project directory will be like this:\n","\n","```\n",".\n","├── raw\n","│   ├── bentham\n","│   │   ├── BenthamDatasetR0-GT\n","│   │   └── BenthamDatasetR0-Images\n","│   ├── iam\n","│   │   ├── ascii\n","│   │   ├── forms\n","│   │   ├── largeWriterIndependentTextLineRecognitionTask\n","│   │   ├── lines\n","│   │   └── xml\n","│   ├── rimes\n","│   │   ├── eval_2011\n","│   │   ├── eval_2011_annotated.xml\n","│   │   ├── training_2011\n","│   │   └── training_2011.xml\n","│   ├── saintgall\n","│   │   ├── data\n","│   │   ├── ground_truth\n","│   │   ├── README.txt\n","│   │   └── sets\n","│   └── washington\n","│       ├── data\n","│       ├── ground_truth\n","│       ├── README.txt\n","│       └── sets\n","└── src\n","    ├── data\n","    │   ├── evaluation.py\n","    │   ├── generator.py\n","    │   ├── preproc.py\n","    │   ├── reader.py\n","    │   ├── similar_error_analysis.py\n","    ├── main.py\n","    ├── network\n","    │   ├── architecture.py\n","    │   ├── layers.py\n","    │   ├── model.py\n","    └── tutorial.ipynb\n","\n","```\n","\n","After that, create virtual environment and install the dependencies with python 3 and pip:\n","\n","> ```python -m venv .venv && source .venv/bin/activate```\n","\n","> ```pip install -r requirements.txt```"]},{"cell_type":"markdown","metadata":{"id":"WyLRbAwsWSYA"},"source":["### 1.3 HDF5 files\n","\n","Now, you'll run the *transform* function from **main.py**. For this, execute on **src** folder:\n","\n","> ```python main.py --source=<DATASET_NAME> --transform```\n","\n","Your data will be preprocess and encode, creating and saving in the **data** folder. Now your project directory will be like this:\n","\n","\n","```\n",".\n","├── data\n","│   ├── bentham.hdf5\n","│   ├── iam.hdf5\n","│   ├── rimes.hdf5\n","│   ├── saintgall.hdf5\n","│   └── washington.hdf5\n","├── raw\n","│   ├── bentham\n","│   │   ├── BenthamDatasetR0-GT\n","│   │   └── BenthamDatasetR0-Images\n","│   ├── iam\n","│   │   ├── ascii\n","│   │   ├── forms\n","│   │   ├── largeWriterIndependentTextLineRecognitionTask\n","│   │   ├── lines\n","│   │   └── xml\n","│   ├── rimes\n","│   │   ├── eval_2011\n","│   │   ├── eval_2011_annotated.xml\n","│   │   ├── training_2011\n","│   │   └── training_2011.xml\n","│   ├── saintgall\n","│   │   ├── data\n","│   │   ├── ground_truth\n","│   │   ├── README.txt\n","│   │   └── sets\n","│   └── washington\n","│       ├── data\n","│       ├── ground_truth\n","│       ├── README.txt\n","│       └── sets\n","└── src\n","    ├── data\n","    │   ├── evaluation.py\n","    │   ├── generator.py\n","    │   ├── preproc.py\n","    │   ├── reader.py\n","    │   ├── similar_error_analysis.py\n","    ├── main.py\n","    ├── network\n","    │   ├── architecture.py\n","    │   ├── layers.py\n","    │   ├── model.py\n","    └── tutorial.ipynb\n","\n","```\n","\n","Then upload the **data** and **src** folders in the same directory in your Google Drive."]},{"cell_type":"markdown","metadata":{"id":"jydsAcWgWVth"},"source":["## 2 Google Drive Environment\n"]},{"cell_type":"markdown","metadata":{"id":"wk3e7YJiXzSl"},"source":["### 2.1 TensorFlow 2.x"]},{"cell_type":"markdown","metadata":{"id":"Z7twXyNGXtbJ"},"source":["Make sure the jupyter notebook is using GPU mode."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":413,"status":"ok","timestamp":1680695024604,"user":{"displayName":"Rémi Petitpierre","userId":"01448730486989080326"},"user_tz":-120},"id":"mHw4tODULT1Z","outputId":"34ebef67-5669-47a3-8069-862db2b29a22"},"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Apr  5 11:43:44 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   77C    P0    33W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4760,"status":"ok","timestamp":1680695029742,"user":{"displayName":"Rémi Petitpierre","userId":"01448730486989080326"},"user_tz":-120},"id":"FMg-B5PH9h3r","outputId":"3e658235-ab64-4351-fd78-04bbc9a77333"},"outputs":[{"output_type":"stream","name":"stdout","text":["Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n","Found GPU at: /device:GPU:0\n"]}],"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","\n","device_name = tf.test.gpu_device_name()\n","\n","if device_name != \"/device:GPU:0\":\n","    raise SystemError(\"GPU device not found\")\n","\n","print(\"Found GPU at: {}\".format(device_name))"]},{"cell_type":"markdown","metadata":{"id":"FyMv5wyDXxqc"},"source":["### 2.2 Google Drive"]},{"cell_type":"markdown","metadata":{"id":"P5gj6qwoX9W3"},"source":["Mount your Google Drive partition.\n","\n","**Note:** *\\\"Colab Notebooks/handwritten-text-recognition/src/\\\"* was the directory where you put the project folders, specifically the **src** folder."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3440,"status":"ok","timestamp":1680695077921,"user":{"displayName":"Rémi Petitpierre","userId":"01448730486989080326"},"user_tz":-120},"id":"ACQn1iBF9k9O","outputId":"ddd0440e-3b11-4ce3-90d2-015d5c3c97d0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at ./gdrive\n","/content/gdrive/My Drive/handwritten-text-recognition_3/src\n","total 1673\n","drwx------ 2 root root    4096 Jul 26  2022 data\n","drwx------ 2 root root    4096 Jul 26  2022 language\n","-rw------- 1 root root   11223 Apr 15  2022 main.py\n","drwx------ 2 root root    4096 Jul 26  2022 network\n","-rw------- 1 root root   82847 Apr  5 11:41 tutorial_1.ipynb\n","-rw------- 1 root root 1606229 Jan 24 09:18 tutorial.ipynb\n"]}],"source":["from google.colab import drive\n","\n","drive.mount(\"./gdrive\", force_remount=True)\n","\n","%cd \"./gdrive/My Drive/handwritten-text-recognition_3/src/\"\n","!ls -l"]},{"cell_type":"markdown","metadata":{"id":"YwogUA8RZAyp"},"source":["After mount, you can see the list os files in the project folder."]},{"cell_type":"markdown","metadata":{"id":"-fj7fSngY1IX"},"source":["## 3 Set Python Classes"]},{"cell_type":"markdown","metadata":{"id":"p6Q4cOlWhNl3"},"source":["### 3.1 Environment"]},{"cell_type":"markdown","metadata":{"id":"wvqL2Eq5ZUc7"},"source":["First, let's define our environment variables.\n","\n","Set the main configuration parameters, like input size, batch size, number of epochs and list of characters. This make compatible with **main.py** and jupyter notebook:\n","\n","* **dataset**: \"bentham\", \"iam\", \"rimes\", \"saintgall\", \"washington\"\n","\n","* **arch**: network to run: \"bluche\", \"puigcerver\", \"flor\"\n","\n","* **epochs**: number of epochs\n","\n","* **batch_size**: number size of the batch"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":262,"status":"ok","timestamp":1680700359965,"user":{"displayName":"Rémi Petitpierre","userId":"01448730486989080326"},"user_tz":-120},"id":"_Qpr3drnGMWS","outputId":"a57b3dfa-08b8-47d1-e2ca-0bdf2059a9de"},"outputs":[{"output_type":"stream","name":"stdout","text":["source: ../data/polier_5.hdf5\n","output ../output/polier_5-1/puigcerver\n","target ../output/polier_5-1/puigcerver/checkpoint_weights.hdf5\n","charset: 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"&'(),.:;?éàôèêû =+°{/-\n"]}],"source":["import os\n","import datetime\n","import string\n","\n","# define parameters\n","source = \"polier\"\n","arch = \"puigcerver\"\n","epochs = 1000\n","batch_size = 16\n","\n","# define paths\n","#source_path = os.path.join(\"..\", \"data\", f\"{source}.hdf5\")\n","#output_path = os.path.join(\"..\", \"output\", source, arch)\n","#target_path = os.path.join(output_path, \"checkpoint_weights.hdf5\")\n","#os.makedirs(output_path, exist_ok=True)\n","\n","# chemin des données transformées depuis Transkribus\n","source_path = '../data/polier_5.hdf5'\n","\n","# chemin de sauvegarde des données (e.g. prediction)\n","output_path = '../output/polier_5-1/puigcerver'\n","os.makedirs(output_path, exist_ok=True)\n","\n","# chemin vers le modèle\n","#target_path = '../output/bentham/puigcerver/checkpoint_weights_bentham_puigcerver.hdf5'\n","target_path = '../output/polier_5-1/puigcerver/checkpoint_weights.hdf5'\n","\n","# define input size, number max of chars per line and list of valid chars\n","input_size = (1024, 128, 1)\n","max_text_length = 128\n","#charset_base = string.printable[:95]\n","charset_base = \"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\\\"&'(),.:;?éàôèêû =+°{/-\"\n","\n","print(\"source:\", source_path)\n","print(\"output\", output_path)\n","print(\"target\", target_path)\n","print(\"charset:\", charset_base)"]},{"cell_type":"markdown","metadata":{"id":"BFextshOhTKr"},"source":["### 3.2 DataGenerator Class"]},{"cell_type":"markdown","metadata":{"id":"KfZ1mfvsanu1"},"source":["The second class is **DataGenerator()**, responsible for:\n","\n","* Load the dataset partitions (train, valid, test);\n","\n","* Manager batchs for train/validation/test process."]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2974,"status":"ok","timestamp":1680700366137,"user":{"displayName":"Rémi Petitpierre","userId":"01448730486989080326"},"user_tz":-120},"id":"8k9vpNzMIAi2","outputId":"f6bedbea-b3dc-4788-cdcd-745bb9370766"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train images: 1460\n","Validation images: 213\n","Test images: 421\n"]}],"source":["from data.generator import DataGenerator\n","\n","dtgen = DataGenerator(source=source_path,\n","                      batch_size=batch_size,\n","                      charset=charset_base,\n","                      max_text_length=max_text_length)\n","\n","print(f\"Train images: {dtgen.size['train']}\")\n","print(f\"Validation images: {dtgen.size['valid']}\")\n","print(f\"Test images: {dtgen.size['test']}\")"]},{"cell_type":"markdown","metadata":{"id":"-OdgNLK0hYAA"},"source":["### 3.3 HTRModel Class"]},{"cell_type":"markdown","metadata":{"id":"jHktk8AFcnKy"},"source":["The third class is **HTRModel()**, was developed to be easy to use and to abstract the complicated flow of a HTR system. It's responsible for:\n","\n","* Create model with Handwritten Text Recognition flow, in which calculate the loss function by CTC and decode output to calculate the HTR metrics (CER, WER and SER);\n","\n","* Save and load model;\n","\n","* Load weights in the models (train/infer);\n","\n","* Make Train/Predict process using *generator*.\n","\n","To make a dynamic HTRModel, its parameters are the *architecture*, *input_size* and *vocab_size*."]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8610,"status":"ok","timestamp":1680700377517,"user":{"displayName":"Rémi Petitpierre","userId":"01448730486989080326"},"user_tz":-120},"id":"nV0GreStISTR","outputId":"cbf57cf8-b69b-44ff-aff4-09c052b7baad"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_8\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input (InputLayer)          [(None, 1024, 128, 1)]    0         \n","                                                                 \n"," conv2d_40 (Conv2D)          (None, 1024, 128, 16)     160       \n","                                                                 \n"," batch_normalization_40 (Bat  (None, 1024, 128, 16)    64        \n"," chNormalization)                                                \n","                                                                 \n"," leaky_re_lu_40 (LeakyReLU)  (None, 1024, 128, 16)     0         \n","                                                                 \n"," max_pooling2d_24 (MaxPoolin  (None, 512, 64, 16)      0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_41 (Conv2D)          (None, 512, 64, 32)       4640      \n","                                                                 \n"," batch_normalization_41 (Bat  (None, 512, 64, 32)      128       \n"," chNormalization)                                                \n","                                                                 \n"," leaky_re_lu_41 (LeakyReLU)  (None, 512, 64, 32)       0         \n","                                                                 \n"," max_pooling2d_25 (MaxPoolin  (None, 256, 32, 32)      0         \n"," g2D)                                                            \n","                                                                 \n"," dropout_32 (Dropout)        (None, 256, 32, 32)       0         \n","                                                                 \n"," conv2d_42 (Conv2D)          (None, 256, 32, 48)       13872     \n","                                                                 \n"," batch_normalization_42 (Bat  (None, 256, 32, 48)      192       \n"," chNormalization)                                                \n","                                                                 \n"," leaky_re_lu_42 (LeakyReLU)  (None, 256, 32, 48)       0         \n","                                                                 \n"," max_pooling2d_26 (MaxPoolin  (None, 128, 16, 48)      0         \n"," g2D)                                                            \n","                                                                 \n"," dropout_33 (Dropout)        (None, 128, 16, 48)       0         \n","                                                                 \n"," conv2d_43 (Conv2D)          (None, 128, 16, 64)       27712     \n","                                                                 \n"," batch_normalization_43 (Bat  (None, 128, 16, 64)      256       \n"," chNormalization)                                                \n","                                                                 \n"," leaky_re_lu_43 (LeakyReLU)  (None, 128, 16, 64)       0         \n","                                                                 \n"," dropout_34 (Dropout)        (None, 128, 16, 64)       0         \n","                                                                 \n"," conv2d_44 (Conv2D)          (None, 128, 16, 80)       46160     \n","                                                                 \n"," batch_normalization_44 (Bat  (None, 128, 16, 80)      320       \n"," chNormalization)                                                \n","                                                                 \n"," leaky_re_lu_44 (LeakyReLU)  (None, 128, 16, 80)       0         \n","                                                                 \n"," reshape_8 (Reshape)         (None, 128, 1280)         0         \n","                                                                 \n"," bidirectional_40 (Bidirecti  (None, 128, 512)         3147776   \n"," onal)                                                           \n","                                                                 \n"," bidirectional_41 (Bidirecti  (None, 128, 512)         1574912   \n"," onal)                                                           \n","                                                                 \n"," bidirectional_42 (Bidirecti  (None, 128, 512)         1574912   \n"," onal)                                                           \n","                                                                 \n"," bidirectional_43 (Bidirecti  (None, 128, 512)         1574912   \n"," onal)                                                           \n","                                                                 \n"," bidirectional_44 (Bidirecti  (None, 128, 512)         1574912   \n"," onal)                                                           \n","                                                                 \n"," dropout_35 (Dropout)        (None, 128, 512)          0         \n","                                                                 \n"," dense_8 (Dense)             (None, 128, 89)           45657     \n","                                                                 \n","=================================================================\n","Total params: 9,586,585\n","Trainable params: 9,586,105\n","Non-trainable params: 480\n","_________________________________________________________________\n"]}],"source":["from network.model import HTRModel\n","\n","# create and compile HTRModel\n","model = HTRModel(architecture=arch,\n","                 input_size=input_size,\n","                 vocab_size=dtgen.tokenizer.vocab_size,\n","                 beam_width=10,\n","                 stop_tolerance=50,\n","                 reduce_tolerance=20)\n","\n","model.compile(learning_rate=0.0005)\n","model.summary(output_path, \"summary.txt\")\n","\n","# get default callbacks and load checkpoint weights file (HDF5) if exists\n","model.load_checkpoint(target=target_path)\n","\n","callbacks = model.get_callbacks(logdir=output_path, checkpoint=target_path, verbose=1)"]},{"cell_type":"markdown","metadata":{"id":"T1fnz0Eugqru"},"source":["## 4 Training"]},{"cell_type":"markdown","metadata":{"id":"w1mLOcqYgsO-"},"source":["The training process is similar to the *fit()* of the Keras. After training, the information (epochs and minimum loss) is save."]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2P6MSoxCISlD","outputId":"18c6c473-0673-49ff-df9f-24e3c686b76d","executionInfo":{"status":"ok","timestamp":1680701487031,"user_tz":-120,"elapsed":1109520,"user":{"displayName":"Rémi Petitpierre","userId":"01448730486989080326"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/1000\n","92/92 [==============================] - ETA: 0s - loss: 10.8731\n","Epoch 1: val_loss improved from inf to 32.43268, saving model to ../output/polier_5-1/puigcerver/checkpoint_weights.hdf5\n","92/92 [==============================] - 46s 264ms/step - loss: 10.8731 - val_loss: 32.4327 - lr: 5.0000e-04\n","Epoch 2/1000\n","92/92 [==============================] - ETA: 0s - loss: 10.1484\n","Epoch 2: val_loss did not improve from 32.43268\n","92/92 [==============================] - 19s 205ms/step - loss: 10.1484 - val_loss: 125.7918 - lr: 5.0000e-04\n","Epoch 3/1000\n","92/92 [==============================] - ETA: 0s - loss: 10.1454\n","Epoch 3: val_loss did not improve from 32.43268\n","92/92 [==============================] - 18s 200ms/step - loss: 10.1454 - val_loss: 47.8247 - lr: 5.0000e-04\n","Epoch 4/1000\n","92/92 [==============================] - ETA: 0s - loss: 10.2818\n","Epoch 4: val_loss did not improve from 32.43268\n","92/92 [==============================] - 18s 198ms/step - loss: 10.2818 - val_loss: 66.3376 - lr: 5.0000e-04\n","Epoch 5/1000\n","92/92 [==============================] - ETA: 0s - loss: 9.9928\n","Epoch 5: val_loss did not improve from 32.43268\n","92/92 [==============================] - 19s 204ms/step - loss: 9.9928 - val_loss: 169.5041 - lr: 5.0000e-04\n","Epoch 6/1000\n","92/92 [==============================] - ETA: 0s - loss: 9.8019\n","Epoch 6: val_loss did not improve from 32.43268\n","92/92 [==============================] - 18s 197ms/step - loss: 9.8019 - val_loss: 38.4990 - lr: 5.0000e-04\n","Epoch 7/1000\n","92/92 [==============================] - ETA: 0s - loss: 9.6240\n","Epoch 7: val_loss improved from 32.43268 to 31.55239, saving model to ../output/polier_5-1/puigcerver/checkpoint_weights.hdf5\n","92/92 [==============================] - 19s 206ms/step - loss: 9.6240 - val_loss: 31.5524 - lr: 5.0000e-04\n","Epoch 8/1000\n","92/92 [==============================] - ETA: 0s - loss: 9.5029\n","Epoch 8: val_loss improved from 31.55239 to 31.16977, saving model to ../output/polier_5-1/puigcerver/checkpoint_weights.hdf5\n","92/92 [==============================] - 19s 208ms/step - loss: 9.5029 - val_loss: 31.1698 - lr: 5.0000e-04\n","Epoch 9/1000\n","92/92 [==============================] - ETA: 0s - loss: 9.3657\n","Epoch 9: val_loss did not improve from 31.16977\n","92/92 [==============================] - 18s 199ms/step - loss: 9.3657 - val_loss: 35.4248 - lr: 5.0000e-04\n","Epoch 10/1000\n","92/92 [==============================] - ETA: 0s - loss: 9.0568\n","Epoch 10: val_loss did not improve from 31.16977\n","92/92 [==============================] - 20s 215ms/step - loss: 9.0568 - val_loss: 32.3498 - lr: 5.0000e-04\n","Epoch 11/1000\n","92/92 [==============================] - ETA: 0s - loss: 8.9224\n","Epoch 11: val_loss did not improve from 31.16977\n","92/92 [==============================] - 20s 222ms/step - loss: 8.9224 - val_loss: 45.7478 - lr: 5.0000e-04\n","Epoch 12/1000\n","92/92 [==============================] - ETA: 0s - loss: 9.2692\n","Epoch 12: val_loss did not improve from 31.16977\n","92/92 [==============================] - 19s 201ms/step - loss: 9.2692 - val_loss: 51.5732 - lr: 5.0000e-04\n","Epoch 13/1000\n","92/92 [==============================] - ETA: 0s - loss: 8.9378\n","Epoch 13: val_loss did not improve from 31.16977\n","92/92 [==============================] - 18s 199ms/step - loss: 8.9378 - val_loss: 158.2626 - lr: 5.0000e-04\n","Epoch 14/1000\n","92/92 [==============================] - ETA: 0s - loss: 8.7906\n","Epoch 14: val_loss did not improve from 31.16977\n","92/92 [==============================] - 18s 200ms/step - loss: 8.7906 - val_loss: 73.1339 - lr: 5.0000e-04\n","Epoch 15/1000\n","92/92 [==============================] - ETA: 0s - loss: 8.7852\n","Epoch 15: val_loss did not improve from 31.16977\n","92/92 [==============================] - 18s 200ms/step - loss: 8.7852 - val_loss: 33.7207 - lr: 5.0000e-04\n","Epoch 16/1000\n","92/92 [==============================] - ETA: 0s - loss: 8.2852\n","Epoch 16: val_loss did not improve from 31.16977\n","92/92 [==============================] - 18s 198ms/step - loss: 8.2852 - val_loss: 36.8369 - lr: 5.0000e-04\n","Epoch 17/1000\n","92/92 [==============================] - ETA: 0s - loss: 7.9011\n","Epoch 17: val_loss did not improve from 31.16977\n","92/92 [==============================] - 18s 200ms/step - loss: 7.9011 - val_loss: 76.8050 - lr: 5.0000e-04\n","Epoch 18/1000\n","92/92 [==============================] - ETA: 0s - loss: 8.1596\n","Epoch 18: val_loss did not improve from 31.16977\n","92/92 [==============================] - 19s 201ms/step - loss: 8.1596 - val_loss: 42.7229 - lr: 5.0000e-04\n","Epoch 19/1000\n","92/92 [==============================] - ETA: 0s - loss: 8.0811\n","Epoch 19: val_loss did not improve from 31.16977\n","92/92 [==============================] - 18s 198ms/step - loss: 8.0811 - val_loss: 32.9768 - lr: 5.0000e-04\n","Epoch 20/1000\n","92/92 [==============================] - ETA: 0s - loss: 8.0957\n","Epoch 20: val_loss did not improve from 31.16977\n","92/92 [==============================] - 18s 198ms/step - loss: 8.0957 - val_loss: 57.5669 - lr: 5.0000e-04\n","Epoch 21/1000\n","92/92 [==============================] - ETA: 0s - loss: 7.7723\n","Epoch 21: val_loss did not improve from 31.16977\n","92/92 [==============================] - 21s 230ms/step - loss: 7.7723 - val_loss: 33.2994 - lr: 5.0000e-04\n","Epoch 22/1000\n","92/92 [==============================] - ETA: 0s - loss: 7.8771\n","Epoch 22: val_loss did not improve from 31.16977\n","92/92 [==============================] - 18s 196ms/step - loss: 7.8771 - val_loss: 32.0756 - lr: 5.0000e-04\n","Epoch 23/1000\n","92/92 [==============================] - ETA: 0s - loss: 7.6336\n","Epoch 23: val_loss did not improve from 31.16977\n","92/92 [==============================] - 18s 200ms/step - loss: 7.6336 - val_loss: 40.8786 - lr: 5.0000e-04\n","Epoch 24/1000\n","92/92 [==============================] - ETA: 0s - loss: 7.3199\n","Epoch 24: val_loss did not improve from 31.16977\n","92/92 [==============================] - 19s 203ms/step - loss: 7.3199 - val_loss: 33.4393 - lr: 5.0000e-04\n","Epoch 25/1000\n","92/92 [==============================] - ETA: 0s - loss: 7.4284\n","Epoch 25: val_loss did not improve from 31.16977\n","92/92 [==============================] - 18s 196ms/step - loss: 7.4284 - val_loss: 43.1898 - lr: 5.0000e-04\n","Epoch 26/1000\n","92/92 [==============================] - ETA: 0s - loss: 7.2710\n","Epoch 26: val_loss did not improve from 31.16977\n","92/92 [==============================] - 18s 200ms/step - loss: 7.2710 - val_loss: 33.5139 - lr: 5.0000e-04\n","Epoch 27/1000\n","92/92 [==============================] - ETA: 0s - loss: 7.1458\n","Epoch 27: val_loss did not improve from 31.16977\n","92/92 [==============================] - 19s 204ms/step - loss: 7.1458 - val_loss: 106.7613 - lr: 5.0000e-04\n","Epoch 28/1000\n","92/92 [==============================] - ETA: 0s - loss: 7.2500\n","Epoch 28: val_loss did not improve from 31.16977\n","\n","Epoch 28: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","92/92 [==============================] - 18s 200ms/step - loss: 7.2500 - val_loss: 80.5495 - lr: 5.0000e-04\n","Epoch 29/1000\n","92/92 [==============================] - ETA: 0s - loss: 6.7309\n","Epoch 29: val_loss did not improve from 31.16977\n","92/92 [==============================] - 18s 199ms/step - loss: 6.7309 - val_loss: 31.5188 - lr: 1.0000e-04\n","Epoch 30/1000\n","92/92 [==============================] - ETA: 0s - loss: 6.4990\n","Epoch 30: val_loss did not improve from 31.16977\n","92/92 [==============================] - 19s 203ms/step - loss: 6.4990 - val_loss: 31.5641 - lr: 1.0000e-04\n","Epoch 31/1000\n","92/92 [==============================] - ETA: 0s - loss: 6.1718\n","Epoch 31: val_loss did not improve from 31.16977\n","92/92 [==============================] - 20s 219ms/step - loss: 6.1718 - val_loss: 31.4014 - lr: 1.0000e-04\n","Epoch 32/1000\n","92/92 [==============================] - ETA: 0s - loss: 6.1210\n","Epoch 32: val_loss did not improve from 31.16977\n","92/92 [==============================] - 19s 203ms/step - loss: 6.1210 - val_loss: 31.5072 - lr: 1.0000e-04\n","Epoch 33/1000\n","92/92 [==============================] - ETA: 0s - loss: 6.0333\n","Epoch 33: val_loss did not improve from 31.16977\n","92/92 [==============================] - 18s 196ms/step - loss: 6.0333 - val_loss: 32.4633 - lr: 1.0000e-04\n","Epoch 34/1000\n","92/92 [==============================] - ETA: 0s - loss: 6.0599\n","Epoch 34: val_loss did not improve from 31.16977\n","92/92 [==============================] - 18s 199ms/step - loss: 6.0599 - val_loss: 31.5290 - lr: 1.0000e-04\n","Epoch 35/1000\n","92/92 [==============================] - ETA: 0s - loss: 5.9541\n","Epoch 35: val_loss did not improve from 31.16977\n","92/92 [==============================] - 19s 203ms/step - loss: 5.9541 - val_loss: 34.4033 - lr: 1.0000e-04\n","Epoch 36/1000\n","92/92 [==============================] - ETA: 0s - loss: 6.0079\n","Epoch 36: val_loss did not improve from 31.16977\n","92/92 [==============================] - 18s 201ms/step - loss: 6.0079 - val_loss: 31.4140 - lr: 1.0000e-04\n","Epoch 37/1000\n","92/92 [==============================] - ETA: 0s - loss: 5.8921\n","Epoch 37: val_loss did not improve from 31.16977\n","92/92 [==============================] - 18s 197ms/step - loss: 5.8921 - val_loss: 31.4160 - lr: 1.0000e-04\n","Epoch 38/1000\n","92/92 [==============================] - ETA: 0s - loss: 5.7791\n","Epoch 38: val_loss did not improve from 31.16977\n","92/92 [==============================] - 19s 204ms/step - loss: 5.7791 - val_loss: 31.8211 - lr: 1.0000e-04\n","Epoch 39/1000\n","92/92 [==============================] - ETA: 0s - loss: 5.7885\n","Epoch 39: val_loss did not improve from 31.16977\n","92/92 [==============================] - 18s 196ms/step - loss: 5.7885 - val_loss: 31.6802 - lr: 1.0000e-04\n","Epoch 40/1000\n","92/92 [==============================] - ETA: 0s - loss: 5.7620\n","Epoch 40: val_loss did not improve from 31.16977\n","92/92 [==============================] - 18s 197ms/step - loss: 5.7620 - val_loss: 31.3696 - lr: 1.0000e-04\n","Epoch 41/1000\n","92/92 [==============================] - ETA: 0s - loss: 5.5815\n","Epoch 41: val_loss did not improve from 31.16977\n","92/92 [==============================] - 21s 225ms/step - loss: 5.5815 - val_loss: 31.7304 - lr: 1.0000e-04\n","Epoch 42/1000\n","92/92 [==============================] - ETA: 0s - loss: 5.7314\n","Epoch 42: val_loss did not improve from 31.16977\n","92/92 [==============================] - 18s 196ms/step - loss: 5.7314 - val_loss: 31.7934 - lr: 1.0000e-04\n","Epoch 43/1000\n","92/92 [==============================] - ETA: 0s - loss: 5.6342\n","Epoch 43: val_loss did not improve from 31.16977\n","92/92 [==============================] - 18s 200ms/step - loss: 5.6342 - val_loss: 31.8641 - lr: 1.0000e-04\n","Epoch 44/1000\n","92/92 [==============================] - ETA: 0s - loss: 5.5453\n","Epoch 44: val_loss did not improve from 31.16977\n","92/92 [==============================] - 18s 201ms/step - loss: 5.5453 - val_loss: 31.9262 - lr: 1.0000e-04\n","Epoch 45/1000\n","92/92 [==============================] - ETA: 0s - loss: 5.5253\n","Epoch 45: val_loss did not improve from 31.16977\n","92/92 [==============================] - 18s 197ms/step - loss: 5.5253 - val_loss: 31.9692 - lr: 1.0000e-04\n","Epoch 46/1000\n","92/92 [==============================] - ETA: 0s - loss: 5.5986\n","Epoch 46: val_loss did not improve from 31.16977\n","92/92 [==============================] - 18s 200ms/step - loss: 5.5986 - val_loss: 32.1269 - lr: 1.0000e-04\n","Epoch 47/1000\n","92/92 [==============================] - ETA: 0s - loss: 5.3063\n","Epoch 47: val_loss did not improve from 31.16977\n","92/92 [==============================] - 19s 202ms/step - loss: 5.3063 - val_loss: 36.9204 - lr: 1.0000e-04\n","Epoch 48/1000\n","92/92 [==============================] - ETA: 0s - loss: 5.5073\n","Epoch 48: val_loss did not improve from 31.16977\n","\n","Epoch 48: ReduceLROnPlateau reducing learning rate to 2.0000000949949027e-05.\n","92/92 [==============================] - 18s 200ms/step - loss: 5.5073 - val_loss: 31.9730 - lr: 1.0000e-04\n","Epoch 49/1000\n","92/92 [==============================] - ETA: 0s - loss: 5.3907\n","Epoch 49: val_loss did not improve from 31.16977\n","92/92 [==============================] - 18s 201ms/step - loss: 5.3907 - val_loss: 31.9015 - lr: 2.0000e-05\n","Epoch 50/1000\n","92/92 [==============================] - ETA: 0s - loss: 5.2353\n","Epoch 50: val_loss did not improve from 31.16977\n","92/92 [==============================] - 19s 203ms/step - loss: 5.2353 - val_loss: 31.8046 - lr: 2.0000e-05\n","Epoch 51/1000\n","92/92 [==============================] - ETA: 0s - loss: 5.1539\n","Epoch 51: val_loss did not improve from 31.16977\n","92/92 [==============================] - 20s 220ms/step - loss: 5.1539 - val_loss: 31.8516 - lr: 2.0000e-05\n","Epoch 52/1000\n","92/92 [==============================] - ETA: 0s - loss: 5.1312\n","Epoch 52: val_loss did not improve from 31.16977\n","92/92 [==============================] - 19s 205ms/step - loss: 5.1312 - val_loss: 31.8128 - lr: 2.0000e-05\n","Epoch 53/1000\n","92/92 [==============================] - ETA: 0s - loss: 5.1588\n","Epoch 53: val_loss did not improve from 31.16977\n","92/92 [==============================] - 18s 201ms/step - loss: 5.1588 - val_loss: 31.8434 - lr: 2.0000e-05\n","Epoch 54/1000\n","92/92 [==============================] - ETA: 0s - loss: 5.0938\n","Epoch 54: val_loss did not improve from 31.16977\n","92/92 [==============================] - 18s 200ms/step - loss: 5.0938 - val_loss: 31.9113 - lr: 2.0000e-05\n","Epoch 55/1000\n","92/92 [==============================] - ETA: 0s - loss: 5.1327\n","Epoch 55: val_loss did not improve from 31.16977\n","92/92 [==============================] - 19s 205ms/step - loss: 5.1327 - val_loss: 31.9905 - lr: 2.0000e-05\n","Epoch 56/1000\n","92/92 [==============================] - ETA: 0s - loss: 5.2387\n","Epoch 56: val_loss did not improve from 31.16977\n","92/92 [==============================] - 19s 202ms/step - loss: 5.2387 - val_loss: 31.9613 - lr: 2.0000e-05\n","Epoch 57/1000\n","92/92 [==============================] - ETA: 0s - loss: 5.1926\n","Epoch 57: val_loss did not improve from 31.16977\n","92/92 [==============================] - 18s 201ms/step - loss: 5.1926 - val_loss: 31.9421 - lr: 2.0000e-05\n","Epoch 58/1000\n","92/92 [==============================] - ETA: 0s - loss: 5.0086\n","Epoch 58: val_loss did not improve from 31.16977\n","Restoring model weights from the end of the best epoch: 8.\n","92/92 [==============================] - 19s 205ms/step - loss: 5.0086 - val_loss: 31.9768 - lr: 2.0000e-05\n","Epoch 58: early stopping\n","Total train images:      1460\n","Total validation images: 213\n","Batch:                   16\n","\n","Total time:              0:18:29.432633\n","Time per epoch:          0:00:19.128149\n","Time per item:           0:00:00.011433\n","\n","Total epochs:            58\n","Best epoch               8\n","\n","Training loss:           9.50285530\n","Validation loss:         31.16977119\n"]}],"source":["# to calculate total and average time per epoch\n","start_time = datetime.datetime.now()\n","\n","h = model.fit(x=dtgen.next_train_batch(),\n","              epochs=epochs,\n","              steps_per_epoch=dtgen.steps['train'],\n","              validation_data=dtgen.next_valid_batch(),\n","              validation_steps=dtgen.steps['valid'],\n","              callbacks=callbacks,\n","              shuffle=True,\n","              verbose=1)\n","\n","total_time = datetime.datetime.now() - start_time\n","\n","loss = h.history['loss']\n","val_loss = h.history['val_loss']\n","\n","min_val_loss = min(val_loss)\n","min_val_loss_i = val_loss.index(min_val_loss)\n","\n","time_epoch = (total_time / len(loss))\n","total_item = (dtgen.size['train'] + dtgen.size['valid'])\n","\n","t_corpus = \"\\n\".join([\n","    f\"Total train images:      {dtgen.size['train']}\",\n","    f\"Total validation images: {dtgen.size['valid']}\",\n","    f\"Batch:                   {dtgen.batch_size}\\n\",\n","    f\"Total time:              {total_time}\",\n","    f\"Time per epoch:          {time_epoch}\",\n","    f\"Time per item:           {time_epoch / total_item}\\n\",\n","    f\"Total epochs:            {len(loss)}\",\n","    f\"Best epoch               {min_val_loss_i + 1}\\n\",\n","    f\"Training loss:           {loss[min_val_loss_i]:.8f}\",\n","    f\"Validation loss:         {min_val_loss:.8f}\"\n","])\n","\n","with open(os.path.join(output_path, \"train.txt\"), \"w\") as lg:\n","    lg.write(t_corpus)\n","    print(t_corpus)"]},{"cell_type":"markdown","metadata":{"id":"13g7tDjWgtXV"},"source":["## 5 Predict"]},{"cell_type":"markdown","metadata":{"id":"ddO26OT-g_QK"},"source":["The predict process is similar to the *predict* of the Keras:"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1GSqVJU9G76HardSUGUSg4hY66mjqHRae"},"executionInfo":{"elapsed":51715,"status":"ok","timestamp":1680701637517,"user":{"displayName":"Rémi Petitpierre","userId":"01448730486989080326"},"user_tz":-120},"id":"a9iHL6tmaL_j","outputId":"bcf0061e-5449-495f-c400-1998b27252e4"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["from data import preproc as pp\n","from google.colab.patches import cv2_imshow\n","\n","start_time = datetime.datetime.now()\n","\n","# predict() function will return the predicts with the probabilities\n","predicts, _ = model.predict(x=dtgen.next_test_batch(),\n","                            steps=dtgen.steps['test'],\n","                            ctc_decode=True,\n","                            verbose=1)\n","\n","# decode to string\n","predicts = [dtgen.tokenizer.decode(x[0]) for x in predicts]\n","ground_truth = [x.decode() for x in dtgen.dataset['test']['gt']]\n","\n","total_time = datetime.datetime.now() - start_time\n","\n","# mount predict corpus file\n","with open(os.path.join(output_path, \"predict.txt\"), \"w\") as lg:\n","    for pd, gt in zip(predicts, ground_truth):\n","        lg.write(f\"TE_L {gt}\\nTE_P {pd}\\n\")\n","   \n","for i, item in enumerate(dtgen.dataset['test']['dt'][:100]):\n","    print(\"=\" * 1024, \"\\n\")\n","    cv2_imshow(pp.adjust_to_see(item))\n","    print(ground_truth[i])\n","    print(predicts[i], \"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"9JcAs3Q3WNJ-"},"source":["## 6 Evaluate"]},{"cell_type":"markdown","metadata":{"id":"8LuZBRepWbom"},"source":["Evaluation process is more manual process. Here we have the `ocr_metrics`, but feel free to implement other metrics instead. In the function, we have three parameters: \n","\n","* predicts\n","* ground_truth\n","* norm_accentuation (calculation with/without accentuation)\n","* norm_punctuation (calculation with/without punctuation marks)"]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0gCwEYdKWOPK","outputId":"47629908-a2fb-41a2-aa4d-1b091339cd6e","executionInfo":{"status":"ok","timestamp":1680701637519,"user_tz":-120,"elapsed":8,"user":{"displayName":"Rémi Petitpierre","userId":"01448730486989080326"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Total test images:    421\n","Total time:           0:00:43.435773\n","Time per item:        0:00:00.103173\n","\n","Metrics:\n","Character Error Rate: 0.24184504\n","Word Error Rate:      0.59150116\n","Sequence Error Rate:  0.95724466\n"]}],"source":["from data import evaluation\n","\n","evaluate = evaluation.ocr_metrics(predicts, ground_truth)\n","\n","e_corpus = \"\\n\".join([\n","    f\"Total test images:    {dtgen.size['test']}\",\n","    f\"Total time:           {total_time}\",\n","    f\"Time per item:        {total_time / dtgen.size['test']}\\n\",\n","    f\"Metrics:\",\n","    f\"Character Error Rate: {evaluate[0]:.8f}\",\n","    f\"Word Error Rate:      {evaluate[1]:.8f}\",\n","    f\"Sequence Error Rate:  {evaluate[2]:.8f}\"\n","])\n","\n","with open(os.path.join(output_path, \"evaluate.txt\"), \"w\") as lg:\n","    lg.write(e_corpus)\n","    print(e_corpus)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["oMty1YwuWHpN"],"provenance":[]},"file_extension":".py","kernelspec":{"display_name":"Python 3","name":"python3"},"mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"nbformat":4,"nbformat_minor":0}